CHAPTER 2

# Pipelined Processors

CHAPTER OUTLINE
* 2.1 Pipelining Fundamentals 
* 2.2 Pipelined Processor Design 
* 2.3 Deeply Pipelined Processors 
* 2.4 Summary
* References
* Homework Problems

Pipelining is a powerful implementation technique for enhancing system throughput without requiring massive replication of hardware. It was first employed in the early 1960s in the design of high-end mainframes. Instruction pipelining was first introduced in the IBM 7030, nicknamed the Stretch computer [Bloch, 1959, Bucholtz, 1962]. Later the CDC 6600 incorporated both pipelining and the use of multiple functional units [Thornton, 1964].

During the 1980s, pipelining became the cornerstone of the RISC approach to processor design. Most of the techniques that constituted the RISC approach are directly or indirectly related to the objective of efficient pipelining. Since then, pipelining has been effectively applied to CISC processors as well. The Intel i486 was the first pipelined implementation of the IA32 architecture [Crawford, 1990].

Pipelined versions of Digital's V AX and Motorola's M68K architectures were also quite successful commercially. Pipelining is a technique that is now widely employed in the design of instruction set processors. This chapter focuses on the design of (scalar) pipelined processors. Many of the approaches and techniques related to the design of pipelined processors, such as pipeline interlock mechanisms for hazard detection and resolution, are foundational to the design of superscalar processors.


The current trend is toward very deep pipelines. Pipeline depth has increased from less than 10 to more than 20. Deep pipelines are necessary for achieving very high clock frequencies. This has been a very effective means of gaining greater processor performance. There are some indications that this trend will continue.




## 2.1 Pipelining Fundamentals

This section presents the motivations and the fundamental principles of pipelining.
Historically there are two major types of pipelines: arithmetic pipelines and instruction pipelines. While instruction pipelines are the focus of this book, we begin by examining an arithmetic pipeline example. Arithmetic pipelines more readily illustrate a set of idealized assumptions underlying the principles of pipelined designs.
We term these idealized assumptions the pipelining idealism. It is dealing with the discrepancy between these idealized assumptions and realistic considerations in instruction pipelining that makes pipelined processor design so interesting.



### 2.1.1 Pipelined Design
This subsection introduces the foundational notions of pipelined design. The motivations and limitations of pipelining are presented. A theoretical model, proposed by Peter Kogge, of optimal pipelining from the hardware design perspective is described [Kogge, 1981].


#### 2.1.1.1 Motivations. 
The primary motivation for pipelining is to increase the throughput of a system with little increase in hardware. The throughput, or bandwidth, of a system is measured in terms of the number of tasks performed per unit time, and it characterizes the performance of the system. For a system that operates on one task at a time, the throughput P is equal to 1I D , where D is the latency of a task or the delay associated with the performance of a task by the system. The throughput of a system can be increased by pipelining if there are many tasks that require the use of the same system. The actual latency for each task still remains the same or may even increase slightly.

Pipelining involves partitioning the system into multiple stages with added buffering between the stages. These stages and the interstage buffers constitute the pipeline. The computation carried out by the original system is decomposed into k subcomputations, carried out in the k stages of the pipeline. A new task can start into the pipeline as soon as the previous task has traversed the first stage. Hence, instead of initiating a new task every D units of time, a new task can be initiated every Dlk units of time, where k is the number of stages in the pipeline, and the processing of k computations is now overlapped in the pipeline. It is assumed that the original latency of D has been evenly partitioned into k stages and that no additional delay is introduced by the added buffers. Given that the total number of tasks to be processed is very large, the throughput of a pipelined system can potentially approach k times that of a nonpipelined system. This potential performance increase by a factor of k by simply adding new buffers in a k-stage pipeline is the primary attraction of the pipelined design. Figure 2.1 illustrates the potential k-fold increase of throughput in a k-stage pipelined system.

So far we have assumed that the addition of interstage buffers does not introduce any additional delay. This is not unrealistic. The Earle latch shown in Figure 2.2(a) was designed and used in the IBM 360/91 for buffering between stages of carry-save adders in the pipelined multiply unit. In the Earle latch, the output Z follows the input D when clock C = 1. When the clock goes low, the value at D is latched at Z through the latching loop, and then the output Z becomes insensitive to further changes at D. Proper hold time is required on the D input to ensure proper latching.

The middle AND gate ensures glitch-free operation; the product term represented by this AND gate "covers" a potential hazard. A hazard is a spurious pulse caused by a race condition involving simultaneous change of multiple signals. The top and bottom inputs to the OR gate can potentially change simultaneously in opposite directions. Under such a condition, if the OR gate does not have the middle (redundant) input, a spurious pulse (the hazard) can potentially appear at the output of the OR gate. The Earle latch has this desirable glitch-free operation feature. Furthermore, the Earle latch can be integrated into the logic function so as not to incur any additional gate delay. Figure 2.2(b) illustrates how the latching function can be merged into the last two AND-OR levels of the combinational logic circuit resulting in no additional gate delay for the addition of the latch. The circuit in Figure 2.2(b) performs the same logic function as that of Figure 2.2(a) without incurring two additional gate delays for latching. The increase of gate fan-in by one can slightly increase the delay through these gates.




#### 2.1.1.2 Limitations. 
Since the performance gained in a pipelined design is proportional to the depth, that is, the number of stages, of a pipeline, it might seem that the best design is always to maximize the number of stages of a pipelined system. However, due to clocking constraints, there are physical limitations to how finely an original computation can be partitioned into pipeline stages.

Each stage of a pipeline can be viewed as a piece of combinational logic F followed by a set of latches L. Signals must propagate through F and be latched at L. Let T M be the maximum propagation delay through F, that is, the delay through the longest signal path; let Tm be the minimum propagation delay through F, that is, the delay through the shortest signal path. Let TL be the additional time needed for proper clocking. Delay 1'L can include the necessary setup and hold times to ensure proper latching, as well as the potential clock skews, that is, the worst-case disparity between the arrival times of the clock edge at different latches. If the first set of signals X I is applied at the inputs to the stage at time T I , then the outputs of F must be valid at TI + T M' For proper latching at L, the signals at the outputs of F must continue to be valid until TI + T M + T L â€¢ When the second set of signals X2 is applied at the inputs to F at time T 2 , it takes at least until T2 + Till for the effects to be felt at the latches L. To ensure that the second set of signals does not overrun the first set, it is required that (2.1)


which means that the earliest possible arrival of X 2 at the latches must not be sooner than the time required for the proper latching of XI . This inequality can be rewritten as

where T2 - TI is effectively the minimum clocking period T. Therefore, the clocking period T must be greater than T M - T m + Tu and the maximum clocking rate cannot exceed 1/T.

Based on the foregoing analysis, two factors limit the clocking rate. One is the difference between the maximum and minimum propagation delays through the logic, namely, TM - T m' The other is the additional time required for proper clocking, namely, T L . The fIrst factor can be eliminated if all signal propagation paths are of the same length. This can be accomplished by padding the short paths. Hence, TM - T m is close to zero. The second factor is dictated by the need to latch the results of the pipeline stages. Proper latching requires the propagation of a signal through a feedback loop and the stabilizing of that signal value in the loop. Another contribution to TL is the worst-case clock skew. The clock signal may arrive at different latches at slightly different times due to the generation and distribution of the clock signals to all the latches. In a fully synchronous system, this worst-case clock skew must be accounted for in the clocking period. Ultimately, the limit of how deeply a synchronous system can be pipelined is determined by the minimum time required for latching and the uncertainty associated with the delays in the clock distribution network.




#### 2.1.1.3 Tradeoff. 
Clocking constraints determine the ultimate physical limit to the depth of pipelining. Aside from this limit, maximum pipeline depth may not be the optimal design when cost, or pipelining overhead, is considered. In the hardware design of a pipelined system, the tradeoff between cost and performance must be considered. A cost/performance tradeoff model for pipelined design has been proposed by Peter Kogge and is summarized here [Kogge, 1981]. Models for both cost and performance are proposed. The cost of a nonpipelined design is denoted as G. This cost can be in terms of gate count, transistor count, or silicon real estate. The cost C for a k-stage pipelined design is equal to 

C=G+kxL


where k is the number of stages in the pipeline, L is the cost of adding each latch, and G is the cost of the original nonpipelined hardware. Based on this cost model, the pipeline cost C is a linear function of k, the depth of the pipeline. Basically, the cost of a pipeline goes up linearly with respect to the depth of the pipeline.

Assume that the latency in the nonpipelined system is T. Then the performance of the nonpipelined design is 1/T, the computation rate. The performance P of the pipe lined design can be modeled as 1/( TI k + S), where T is the latency of the original nonpipelined design and S is the delay due to the addition of the latch. 
Assuming that the original latency T can be evenly divided into k stages, (T/k + S) is the delay associated with each stage and is thus the clocking period of the pipeline.
Consequently, 1/(T/k + S) is equal to the clocking rate and the throughput of the pipelined design. Hence, the performance of the pipelined design is 

Note that P is a nonlinear function of k.

Given these models for cost and performance, the expression for the cost/ performance ratio is

This expression can be rewritten as 

which is plotted in Figure 2.3 for two sets of sample values of G, L, T, and S.
Equation (2.6) expresses the cost/performance ratio as a function of k. The first derivative can be taken and set equal to zero to determine the value of k that will produce the minimal cost/performance ratio. This value of k, shown in Equation (2.7), is the optimal pipelining depth in terms of the other parameters.

Given this expression for the optimal value of k, a pipelined design with k < kopt can be considered as underpipelined in that further pipelining or increasing the pipeline depth is beneficial and the increased cost is justified by the increase of performance. On the other hand, k > kopt indicates an overpipelined design in which there is a diminishing return of performance for the increased cost of pipelining. The foregoing tradeoff model is based purely on hardware design considerations; there is no consideration of the dynamic behavior of the pipeline or the computations being performed. We will take up these issues later, beginning in Section 2.2.




### 2.1.2

Arithmetic Pipeline Example

There are two major types of pipelines: arithmetic pipelines and instruction pipelines. Although instruction pipeline design is the focus of this chapter, we will begin by looking at an arithmetic pipeline example. Arithmetic pipelines clearly illustrate the effectiveness of pipelining without having to deal with some of the complex issues involved in instruction pipeline design. These complex issues will be addressed in subsequent sections of this chapter.



#### 2.1.2.1 Floating-Point Multiplication.  
The design of a pipelined floating-point multiplier is used as the example. This "vintage" board-level design is taken from a classic text by Shlomo Waser and Mike Flynn [Waser and Flynn, 1982]. (Even though this design assumes 1980 technology, nonetheless it still serves as an effective vehicle to illustrate arithmetic pipelining.) This design assumes a 64-bit floatingpoint format that uses the excess-128 notation for the exponent e (8 bits) and the sign-magnitude fraction notation with the hidden bit for the mantissa m (57 bits, including the hidden bit).

The floating-point multiplication algorithm implemented in this design is as follows .

1. Check to see if any operand is zero. If it is, the result is immediately set to zero.
2. Add the two characteristics (physical bit patterns of the exponents) and correct for the excess-128 bias, that is, e I + (e 2 - 128).
3. Perform fixed-point multiplication of the two mantissas m I and m2' 
4. Normalize the product of the mantissas, which involves shifting left by one bit and decrementing the exponent by 1. (The normalized representation of the mantissa has no leading zeros.) 
5. Round the result by adding 1 to the first guard bit (the bit immediately to the right of the least-significant bit of the mantissa). This is effectively rounding up. If the mantissa overflows, then the mantissa must be shifted right one bit and the exponent incremented by 1 to maintain the normalized representation for the mantissa.

Figure 2.4 illustrates in the functional block diagram the nonpipelined design of the floating-point multiplier. The input latches store the two operands to be multiplied.
At the next clock the product of the two operands will be stored in the output latches.

The fixed-point mantissa multiplier represents the most complex module in this design and consists of three submodules for partial product generation, partial product reduction, and final reduction. The hardware complexity, in terms of the number of integrated circuit (IC) chips, and the propagation delay, in nanoseconds, of each submodule can be obtained.

* Partial product generation. Simultaneous generation of the partial products can be performed using 8 x 8 hardware multipliers. To generate all the partial products, 34 such 8 x 8 multipliers are needed. The delay involved is 125 ns.
* Partial product reduction. Once all the partial products are generated, they must be reduced or summed. A summing circuit called the (5, 5, 4) counter can be used to reduce two columns of 5 bits each into a 4-bit sum. A (5 , 5, 4) counter can be implemented using a 1 K x 4 read-only memory (ROM) with a delay of 50 ns. Three levels of (5, 5, 4) counters are needed to reduce all the partial products. Hence a total of 72 such 1K x 4 ROMs are needed, incurring a total delay of 150 ns.
* Final reduction. Once all the partial products have been reduced down to two partial products a final level of reduction can be implemented using fast carry-Iookahead (CLA) adders to produce the final result. Sixteen 4-bit adder chips with CLA plus five 4-bit CLA units are needed for this final reduction step. A total of 21 IC chips and a 55-ns delay are required.


Two additional modules are needed for the mantissa section, namely, a shifter for performing normalization (2 chips, 20-ns delay) and an incrementer for performing rounding (15 chips, 50-ns delay). The Add/Sub modules in the exponent section require another 4 chips; their delays are unimportant because they are not in the critical delay path. An additional 17 and 10 chips are needed for implementing the input and output latches, respectively. The total chip counts and critical delays of the modules in the nonpipelined design are summarized in Table 2.l.

Based on the tabulation in Table 2.1, the nonpipelined design of the floatingpoint multiplier requires 175 chips and can be clocked at 2.5 MHz with a clock period of 400 ns. This implies that the nonpipelined design can achieve a throughput of 2.5 MFLOPS (million floating-point operations per second).



#### 2.1.2.2 Pipelined Floating-Point Multiplier.  
The nonpipelined design of the floating-point multiplier can be pipelined to increase its throughput. In this example, we will assume that there is no pipelining within a submodule; that is, the finest granularity for partitioning into pipeline stages is at the submodule level. We now examine the delays associated with each of the (sub)modules in the critical delay path. These delays are shown in the third column of Table 2.1. The partial product Table 2.1

Chip counts and critical delays of the modules in the nonpipelined Roating-point multiplier design.


reduction submodule has the longest delay, 150 ns; this delay then determines the delay of a stage in the pipeline. The five (sub )modules in the critical path can be partitioned into three fairly even stages with delays of 125 ns (partial product generation), 150 ns (partial product reduction), and 125 ns (final reduction, normalization, and rounding). The resultant three-stage pipelined design is shown in Figure 2.5.

In determining the actual clocking rate of the pipelined design, we must consider clocking requirements. Assuming that edge-triggered registers are used for buffering between pipeline stages, we must add the clock-edge-to-register-output delay of 17 ns and the setup time of 5 ns to the stage delay of 150 ns. This results in the minimum clocking period of 172 ns. Therefore, instead of clocking at the rate of 2.5 MHz, the new pipelined design can be clocked at the rate of 5.8 MHz. 
This represents a factor of 2.3 increase in throughput. Note, however, that the latency for performing each multiplication has increased slightly, from 400 to 516ns.

The only additional hardware required for the pipelined design is the edgetriggered register chips for buffering between pipeline stages. On top of the original 175 IC chips, an additional 82 IC chips are required. Using chip count as a measure of hardware complexity, the total of 257 IC chips represents an increase of 45% in terms of hardware complexity. This 45% increase in hardware cost resulted in a 130% increase in performance. Clearly, this three-stage pipelined design of the floating-point multiplier is a win over the original nonpipelined design .

This example assumes board-level implementations using off-the-shelf parts.
Given today's chip technology, this entire design can be easily implemented as a small module on a chip. While a board-level implementation of the floating-point multiplier may be viewed as outdated, the purpose of this example is to succinctly illustrate the effectiveness of pipelining using a published specific design with actual latency and hardware cost parameters. In fact, the upper curve in Figure 2.3 reflects the parameters from this example.




### 2.1.3 Pipelining Idealism 
Recall that the motivation for a k-stage pipelined design is to achieve a k-fold increase in throughput, as illustrated in Figure 2.1. However, in the foregoing example, the three-stage pipelined floating-point multiplier only achieved a factor of 2.3 increase in throughput. The main reason for falling short of the three-fold increase of throughput is that the k-fold increase in throughput for a k-stage pipelined design represents the ideal case and is based on three idealized assumptions, which we referred to as the pipelining idealism. The understanding of pipe lining idealism is crucial to the appreciation of pipelined designs. The unavoidable deviations from this idealism in real pipelines make pipelined designs challenging. The solutions for dealing with this idealism-realism gap comprise the interesting techniques for pipelined designs. The three points of pipelining idealism are 

1. Uniform subcomputations. The computation to be performed can be evenly partitioned into uniform-latency subcomputations.

2. Identical computations. The same computation is to be performed repeatedly on a large number of input data sets.
3. Independent computations. All the repetitions of the same computation are mutually independent.


#### 2.1.3.1 Uniform Subcomputations.  
The first point of pipelining idealism states that the computation to be pipe lined can be evenly partitioned into k uniformlatency subcomputations. This means that the original design can be evenly partitioned into k balanced (i.e., having the same latency) pipeline stages. If the latency of the original computation, and hence the clocking period of the nonpipelined design, is T, then the clocking period of a k-stage pipelined design is exactly Tlk, which is the latency of each of the k stages. Given this idealized assumption, the k-fold increase in throughput is achieved due to the k-fold increase of the clocking rate.

This idealized assumption may not be true in an actual pipelined design. It may not be possible to partition the computation into perfectly balanced stages.
We see in our floating-point multiplier example that the latency of 400 ns of the original computation is partitioned into three stages with latencies of 125, 150, and 125 ns, respectively. Clearly the original latency has not been evenly partitioned into three balanced stages. Since the clocking period of a pipelined design is dictated by the stage with the longest latency, the stages with shorter latencies in effect will incur some inefficiency or penalty. In our example, the first and third stages have an inefficiency of 25 ns each; we called such inefficiency within pipeline stages, the internal fragmentation of pipeline stages. Because of such internal fragmentation, the total latency required for performing the same computation will increase from T to TJ , and the clocking period of the pipelined design will be no longer Tlk but T/k. In our example the performance of the three subcomputations will require 450 ns instead of the original 400 ns, and the clocking period will be not 133 ns (400/3 ns) but 150 ns.

There is a secondary implicit assumption, namely, that no additional delay is introduced by the introduction of buffers between pipeline stages and that no additional delay is required for ensuring proper clocking of the pipeline stages. Again, this assumption may not be true in actual designs. In our example, an additional 22 ns is required to ensure proper clocking of the pipeline stages, which resulted in the cycle time of 172 ns for the three-stage pipelined design . The ideal cycle time for a three-stage pipelined design would have been 133 ns. The difference between 172 and 133 ns for the clocking period accounts for the shortfall from the idealized three-fold increase of throughput.

The first point of pipelining idealism basically assumes two things: (1) There is no inefficiency introduced due to the partitioning of the original computation into multiple subcomputations; and (2) there is no additional delay caused by the introduction of the interstage buffers and the clocking requirements. In chip-level design the additional delay incurred for proper pipeline clocking can be minimized by employing latches similar to the Earle latch. The partitioning of a computation into balanced pipeline stages constitutes the first challenge of pipelined design. The goal is to achieve stages as balanced as possible to minimize internal fragmentation.

Internal fragmentation due to imperfectly balanced pipeline stages is the primary cause of deviation from the first point of pipe lining idealism. This deviation becomes a form of pipelining overhead and leads to the shortfall from the idealized k-fold increase of throughput in a k-stage pipelined design.



#### 2.1.3.2 Identical Computations.  
The second point of pipelining idealism states that many repetitions of the same computation are to be performed by the pipeline.
The same computation is repeated on multiple sets of input data; each repetition requires the same sequence of subcomputations provided by the pipeline stages.
For our floating-point multiplier example, this means that many pairs of floatingpoint numbers are to be multiplied and that each pair of operands is sent through the same three pipeline stages. Basically this assumption implies that all the pipeline stages are used by every repetition of the computation. This is certainly true for our example.

This assumption holds for the floating-point multiplier example because this pipeline performs only one function, that is, floating-point multiplication. If a pipeline is designed to perform multiple functions, this assumption may not hold. For example, an arithmetic pipeline can be designed to perform both addition and multiplication. In a multiple-function pipeline, not all the pipeline stages may be required by each of the functions supported by the pipeline. It is possible that a different subset of pipeline stages is required for performing each of the functions and that each computation may not require all the pipeline stages. Since the sequence of data sets traverses the pipeline in a synchronous manner, some data sets will not require some pipeline stages and effectively will be idling during those stages. These unused or idling pipeline stages introduce another form of pipeline inefficiency that can be called external fragmentation of pipeline stages. Similar to internal fragmentation , external fragmentation is a form of pipelining overhead and should be minimized in multifunction pipelines. For the pipelined floating-point multiplier example, there is no external fragmentation.

The second point of pipelining idealism effectively assumes that all pipeline stages are always utilized. Aside from the implication of having no external fragmentation, this idealized assumption also implies that there are many sets of data to be processed. It takes k cycles for the first data set to reach the last stage of the pipeline; these cycles are referred to as the pipeline fill time. After the last data set has entered the first pipeline stage, an additional k cycles are needed to drain the pipeline. During pipeline fill and drain times, not all the stages will be busy.

The main reason for assuming the processing of many sets of input data is that the pipeline fill and drain times constitute a very small fraction of the total time.
Hence, the pipeline stages can be considered, for all practical purposes, to be always busy. In fact, the throughput of 5.8 MFLOPS for the pipelined floatingpoint multiplier is based on this assumption.


#### 2.1.3.3 Independent Computations.  
The third point of pipelining idealism states that the repetitions of computation, or simply computations, to be processed by the pipeline are independent. This means that all the computations that are concurrently resident in the pipeline stages are independent, that is, have no data or control dependences between any pair of the computations. This assumption permits the pipeline to operate in "streaming" mode, in that a later computation need not wait for the completion of an earlier computation due to a dependence between them. For our pipelined floating-point multiplier this assumption holds. If there are multiple pairs of operands to be multiplied, the multiplication of a pair of operands does not depend on the result from another multiplication. These pairs can be processed by the pipeline in streaming mode.

For some pipelines this point may not hold. A later computation may require the result of an earlier computation. Both of these computations can be concurrently resident in the pipeline stages. If the later computation has entered the pipeline stage that needs the result while the earlier computation has not reached the pipeline stage that produces the needed result, the later computation must wait in that pipeline stage. This waiting is referred to as a pipeline stall. If a computation is stalled in a pipeline stage, all subsequent computations may have to be stalled as well.

Pipeline stalls effectively introduce idling pipeline stages, and this is essentially a dynamic form of external fragmentation and results in the reduction of pipeline throughput. In designing pipelines that need to process computations that are not necessarily independent, the goal is to produce a pipeline design that minimizes the amount of pipeline stalls.



### 2.1.4 Instruction Pipelining

The three points of pipelining idealism are three idealized assumptions about pipelined designs. For the most part, in arithmetic pipelines the reality is not far from these idealized assumptions. However, for instruction pipelining the gap between realism and idealism is greater. It is the bridging of this gap that makes instruction pipelining interesting and challenging. In designing pipelined processors, these three points become the three major challenges. These three challenges are now briefly introduced and will be addressed in depth in Section 2.2 on pipelined processor design. These three challenges also provide a nice road map for keeping track of all the pipelined processor design techniques.



#### 2.1.4.1 Instruction Pipeline Design.  
The three points of pipelining idealism become the objectives, or desired goals, for designing instruction pipelines. The processing of an instruction becomes the computation to be pipelined. This computation must be partitioned into a sequence of fairly uniform subcomputations that will result in fairly balanced pipeline stages. The latency for processing an instruction is referred to as the instruction cycle; the latency of each pipeline stage determines the machine cycle. The instruction cycle can be viewed as a logical concept that specifies the processing of an instruction. The execution of a program with many instructions involves the repeated execution of this computation. The machine cycle is a physical concept that involves the clocking of storage elements in digital logic circuits, and it is essentially the clocking period of the pipeline stages.

We can view the earlier floating-point multiplier as an example of a very simple processor with only one instruction, namely, floating-point multiply. The instruction cycle involves the performance of a floating-point multiply; see Figure 2.6(a).
This computation can be naturally partitioned, based on obvious functional unit boundaries, into the following five subcomputations.

1. Partial product generation (125 ns).
2. Partial product reduction (150 ns).
3. Final reduction (55 ns).
4. Normalization (20 ns).
5. Rounding (50 ns).

For the purpose of pipelining, we had grouped the last three subcomputations into one subcomputation. This resulted in the three pipeline stages shown in Figure 2.6(b).
The instruction cycle of Figure 2.6(a) has been mapped into the three machine cycles of Figure 2.6(b), resulting in a three-stage pipelined design. We can refer to the instruction cycle as an architected (logical) primitive which is specified in the instruction set architecture, whereas the machine cycle is a machine (physical) primitive and is specified in the microarchitecture. The pipelined design of Figure 2.6(b) is an implementation of the architecture specified in Figure 2.6(a).

A main task of instruction pipelining can be stated as the mapping of the logical instruction cycle to the physical machine cycles. In other words, the computation represented by the instruction cycle must be partitioned into a sequence of subcomputations to be carried out by the pipeline stages. To perform this mapping or partitioning effectively, the three points of pipelining idealism must be considered.

**Uniform Subcomputations.** The partitioning of the instruction cycle to multiple machine cycles can be called stage quantization, and it should be performed to minimize internal fragmentation of the pipeline stages. If care is not taken in stage quantization, the internal fragmentation introduced can quickly undermine the efficiency of the pipeline. This first point of pipelining idealism leads to the first challenge of instruction pipelining, namely, the need to balance the pipeline stages. The more balanced the pipeline stages are, the less will be the internal fragmentation.

**Identical Computations.** Unlike a single-function arithmetic pipeline, an instruction pipeline is inherently a multifunction pipeline, in that it must be able to process different instruction types. Different instruction types will require slightly different sequences of subcomputations and consequently different hardware resources. The second challenge of instruction pipelining involves the efficient coalescing or unifying of the different resource requirements of different instruction types. The pipeline must be able to support the processing of all instruction types, while minimizing unused or idling pipeline stages for each instruction type. This essentially is equivalent to minimizing the external fragmentation.

**Independent Computations.** Again, unlike an arithmetic pipeline that processes array data, an instruction pipeline processes instructions that are not necessarily independent of one another. Hence, the instruction pipeline must have built-in mechanisms to detect the occurrences of dependences between instructions and to ensure that such dependences are not violated. The enforcing of interinstruction dependences may incur penalties in the form of pipeline stalls. Recall that pipeline stalls are a dynamic form of external fragmentation which reduces the throughput of the pipeline. Therefore. the third challenge of instruction pipelining is the minimizing of pipeline stalls.



#### 2.1.4.2 Instruction Set Architecture Impacts.  
Before we address the three major challenges of instruction pipelining in earnest. it might be enlightening to briefly consider the impacts that instruction set architectures (IS As) can have on instruction pipelining. Again, the three points of pipe lining idealism are considered in turn.


**Uniform Subcomputations.** The first challenge of balancing the pipeline stages implies that a set of uniform subcomputations must be identified. Looking at all the subcomputations that are involved in the processing of an instruction, one must identify the one critical subcomputation that requires the longest latency and cannot be easily further partitioned into multiple finer subcomputations. In pipelined processor design, one such critical subcomputation is the accessing of main memory. Because of the disparity of speed between the processor and main memory, memory accessing can be the critical subcomputation. To support more efficient instruction pipelining, addressing modes that involve memory access should be minimized, and fast cache memories that can keep up with the processor speed should be employed.

**Identical Computations.** The second challenge of unifying the resource requirements of different instruction types is one of the primary motivations for the RISC architectures. By reducing the complexity and diversity of the different instruction types, the task of unifying different instruction types is made easier. Complex addressing modes not only require additional accesses to memory, but also increase the diversity of resource requirements. To unify all these resource requirements into one instruction pipeline is extremely difficult, and the resultant pipeline can become very inefficient for many of the instructions with less complex resource requirements. These instructions would have to pay the external fragmentation overhead in that they underutilize the stages in the pipeline. The unifying of instruction types for a pipelined implementation of a RISC architecture is clean and results in an efficient instruction pipeline with little external fragmentation.

**Independent Computations.** The third challenge of minimizing pipeline stalls due to interinstruction dependences is probably the most fascinating area of pipelined processor design. For proper operation, an instruction pipeline must detect and enforce interinstruction dependences. Complex addressing modes, especially those that involve memory accessing, can make dependence detection very difficult due to the memory reference specifiers. In general, register dependences are easier to check because registers are explicitly specified in the instruction. Clean and symmetric instruction formats can facilitate the decoding of the instructions and the detection of dependences. Both the detection and the enforcement of dependences can be done either statically at compile time or dynamically at run time. The decision of what to do at compile time vs. run time involves the definition of the dynamic-static interface (DSI). The placement of the DSI induces interesting and subtle tradeoffs. These tradeoffs highlight the intimate relationship between compilers and (micro)architectures and the importance of considering both in the design of processors.




## 2.2 Pipelined Processor Design

In designing instruction pipelines or pipelined processors, the three points of pipelining idealism manifest as the three primary design challenges. Dealing with these deviations from the idealized assumptions becomes the primary task in designing pipelined processors. The three points of pipelining idealism and the corresponding three primary challenges for pipelined processor design are as follows:

1. Uniform subcomputations => balancing pipeline stages 
2. Identical computations => unifying instruction types 
3. Independent computations => minimizing pipeline stalls 

These three challenges are addressed in turn in Subsections 2.2.1 to 2.2.3. These three challenges provide a nice framework for presenting instruction pipelining techniques. All pipelined processor design techniques can be viewed as efforts in addressing these three challenges.

### 2.2.1 Balancing Pipeline Stages

In pipelined processor design, the computation to be pipelined is the work to be done in each instruction cycle. A typical instruction cycle can be functionally partitioned into the following five generic subcomputations.

1. Instruction fetch (IF)
2. Instruction decode (ID)
3. Operand(s) fetch (OF)
4. Instruction execution (EX) 5. Operand store (OS)

A typical instruction cycle begins with the fetching of the next instruction to be executed, which is followed by the decoding of the instruction to determine the work to be performed by this instruction. Usually one or more operands are specified and need to be fetched. These operands can reside in the registers or in memory locations depending on the addressing modes used. Once the necessary operands are available, the actual operation specified by the instruction is performed. The instruction cycle ends with the storing of the result produced by the specified operation. The result can be stored in a register or in a memory location, again depending on the addressing mode specified. In a sequential processor, this entire sequence of subcomputations is then repeated for the next instruction. During these five generic subcomputations some side effects can also occur as part of the execution of this instruction. Usually these side effects take the form of certain modifications to the machine state. These changes to the machine state are referred to as side effects because these effects are not necessarily explicitly specified in the instruction. The implementation complexity and resultant latency for each of the five generic subcomputations can vary significantly depending on the actual ISA specified.



#### 2.2.1.1 Stage Quantization.  
One natural partitioning of the instruction cycle for pipelining is based on the five generic subcomputations. Each of the five generic subcomputations is mapped to a pipeline stage, resulting in a five-stage instruction pipeline; see Figure 2.7. We called this example pipeline the GENERIC (GNR) instruction pipeline. In the GNR pipeline, the logical instruction cycle has been mapped into five physical machine cycles. The machine cycles/instruction cycle ratio of 5 reflects the degree of pipelining and gives some indication of the granularity of the pipeline stages.

The objective of stage quantization is to partition the instruction cycle into balanced pipeline stages so as to minimize internal fragmentation in the pipeline stages.
Stage quantization can begin with the natural functional partition of the instruction cycle, for example, the five generic subcomputations. Multiple subcomputations with short latencies can be grouped into one new subcomputation to achieve more balanced stages. For example, the three subcomputations-final reduction, normalization, and rounding-of the floating-point multiplication computation are grouped I. Instruction into one subcomputation in the pipelined design of Figure 2.6(b). Similarly, some of the five generic subcomputations of a typical instruction cycle can be grouped to achieve more balanced stages. For example, if an instruction set architecture employs fixed instruction length, simple addressing modes, and orthogonal fields in the instruction format, then both the IF and ID sUbcomputations should be quite straightforward and relatively simple compared to the other three subcomputations. These two subcomputations can potentially be combined into one new subcomputation, resulting in four subcomputations that are more balanced in terms of their required latencies. Based on these four subcomputations a four-stage instruction pipeline can be implemented; see Figure 2.8(a). In fact, the combining of the IF and ID subcomputations is employed in the MIPS R2000IR3000 pipelined processors [Moussouris et aI., 1986, Kane, 1987]. This approach essentially uses the subcomputation with the longest latency as a reference and attempts to group other subcomputations with shorter latencies into a new subcomputation with comparable latency as the reference. 
This will result in a coarser-grained machine cycle and a lower degree of pipelining.

Instead of combining subcomputations with short latencies, an opposite approach can be taken to balance the pipeline stages. A given subcomputation with extra-long latency can be further partitioned into multiple subcomputations of shorter latencies. This approach uses the subcomputation with the shortest latency as the reference and attempts to subdivide long-latency subcomputations into many finergrained subcomputations with latencies comparable to the reference. This will result in a finer-grained machine cycle and a higher degree of pipelining. For example, if an ISA employs complex addressing modes that may involve accessing the memory for both the OF and OS subcomputations, these two subcomputations can incur long latencies and can therefore be further subdivided into multiple subcomputations.
Additionally, some operations to be petformed in the EX subcomputation may be quite complex and can be further subdivided into multiple subcomputations as well.
Figure 2.8(b) illustrates such an instruction pipeline with an II-stage design. Both the OF and OS subcomputations are mapped into three pipeline stages, while the IF and EX subcomputations are mapped into two pipeline stages. Essentially, the ID subcomputation is used as the reference to achieve balanced stages.

The two methods presented for stage quantization are (1) merge multiple subcomputations into one and (2) subdivide a subcomputation into multiple subcomputations. A combination of both methods can also be used in the design of an instruction pipeline. As shown in the previous discussion, the instruction set architecture can have a significant impact on stage quantization. In all cases, the goal of stage quantization is to minimize the overall internal fragmentation. For example, assume that the total latency for the five generic subcomputations is 280 ns and that the resultant machine cycle times for the 4-stage design of Figure 2.8(a) and the ll-stage design of Figure 2.8(b) are 80 and 30 ns, respectively. Consequently, the total latency for the 4-stage pipeline is 320 ns (SO ns x 4) and the total latency for the ll-stage pipeline is 330 ns (30 ns x II). The difference between the new total latency and the original total latency of 2S0 ns represents the internal fragmentation.
Hence, the Internal fragmentation for the 4-stage design is 40 ns (320 ns - 2S0 ns), and the internal fragmentation for the II-stage design is 50 ns (330 ns - 2S0 ns). It can be concluded that the 4-stage design is more efficient than the II-stage design in terms of incurring less overhead due to internal fragmentation. Of course, the II-stage design yields a throughput that is 9.3 (2S0 ns / 30 ns) times that of a nonpipelined design, while the 4-stage design's throughput is only 3.5 (2S0 nsf SO ns) times that of a nonpipelined design. As can be seen in both designs, the internal fragmentation has hindered the attainment of the idealized throughput increase by factors of 11 and 4 for the II-stage and the 4-stage pipelines, respectively.



#### 2.2.1.2 Hardware Requirements.  
In most realistic engineering designs, the goal is not simply to achieve the best possible performance, but to achieve the best performance/cost ratio. Hence, in addition to simply maximizing the throughput (performance) of an instruction pipeline, hardware requirements (cost) must be considered. In general, higher degrees of pipelining will incur greater costs in terms of hardware requirements. Clearly there is the added cost due to the additional buffering between pipeline stages. We have already seen in the model presented in Section 2.1.1.3 that there is a point beyond which further pipelining yields diminishing returns due to the overhead of buffering between pipeline stages. Besides this buffering overhead, there are other, and more significant, hardware requirements for highly pipelined designs.

In assessing the hardware requirements for an instruction pipeline, the first thing to keep in mind is that for a k-stage instruction pipeline, in the worst case, or actually best case in terms of performance, there are k instructions concurrently present in the pipeline. There will be an instruction resident in each pipeline stage, with a total of k instructions all in different phases of the instruction cycle. Hence, the entire pipeline must have enough hardware to support the concurrent processing of k instructions in the k pipeline stages. The hardware requirements fall into three categories: (1) the logic required for control and data manipulation in each stage, (2) register-file ports to support concurrent register accessing by multiple stages, and (3) memory ports to support concurrent memory accessing by mUltiple stages.

We first examine the four-stage instruction pipeline of Figure 2.S(a). Assuming a load/store architecture, a typical register-register instruction will need to read the two register operands in the first stage and store the result back to a register in the fourth stage. A load instruction will need to read from memory in the second stage, while a store instruction will need to write to memory in the fourth stage. Combining the requirements for all four stages, a register file with two read ports and one write port will be required, and a data memory interface capable of performing one memory read and one memory write in every machine cycle will be required. In addition, the first stage needs to read from the instruction memory in every cycle for instruction fetch . If a unified (instruction and data) memory is used, then this memory must be able to support two read accesses and one write access in every machine cycle.

Similar analysis of hardware requirements can be performed for the II-stage instruction pipeline of Figure 2.8(b). To accommodate slow instruction memory, the IF generic subcomputation is subdivided and mapped to two pipeline stages, namely, the IFI and IF2 stages. Instruction fetch is initiated in IFI and completes in IF2. Even though instruction fetch takes two machine cycles, it is pipelined; that is, while the first instruction is completing the fetching in IF2, the second instruction can begin fetching in IFl. This means that the instruction memory must be able to support two concurrent accesses, by IFl and IF2 pipeline stages, in every machine cycle. Similarly, the mapping of both the OF and OS generic subcomputations to three pipeline stages each implies that at anyone time there could be up to six instructions in the pipeline, all in the process of accessing the data memory. Hence, the data memory must be able to support six independent concurrent accesses without conflict in every machine cycle. This can potentially require a six-ported data memory. Furthermore, if the instruction memory and the data memory are unified into one memory unit, an eight-ported memory unit can potentially be required.
Such multiported memory units are extremely expensive to implement. Less expensive solutions, such as using interleaved memory with multiple banks, that attempt to simulate true multiported functionality usually cannot guarantee conflictfree concurrent accesses at all times.

As the degree of pipelining, or the pipeline depth, increases, the amount of hardware resources needed to support such a pipeline increases significantly. The most significant increases of hardware resources are the additional ports to the register file(s) and the memory unites) needed to support the increased degree of concurrent accesses to these data storage units. Furthermore, to accommodate long memory access latency, the memory access subcomputation must be pipelined. However, the physical pipelining of memory accessing beyond two machine cycles can become quite complex, and frequently conflict-free concurrent accesses must be compromised.



#### 2.2.1.3 Example Instruction Pipelines.  
The stage quantization of two commercial pipelined processors is presented here to provide illustrations of real instruction pipelines. The MIPS R20001R3000 RISe processors employ a five-stage instruction pipeline, as shown in Figure 2.9(a). The MIPS architecture is a load/store architecture. The IF and ID generic subcomputations are merged into the IF stage, which will require one memory (I-cache) read in every machine cycle. The OF generic subcomputation is carried out in both the RD and MEM stages. For ALU instructions that access only register operands, operand fetch is done in the RD stage and requires the reading of two registers. For load instructions, the operand fetch also requires accessing the memory (D-cache) and is carried out in the MEM stage, which is the only stage in the pipeline that can access the D-cache. The OS generic subcomputation is carried out in the MEM and WB stages. Store instructions must access the D-cache and are done in the MEM stage. ALU and load instructions write their results back to the register file in the WB stage.

MIPS processors normally employ separate instruction and data caches. In every machine cycle the R2000/R3000 pipeline must support  the concurrent access of one I-cache read by the IF stage and one D-cache read (for a load instruction) or write (for a store instruction) by the MEM stage. Note that with the split cache configuration, both the I-cache and the D-cache need not be multiported. On the other hand, if both instructions and data are stored in the same cache, the unified cache will need to be dual-ported to support this pipeline. The register file must provide adequate ports to support two register reads by the RD stage and one register write by the WB stage in every machine cycle.

Figure 2.9(b) illustrates the 12-stage instruction pipeline of the AMDAHL 470V17. The IF generic subcomputation is implemented in the first three stages.
Because of the complex addressing modes that must be supported, the OF generic subcomputation is mapped into four stages. Both the EX and OS generic subcomputations are partitioned into two pipeline stages. In stage 1 of this 12-stage pipeline, the address of the next sequential instruction is computed. Stage 2 initiates cache access to read the instruction; stage 3 loads the instruction from the cache into the I-unit (instruction unit). Stage 4 decodes the instruction. Two general-purpose registers are read during stage 5; these registers are used as address registers. Stage 6 computes the address of an operand in memory. Stage 7 initiates cache access to read the memory operand; stage 8 loads the operand from the cache into the I-unit and also reads register operands. Stages 9 and 10 are the two execute stages in the E-unit (execute unit). In Stage 11 error checking is performed on the computed result. The final result is stored into the destination register in stage 12.

This 12-stage pipeline must support the concurrent accesses of two register reads by stage 5 and one register write by stage 12 in every machine cycle, along with four cache memory reads by stages 2, 3, 7, and 8 in every machine cycle. The memory subsystem of this pipelined processor is clearly much more complicated than that of the MIPS R2000/R3000 pipeline.

The current trend in pipelined processor design is toward higher degrees of pipelining with deeper pipeline depth. This produces finer-grained pipelined stages that can be clocked at higher rates. While four or five stages are common in firstgeneration pipelined RISC processors, instruction pipelines with more than ten stages are becoming commonplace. There is also the trend toward implementing multiple pipelines with different numbers of stages. This is the subject of superscalar processor design, which will be addressed in Chapter 4.



### 2.2.2 Unifying Instruction Types The second point of pipelining idealism assumes that the same computation is to be performed repeatedly by the pipeline. 
For most instruction pipelines, this idealized assumption of repetition of identical computations does not hold. While the instruction pipeline repeatedly processes instructions, there are different types of instructions involved. Although the instruction cycle is repeated over and over, repetitions of the instruction cycle may involve the processing of different instruction types.

Different instruction types have different resource requirements and may not require the exact same sequence of subcomputations. The instruction pipeline must be able to support the different requirements and must provide a superset of all the subcomputations needed by all the instruction types. Each instruction type may not require all the pipeline stages in the instruction pipeline. For each instruction type, the unnecessary pipeline stages become a form of inefficiency or overhead for that instruction type; such inefficiency or overhead has been referred to as external fragmentation of the pipeline in Section 2.1.3.2. The goal for unifying instruction types, the key challenge resulting from the second point of pipelining idealism, is to minimize the external fragmentations for all the instruction types.



#### 2.2.2.1 Classification of Instruction Types.  

To perform a computation, a computer must do three generic tasks:

1. Arithmetic operation
2. Data movement
3. Instruction sequencing

These three generic tasks are carried out by the processing of instructions in the processor. The arithmetic operation task involves the performing of arithmetic and logical operations on specified operands. This is the most obvious part of performing a computation and has often been equated to computation. A processor can support a large variety of arithmetic operations. The data movement task is responsible for moving the operands and the results between storage locations. Typically there is a hierarchy of storage locations, and explicit instructions are used to move the data among these locations. The instruction sequencing task is responsible for the sequencing of instructions. Typically a computation is specified in a program consisting of many instructions. The performance of the computation involves the processing of a sequence of instructions. This sequencing of instructions, or the program flow, can be explicitly specified by the instructions themselves.

How these three generic tasks are assigned to the various instructions of an ISA is a key component of instruction set design. A very complex instruction can be specified that actually performs all three of these generic tasks. In a typical horizontally microcoded machine, every microinstruction has fields that are used to specify all three of these generic tasks. In more traditional instruction set architectures known as complex instruction set computer (CISC) architectures, many of the instructions carry out more than one of these three generic tasks.

Influenced by the RISC research of the 1980s, most recent instruction set architectures all share some common attributes. These recent architectures include Hewlett-Packard's Precision architecture, IBM's Power architecture, IBM/Motorola' s PowerPC architecture, and Digital's Alpha architecture. These modern IS As tend to have fixed-length instructions, symmetric instruction formats, load/store architectures, and simple addressing modes. Most of these attributes are quite compatible with instruction pipelining. For the most part, this book adopts and assumes such a typical RISC architecture in its examples and illustrations.

In a typical modem RISC architecture, the instruction set employs a dedicated instruction type for each of the three generic tasks; each instruction only carries out one of the three generic tasks. Based on the three generic tasks, instructions can be classified into three types:

1. ALU instructions. For performing arithmetic and logical operations.
2. Load/store instructions. For moving data between registers and memory locations.
3. Branch instructions. For controlling instruction sequencing.

ALU instructions perform arithmetic and logical operations strictly on register operands. Only load and store instructions can access the data memory. Both the load/store and branch instructions employ fairly simple addressing modes. Typically only register-indirect with an offset addressing mode is supported. Often PC-relative addressing mode is also supported for branch instructions. In the following detailed specification of the three instruction types, the use of an instruction cache (I-cache) and a data cache (D-cache) is also assumed.

The semantics of each of the three instruction types can be specified based on the sequence of subcomputations performed by that instruction type. This specification can begin with the five generic subcomputations (Section 2.2.1) with subsequent further refinements. Eventually, these subcomputations specify the sequence of register transfers used for hardware implementation. For convenience, ALU instructions are further divided into integer and floating-point instructions. The semantics of the ALU instructions are specified in Table 2.2.

In a load/store architecture, load and store instructions are the only instructions that access the data memory. A load instruction moves data from a memory location into a register; a store instruction moves data from a register to a memory location. In the specification of the semantics of load and store instructions in Table 2.3, it is assumed that the only addressing mode is register-indirect with an offset. This addressing mode computes an effective address by adding the content of a register with the offset specified in the immediate field of the instruction.

Comparing the specifications in Tables 2.2 and 2.3, we can observe that the sequences of subcomputations required for ALU and load/store instruction types are similar but not exactly the same. ALU instructions need not generate memory addresses. On the other hand, load/store instructions, other than having to generate the effective address, do not have to perform explicit arithmetic or logical operations. They simply move data between registers and memory locations. Even between load and store instructions there are subtle differences. For the load instruction, the OF generic subcomputation expands into three sUbcomputations involving accessing the register file for the base address, generating the effective address, and accessing the memory location. Similarly for the store instruction, the OS generic subcomputation consists of two subcomputations involving generating the effective address and storing a register operand into a memory location. This assumes that the base address and the register operand are both accessed from the register file during the OF generic subcomputation.

Finally the sequences of subcomputations that specify the unconditional jump and the conditional branch instructions are presented in Table 2.4. A similar addressing mode as that for the load/store instructions is employed for the branch instructions. A PC-relative addressing mode can also be supported. In this addressing mode, the address of the target of the branch (or jump) instruction is generated by adding a displacement to the current content of the program counter. Typically this displacement can be either a positive or a negative value, to facilitate both forward and backward branches.

Examining the specifications of the three major instruction types in Tables 2.2 to 2.4, we see that the initial subcomputations for all three types are quite similar.
However, there are differences in the later subcomputations. For example, ALU instructions do not access data memory, and hence for them no memory address generation is needed. On the other hand, load/store and branch instruction types share the same required subcomputation of effective address generation. Load/ store instructions must access the data memory, while branch instructions must provide the address of the target instruction. We also see that for a conditional branch instruction, in addition to generating the effective address, evaluation of the branch condition must be performed. This can involve simply the checking of a status bit generated by an earlier instruction, or it can require the performance of an arithmetic operation on a register operand as part of the processing of the branch instruction, or it can involve checking the value of a specified register.

Based on the foregoing specifications of the instruction semantics, resource requirements for the three major instruction types can be determined. While the three instruction types share some commonality in terms of the fetching and decoding of the instructions, there are differences between the instruction types. These differences in the instruction semantics will lead to differences in the resource requirements.




#### 2.2.2.2 Coalescing of Resource Requirements.  
The challenge of unifying the different instruction types involves the efficient coalescing of the different resource requirements into one instruction pipeline that can accommodate all the instruction types. The objective is to minimize the total resources required by the pipeline and at the same time maximize the utilization of all the resources in the pipeline. The procedure for unifying different instruction types can be informally stated as consisting of the following three steps.

1. Analyze the sequence of subcomputations of each instruction type, and 
determine the corresponding resource requirements.
2. Find commonality between instruction types, and merge common subcomputations to share the same pipeline stage.
3. If there exists flexibility, without violating the instruction semantics, shift or reorder the subcomputations to facilitate further merging.

This procedure of unifying instruction types can be illustrated by applying it to the instruction types specified in Tables 2.2 to 2.4. For simplicity and clarity, floatingpoint instructions and unconditional jumps are not considered. Summary specifications of the ALU, load, store, and branch instruction types are repeated in Figure 2.10. The four sequences of subcomputations required by these four instruction types are taken from Tables 2.2 to 2.4 and are summarized in the four columns on the left-hand side of Figure 2.10. We now apply the unifying procedure from the top down, by examining the four sequences of subcomputations and the associated ALU hardware resources required to support them. This procedure results in the definition of the stages of an instruction pipeline.

All four instruction types share the same common subcomputations for IF and ID. Hence, the first two subcomputations for all four instruction types can be easily merged and used to define the first two pipeline stages, labeled IF and ID, for instruction fetching and instruction decoding.
All four instruction types also read from the register file for the OF generic subcomputation. ALU instructions access the two register operands. Load and branch instructions access a register to obtain the base address. Store instructions access a register to obtain the register operand and another register for the base address. In all four cases either one or two registers are read. These similar subcomputations can be merged into the third stage of the pipeline, called RD, for reading up to two registers from the register file. The register file must be capable of supporting two independent and concurrent reads in every machine cycle.

ALU instructions require an ALU functional unit for performing the necessary arithmetic and logical operations. While load, store, and branch instructions do not need to perform such operations, they do need to generate an effective address for accessing memory. It can be observed that the address generation task can be performed by the ALU functional unit. Hence, these subcomputations can be merged into the fourth stage of the pipeline, called ALU, which consists primarily of the ALU functional unit for performing arithmetic/logical operations or effective address generation.

Both the load and store instruction types need to access the data memory.
Hence a pipeline stage must be devoted to this subcomputation. The fifth stage of the pipeline, labeled MEM, is included for this purpose.

Both the ALU and load instruction types must write a result back to the register file as their last subcomputation. An ALU instruction writes the result of the operation performed on the register operands into a destination register. A load instruction loads into the destination register the data fetched from memory. No memory access is required by an ALU instruction; hence, the writing back to the destination register can theoretically take place immediately after the ALU stage. However, for the purpose of unifying with the register write-back subcomputation of the load instruction type, the register write-back subcomputation for ALU instructions is delayed by one pipeline stage and takes place in the sixth pipeline stage, named WB. This incurs one idle machine cycle for ALU instructions in the MEM pipeline stage. This is a form of external fragmentation and introduces some inefficiency in the pipeline.

For conditional branch instructions, the branch condition must be determined prior to updating the program counter. Since the ALU functional unit is used to perform effective address generation, it cannot be used to perform the branch condition evaluation. If the branch condition evaluation involves only the checking of a register to determine if it is equal to zero, or if it is positive or negative, then only a simple comparator is needed. This comparator can be added, and the earliest pipeline stage in which it can be added is the ALU stage, that is, after the reference register is read in the RD stage. Hence, the earliest pipeline stage in which the program counter can be updated with the branch target address, assuming the conditional branch is taken, is during the MEM stage, that is, after the target address is computed and the branch condition is determined in the ALU stage.

The foregoing coalescing of resource requirements for the different instruction types resulted in the six-stage instruction pipeline shown in the right-hand side of Figure 2.10. This instruction pipeline is identified as the TYPICAL (TYP) instruction pipeline and is used in the remainder of this chapter as an illustration vehicle. Other than the one idling pipeline stage (MEM) for ALU instructions, store and branch instruction types also incur some external fragmentation. Both store and branch instructions do not need to write back to a register and are idling during the WB stage. 
Overall this six-stage instruction pipeline is quite efficient. Load instructions use all six stages of the pipeline; the other three instruction types use five of the six stages.

In unifying different instruction types into one instruction pipeline, there are three optimization objectives. The first is to minimize the total resources required to support all the instruction types. In a way, the objective is to determine the pipeline that is analogous to the least common multiple of all the different resource requirements. The second objective is to maximize the utilization of all the pipeline stages by the different instruction types, in other words, to minimize the idling stages incurred by each instruction type. Idling stages lead to external fragmentation and result in inefficiency and throughput penalty. The third objective is to minimize the overall latency for each of the instruction types. Hence, if an idling stage is unavoidable for a particular instruction type and there is flexibility in terms of the placement of that idling stage, then it is always better to place it at the end of the pipeline. This will allow the instruction to effectively complete earlier and reduce the overall latency for that instruction type.



#### 2.2.2.3 Instruction Pipeline Implementation.  
In the six-stage TYP instruction pipeline (Figure 2.10), there are potentially six different instructions simultaneously present or "in flight" in the pipeline at anyone time. Each of the six instructions is going through one of the pipeline stages. The register file must support two reads (by the instruction in the RD stage) and one write (by the instruction in the WB stage) in every machine cycle. The I-cache must support one read in every machine cycle. Unless interrupted by a branch instruction, the IF stage continually increments the program counter and fetches the next sequential instruction from the I-cache. The D-cache must support one memory read or memory write in every machine cycle. Only the MEM stage accesses the D-cache; hence, at any time only one instruction in the pipeline can be accessing the data memory.

The pipeline diagram in Figure 2.10 is only a logical representation of the sixstage TYP instruction pipeline and illustrates only the ordering of the six pipeline stages. The actual physical organization of the TYP instruction pipeline is shown in Figure 2.11, which is the functional block diagram of the TYP pipelined processor implementation. In this diagram the buffers between the pipeline stages are explicitly identified. The logical buffer between two particular pipeline stages can actually involve multiple physical buffers distributed in this diagram. The single logical path that traverses the six pipeline stages in sequence actually involves mUltiple physical paths in this diagram. The progression of each instruction through the pipeline must be traced along these physical paths.

The physical organization of the six-stage TYP instruction pipeline in Figure 2.11 looks more complex than it really is. To help digest it, we can first examine the pipeline's interfaces to the register file and the memory subsystem.
Assuming a split cache organization, that is, separate caches for storing instructions and data, two single-ported caches, one I-cache and one D-cache, are needed.
The memory subsystem interface of the TYP pipeline is quite simple and efficient, and resembles most scalar pipelined processors. The IF stage accesses the I-cache, and the MEM stage accesses the D-cache, as shown in Figure 2.12. The I-cache can support the fetch of one instruction in every machine cycle; a miss in the I-cache will stall the pipeline. In the MEM stage of the pipeline, a load (store) instruction performs a read (write) from (to) the D-cache. Note that it is assumed here that the latency for accessing the D-cache, and the I-cache, is within one machine cycle.

As caches become larger and processor logic becomes more deeply pipelined, maintaining this one machine cycle latency for the caches will become more difficult.
The interface to the multiported register file is shown in Figure 2.13. Only the RD and the WB stages access the register file. In every machine cycle, the register file must support potentially two register reads by the RD stage and one register write by the WB stage. Hence, a multiported register file with two read ports and one write port is required. Such a register file is illustrated in Figure 2.13. It has three address ports, two data output ports, and one data input port for supporting two reads and one write in every machine cycle. The instruction that is performing the register write is in the WB stage and precedes the instruction that is performing the register reads in the RD stage by three machine cycles or intervening instructions. Consequently, there are three additional pipeline stage buffers at the register write address port to ensure that the register write address specifying the destination register to be written arrives at the register file write address port at exactly the same time as the data to be written are arriving at the input data port of the register file .

Three-ported register files are not very complex. However, as the number of ports increases beyond three, the hardware complexity increases very rapidly. This is especially true for increasing the number of write ports due to circuit design limitations. Multiported register files with up to 20 some ports are feasible and can be found in some high-end microprocessors.

If we look at the logical diagram of the six-stage TYP instruction pipeline of Figure 2.10, it appears that every instruction flows through the single linear path through the six pipeline stages. However, different sets of physical paths in the physical organization of the TYP instruction pipeline of Figure 2.11 are traversed by different instruction types. Some of the flow path segments are labeled in Figure 2.11 to show which pipeline stages they are associated with. Essentially some of the pipeline stages are physically distributed in the physical organization diagram of the pipeline.

The six-stage TYP instruction pipeline is quite similar to two other instruction pipelines, namely, the MIPS R20001R3000 and the instructional DLX processor used in the popular textbook by John Hennessy and David Patterson [2003]. Both are five-stage pipelines. The MIPS pipeline combines the IF and ID stages of the TYP pipeline into one pipeline stage. The DLX pipeline combines the ID and RD stages of the TYP pipeline into one pipeline stage. The other four stages are essentially the same for all three pipelines. The TYP pipeline is used in the remainder of this chapter as a running example.



### 2.2.3 Minimizing Pipeline Stalls 
The third point of pipelining idealism assumes that the computations that are performed by the pipeline are mutually independent. In a k-stage pipeline, there can be k different computations going on at anyone time. For an instruction pipeline, there can be up to k different instructions present or in flight in the pipeline at any one time. These instructions may not be independent of one another; in fact, usually there are dependences between the instructions in flight. Having independent instructions in the pipeline facilitates the streaming of the pipeline; that is, instructions move through the pipeline without encountering any pipeline stalls. When there are inter-instruction dependences, they must be detected and resolved. The resolution of these dependences can require the stalling of the pipeline. The challenge and design objective is to minimize such pipeline stalls and the resultant throughput degradation.



#### 2.2.3.1 Program Dependences and Pipeline Hazards.  At the ISA abstraction
level, a program is specified as a sequence of assembly language instructions. A typical instruction can be specified as a function i: T f- Slop S2, where the domain of instruction i is DO) = {SI, S2 j, the range is R(i) = {T j, and the mapping from the domain to the range is defined by op, the operation. Given two instructions i and j, with j following i in the lexical ordering of the two instructions, a data dependence can exist between i and j, or j can be data-dependent on i, denoted i <> j, if one of the following three conditions exists.

The first condition implies that instruction j requires an operand that is in the range of instruction i. This is referred to as the read-after-write (RAW) or true data dependence and is denoted i<>d j. The implication of a true data dependence is that instruction j cannot begin execution until instruction i completes. The second condition indicates that an operand required by i is in the range of j, or that instruction  j will modify the variable which is an operand of i. This is referred to as the writeafter-read (WAR) or anti data dependence and is denoted i <>a j. The existence of an anti-dependence requires that instruction j not complete prior to the execution of instruction i; otherwise, instruction i will get the wrong operand. The third condition indicates that both instructions i and j share a common variable in their range, meaning that both will modify that same variable. This is referred to as the writeafter-write (W AW) or output data dependence and is denoted i OaJ. The existence of an output dependence requires that instruction j not complete before the completion of instruction i; otherwise, instructions subsequent to j that have the same variable in their domains will receive the wrong operand. Clearly, the read-afterread case involves both instructions i and j accessing the same operand and is harmless regardless of the relative order of the two accesses.

These three possible ways for the domains and ranges of two instructions to overlap induce the three types of possible data dependences between two instructions, namely, true (RAW), anti (WAR), and output (WAW) data dependences. Since, in assembly code, the domains and the ranges of instructions can be variables residing in either the registers or memory locations, the common variable in a dependence can involve either a register or a memory location. We refer to them as register dependences and memory dependences. In this chapter we focus primarily on register dependences. Figure 2.14 illustrates the RA W, WAR, and W AW register data dependences.

Other than data dependences, a control dependence can exist between two instructions. Given instructions i and j, with j following i, j is control-dependent on i, denoted iOj, if whether instruction j is executed or not depends on the outcome of the execution of instruction i. Control dependences are consequences of the control flow structure of the program. A conditional branch instruction causes uncertainty on instruction sequencing. Instructions following a conditional branch can have control dependences on the branch instruction .

An assembly language program consists of a sequence of instructions. The semantics of this program assume and depend on the sequential execution of the instructions. The sequential listing of the instructions implies a sequential precedence between adjacent instructions. If instruction i is followed by instruction i + 1 in the program listing, then it is assumed that first instruction i is executed, and then instruction i + 1 is executed. If such sequential execution is followed, the semantic correctness of the program is guaranteed. To be more precise, since an instruction cycle can involve multiple subcomputations, the implicit assumption is that all the subcomputations of instruction i are carried out before any of the subcomputations of instruction i + 1 can begin. We called this the total sequential execution of the program; that is, all the subcomputations of the sequence of instructions are carried out sequentially.

Given a pipelined processor with k pipeline stages, the processing of k instructions is overlapped in the pipeline. As soon as instruction i finishes its first subcomputation and begins its second subcomputation, instruction i + 1 begins its first sUbcomputation. The k subcomputations, corresponding to the k pipeline stages, of a particular instruction are overlapped with subcomputations of other instructions. 
Hence, the total sequential execution does not hold. While total sequential execution is sufficient to ensure semantic correctness, it is not a necessary requirement for semantic correctness. The total sequential execution implied by the sequential listing of instructions is an overspecification of the semantics of a program. The essential requirement in ensuring that the program semantics are not violated is that all the inter-instruction dependences not be violated. In other words, if there exists a dependence between two instructions i and j, with j following i in the program listing, then the reading/writing of the common variable by instructions i and  j must occur in original sequential order. In pipelined processors, if care is not taken, there is the potential that program dependences can be violated. Such potential violations of program dependences are called pipeline hazards. All pipeline hazards must be detected and resolved for correct program execution.



#### 2.2.3.2 Identification of Pipeline Hazards.  
Once all the instruction types are unified into an instruction pipeline and the functionality for all the pipeline stages is defined, analysis of the instruction pipeline can be performed to identify all the pipeline hazards that can occur in that pipeline. Pipeline hazards are consequences of both the organization of the pipeline and inter-instruction dependences. The focus of this chapter is on scalar instruction pipelines. By definition , a scalar instruction pipeline is a single pipeline with multiple pipeline stages organized in a linear sequential order. Instructions enter the pipeline according to the sequential order specified by the program listing. Except when pipeline stalls occur, instructions flow through a scalar instruction pipeline in the lockstep fashion; that is, each instruction advances to the next pipeline stage with every machine cycle. For scalar instruction pipelines, necessary conditions on the pipeline organi zation for the occurrence of pipeline hazards due to data dependences can be determined.

A pipeline hazard is a potential violation of a program dependence. Pipeline hazards can be classified according to the type of program dependence involved.
A W AW hazard is a potential violation of an output dependence. A WAR hazard is a potential violation of an anti-dependence. A RAW hazard is a potential violation of a true data dependence. A data dependence involves the reading and/or writing of a common variable by two instructions. For a hazard to occur, there must exist at least two pipeline stages in the pipeline which can contain two instructions that can simultaneously access the common variable.

Figure 2.15 illustrates the necessary conditions on the pipeline organization for the occurrence of WAW, WAR, and RAW hazards. These necessary conditions apply to hazards caused by both memory and register data dependences (only register dependences are illustrated in the figure). In order for a WA W hazard to occur due to an output dependence (b a ), there must exist at least two pipeline stages that can perform two simultaneous writes to the common variable; see Figure 2.15(a). If only one stage in the pipeline can write to that variable, then no hazard can occur because both writes-in fact all writes-to that variable will be performed by that pipeline stage according to the original sequential order specified by the program listing.

Figure 2.15(b) specifies that in order for a WAR hazard to occur, there must exist at least two stages in the pipeline, with an earlier stage x and a later stage y, such that stage x can write to that variable and stage y can read that variable. In order for the anti-dependence iDa j to be violated, instruction j must perform the write, that is, reach stage x, prior to instruction i performing the read or reaching stage y. If this necessary condition does not hold, it is impossible for instruction j, a trailing instruction, to perform a write prior to instruction i completing its read. For example, if there exists only one pipeline stage that can perform both the read and write to that variable, then all accesses to that variable are done in the original sequential order, and hence no WAR hazard can occur. In the case where the stage performing the read is earlier in the pipeline than the stage performing the write, the leading instruction i must complete its read before the trailing instruction j can possibly perform the write in a later stage in the pipeline. Again, no WAR hazard can occur in such a pipeline. In actuality, the necessary conditions presented in Figure 2.15 are also sufficient conditions and can be considered as characterizing conditions for the occurrence ofWAW, WAR, and RAW pipeline hazards.

Figure 2.15(c) specifies that in order for a RAW hazard to occur due to a true data dependence iD d  j, there must exist two pipeline stages x and y, with x occurring earlier in the pipeline than y, such that stage x can perform a read and stage y can perform a write to the common variable. With this pipeline organization, the dependence iD,ti can be violated if the trailing instruction j reaches stage x prior to the leading instruction i reaching stage y. Arguments similar to that used for WAR hazards can be applied to show that if this necessary condition does not hold, then no RAW hazard can occur. For example, if only one pipeline stage performs all the reads and writes, then effectively total sequential execution is carried out and no hazard can occur. If the stage performing the read is positioned later in the pipeline than the stage performing the write, then RAW hazards can never occur; the reason is that all the writes of leading instructions will be completed before the trailing instructions perform their reads.

Since pipeline hazards are caused by potential violations of program dependences, a systematic procedure for identifying all the pipeline hazards that can occur in an instruction pipeline can be formulated by considering each dependence type in turn. The specific procedure employed in this chapter examines program dependences in the following order.

1. Memory data dependence
   1. Output dependence
   2. Anti-dependence
   3. True data dependence
2. Register data dependence
   1. Output dependence
   2. Anti-dependence
   3. True data dependence
3. Control dependence

We illustrate this procedure by applying it to the six-stage TYP instruction pipeline. First, memory data dependences are considered. A memory data dependence involves a common variable stored in memory that is accessed (either read or write) by two instructions. Given a load/store architecture, memory data dependences can only occur between load/store instructions. To determine whether pipeline hazards can occur due to memory data dependences, the processing of load/store instructions by the pipeline must be examined. Assuming a split cache design, in the TYP pipeline, only the MEM stage can access the D-cache. Hence, all accessing of memory locations by load/store instructions must and can only occur in the MEM stage; there is only one stage in the pipeline that performs reads and writes to the data memory. Based on the necessary conditions presented in Figure 2.15 no pipeline hazards due to memory data dependences can occur in the TYP pipeline. Essentially, all accesses to the data memory are performed sequentially, and the processing of all load/store instructions is done in the total sequential execution mode. Therefore, for the TYP pipeline, there are no pipeline hazards due to memory data dependences.

Register data dependences are considered next. To determine pipeline hazards that can occur due to register data dependences, all pipeline stages that can access the register file must be identified. In the TYP pipeline, all register reads occur in the RD stage and all register writes occur in the WB stage. An output (W AW) dependence, denoted iOoj, indicates that an instruction i and a subsequent instruction j both share the same destination register. To enforce the output dependence, instruction i must write to that register first ; then instruction j can write to that same register. In the TYP pipeline, only the WB stage can perform writes to the register file. Consequently, all register writes are performed in sequential order by the WB stage; and according to the necessary condition of Figure 2.15(a), no pipeline hazards due to output dependences can occur in the TYP pipeline.

An anti (WAR) dependence, denoted i Ba  j , indicates that instruction i is reading from a register that is the destination register of a subsequent instruction j. It must be ensured that instruction i reads that register before instruction j writes into that register. The only way that an anti-dependence can cause a pipeline hazard is if the trailing instruction j can perform a register write earlier than instruction i can perform its register read. This is an impossibility in the TYP pipeline because all register reads occur in the RD stage, which is earlier in the pipeline than the WB stage, the only stage in which register writes can occur. Hence, the necessary condition of Figure 2.l5(b) does not exist in the TYP pipeline. Consequently, no pipeline hazards due to anti-dependences can occur in the TYP pipeline.

The only type of register data dependences that can cause pipeline hazards in the TYP pipeline are the true data dependences. The necessary condition of Figure 2.l5(c) exists in the TYP pipeline because the pipeline stage RD that performs register reads is positioned earlier in the pipeline than the WB stage that performs register writes. A true data dependence, denoted iB d  j, involves instruction i writing into a register and a trailing instruction j reading from that same register. If instruction j immediately follows instruction i, then when j reaches the RD stage, instruction i will still be in the ALU stage. Hence, j cannot read the register operand that is the result of instruction i until i reaches the WB stage. To enforce this data dependence, instruction j must be prevented from entering the RD stage until instruction i has completed the WB stage. RAW pipeline hazards can occur for true data dependences because a trailing instruction can reach the register read stage in the pipeline prior to the leading instruction completing the register write stage in the pipeline.

Finally, control dependences are considered. Control dependences involve control flow changing instructions, namely, conditional branch instructions. The outcome of a conditional branch instruction determines whether the next instruction to be fetched is the next sequential instruction or the target of the conditional branch instruction. Essentially there are two candidate instructions that can follow a conditional branch. In an instruction pipeline, under normal operation, the instruction fetch stage uses the content of the program counter to fetch the next instruction, and then increments the content of the program counter to point to the next sequential instruction.

This task is repeated in every machine cycle by the instruction fetch stage to keep the pipeline filled. When a conditional branch instruction is fetched, potential disruption of this sequential flow can occur. If the conditional branch is not taken, then the continued fetching by the instruction fetch stage of the next sequential instruction is correct. However, if the conditional branch is actually taken, then the fetching of the next sequential instruction by the instruction fetch stage will be incorrect. The problem is that this ambiguity cannot be resolved until the condition for branching is known.

A control dependence can be viewed as a form of register data (RA W) dependence involving the program counter (PC). A conditional branch instruction writes into the PC, whereas the fetching of the next instruction involves reading of the Pc.
The conditional branch instruction updates the PC with the address of the target instruction if the branch is taken; otherwise, the PC is updated with the address of the next sequential instruction. In the TYP pipeline, the updating of the PC with the target instruction address is performed in the MEM stage, whereas the IF stage uses the content of the PC to fetch the next instruction. Hence, the IF stage performs reads on the PC register, and the MEM stage which occurs later in the pipeline performs writes to the PC register. This ordering of the IF and MEM stages, according to Figure 2.1S(c), satisfies the necessary condition for the occurrence of RAW hazards involving the PC register. Therefore, a control hazard exists in the TYP pipeline, and it can be viewed as a form of RA W hazard involving the Pc.




#### 2.2.3.3 Resolution of Pipeline Hazards.  
Given the organization of the TYP pipeline, the only type of pipeline hazards due to data dependences that can occur are the RAW hazards. In addition, pipeline hazards due to control dependences can occur. All these hazards involve a leading instruction i that writes to a register (or PC) and a trailing instruction j that reads that register. With the presence of pipeline hazards, mechanisms must be provided to resolve these hazards, that is, ensure that the corresponding data dependences are not violated. With regard to each RA W hazard in the TYP pipeline, it must be ensured that the read occurs after the write to the common register, or the hazard register.

To resolve a RAW hazard, the trailing instruction j must be prevented from entering the pipeline stage in which the hazard register is read by j, until the leading instruction i has traversed the pipeline stage in which the hazard register is written by i. This is accomplished by stalling the earlier stages of the pipeline, namely all the stages prior to the stage performing a register read, thus preventing instruction j from entering the critical register read stage. The number of machine cycles by which instruction j must be held back is, in the worst case, equal to the distance between the two critical stages of the pipeline, that is, the stages performing read and write to the hazard register. In the case of the TYP pipeline, if the leading instruction i is either an ALU or a load instruction, the critical register write stage is the WB stage and the critical register read stage for all trailing instruction types is the RD stage. The distance between these two critical stages is three cycles; hence, the worst-case penalty is three cycles, as shown in Table 2.5. The worst-case penalty is incurred when instruction j immediately follows instruction i in the original program listing; that is, j is equal to i + 1. In this case, instruction j must be stalled for three cycles in the ID stage and is allowed to enter the RD stage three cycles later as instruction i exits the WB stage. If the trailing instructionj does not immediately follow instruction i, that is, if there are intervening instructions between i and j, then the penalty will be less than three cycles. It is assumed that the intervening instructions do not depend on instruction i. The actual number of penalty cycles incurred is thus equal to 3 - s, where s is the number of intervening instructions. For example, if there are three instructions between i and j, then no penalty cycle is incurred. In this case, instruction j will be entering the RD stage just as instruction i is exiting the WB stage, and no stalling is required to satisfy the RAW dependence.

For control hazards, the leading instruction i is a branch instruction, which updates the PC in the MEM stage. The fetching of the trailing instruction j requires the reading of the PC in the IF stage. The distance between these two stages is four cycles; hence, the worst-case penalty is four cycles. When a conditional branch instruction is encountered, all further fetching of instructions is stopped by stalling the IF stage until the conditional branch instruction completes the MEM stage in which the PC is updated with the branch target address. This requires stalling the IF stage for four cycles. Further analysis reveals that this stalling is only necessary if the conditional branch is actually taken. If it turns out that the conditional branch is not taken, then the IF stage could have continued its fetching of the next sequential instructions. This feature can be included in the pipeline design, so that following a conditional branch instruction, the instruction fetching is not stalled.

Effectively, the pipeline assumes that the branch will not be taken. In the event that the branch is taken, the PC is updated with the branch target in the MEM stage and all the instructions residing in earlier pipeline stages are deleted, or flushed, and the next instruction fetched is the branch target. With such a design, the four-cycle penalty is incurred only when the conditional branch is actually taken, and there is no penalty cycle otherwise.

Similar to RAW hazards due to register data dependence, the four-cycle penalty incurred by a control hazard can be viewed as the worst-case penalty. If instructions that are not control-dependent on instruction i can be inserted between instruction i and instruction j , the control-dependent instruction, then the actual number of penalty cycles incurred can be reduced by the number of instructions inserted. This is the concept of delayed branches. Essentially these penalty cycles are filled by useful instructions that must be executed regardless of whether the conditional branch is taken. The actual number of penalty cycles is 4 - s, where s is the number of control-independent instructions that can be inserted between instructions i and j. Delayed branches or the filling of penalty cycles due to branches makes it difficult to implement the earlier technique of assuming that the branch is not taken and allowing the IF stage to fetch down the sequential path.

The reason is that mechanisms must be provided to distinguish the filled instructions from the actual normal sequential instructions. In the event that the branch is actually taken, the filled instructions need not be deleted, but the normal sequential instructions must be deleted because they should not have been executed.



#### 2.2.3.4 Penalty Reduction via Forwarding Paths.  So far we have implicitly
assumed that the only mechanism available for dealing with hazard resolution is to stall the dependent trailing instruction and ensure that the writing and reading of the hazard register are done in their normal sequential order. More aggressive techniques are available in the actual implementation of the pipeline that can help reduce the penalty cycles incurred by pipeline hazards. One such technique involves the incorporation ofJorwarding paths in the pipeline.

With respect to pipeline hazards, the leading instruction i is the instruction on which the trailing instruction j depends. For RAW hazards, instruction j needs the result of instruction i for its operand. Figure 2.16 illustrates the processing of the leading instruction i in the case when i is an ALU instruction or a load instruction.

If the leading instruction i is an ALU instruction, the result needed by instruction j is actually produced by the ALU stage and is available when instruction i completes the ALU stage. In other words, the operand needed by instruction j is actually available at the output of the ALU stage when instruction i exits the ALU stage, and j need not wait two more cycles for i to exit the WB stage. If the output of the ALU stage can be made available to the input side of the ALU stage via a physical forwarding path, then the trailing instruction j can be allowed to enter the ALU stage as soon as the leading instruction i leaves the ALU stage. In this case, instruction j need not access the dependent operand by reading the register file in the RD stage; instead, it can obtain the dependent operand by accessing the output of the ALU stage. With the addition of this forwarding path and the associated control logic, the worst-case penalty incurred is now zero cycles when the leading instruction is an ALU instruction. Even if the trailing instruction is instruction i + 1, no stalling is needed because instruction i + 1 can enter the ALU stage as instruction i leaves the ALU stage just as a normal pipeline operation.

In the case that the leading instruction is a load instruction rather than an ALU instruction, a similar forwarding path can be incorporated to reduce the penalty cycles incurred due to a leading load instruction and a dependent trailing instruction. Examining Figure 2.16 reveals that if the leading instruction is a load instruction, the result of this load instruction, that is, the content of the memory location being loaded into the register, is available at the output of the MEM stage when the load instruction completes the MEM stage. Again, a forwarding path can be added from the output of the MEM stage to the input of the ALU stage to support the requirement of the trailing instruction. The trailing instruction can enter the ALU stage as soon as the leading load instruction completes the MEM stage. This effectively reduces the worst-case penalty due to a leading load instruction from three cycles down to just one cycle. In the worst case, the dependent instruction is instruction i + 1, i.e. , j = i + 1. In normal pipeline processing when instruction i is in the ALU stage, instruction i + 1 will be in the RD stage. When instruction i advances to the MEM stage, instruction i + 1 must be held back at the RD stage via stalling the earlier stages of the pipeline. However, in the next cycle when instruction i exits the MEM stage, with the forwarding path from the output of the MEM stage to the input of the ALU stage, instruction i + 1 can be allowed to enter the ALU stage. In effect, instruction i + 1 is only stalled for one cycle in the RD stage; hence the worst-case penalty is one cycle. With the incorporation of the forwarding paths the worst-case penalties for RAW hazards can be reduced as shown in Table 2.6.

The penalty due to a RAW hazard with an ALU instruction as the leading instruction is referred to as the ALU penalty. Similarly, the penalty due to a leading load instruction is referred to as the load penalty. For the TYP pipeline, with forwarding paths added, the ALU penalty is zero cycles. In effect, when the leading instruction is an ALU instruction, no penalty is incurred. Note that the source of the forwarding path is the output of the ALU stage, this being the earliest point where the result of instruction i is available. The destination of the forwarding path is the input to the ALU stage, this being the latest point where the result from instruction i is needed by instruction j. A forwarding path from the earliest point a result is available to the latest point that result is needed by a dependent instruction is termed the critical forwarding path, and it represents the best that can be done in terms of reducing the hazard penalty for that type of leading instruction.

In addition to the critical forwarding path, additional forwarding paths are needed. For example, forwarding paths are needed that start from the outputs of the MEM and WB stages and end at the input to the ALU stage. These two additional forwarding paths are needed because the dependent instruction j could potentially be instruction i + 2 or instruction i + 3. If j = i + 2, then when instruction j is ready to enter the ALU stage, instruction i will be exiting the MEM stage. Hence, the result of instruction i, which still has not been written back to the destination register and is needed by instruction j, is now available at the output of the MEM stage and must be forwarded to the input of the ALU stage to allow instruction j to enter that stage in the next cycle. Similarly, if j = i + 3, the result of instruction i must be forwarded from the output of the WB stage to the input of the ALU stage. In this case, although instruction i has completed the write back to the destination register, instruction j has already traversed the RD stage and is ready to enter the ALU stage.

Of course, in the case that j = i + 4, the RAW dependence is easily satisfied via the normal reading of the register file by j without requiring the use of any forwarding path. By the time j reaches the RD stage, i will have completed the WB stage.
If the leading instruction is a load instruction, the earliest point at which the result of instruction i is available is at the output of the MEM stage, and the latest point where this result is needed is at the input to the ALU stage. Hence the critical forwarding path for a leading load instruction is from the output of the MEM stage to the input of the ALU stage. This represents the best that can be done, and in this case the incurring of the one cycle penalty is unavoidable. Again, another forwarding path from the output of the WB stage to the input of the ALU stage is needed in case the dependent trailing instruction is ready to enter the ALU stage when instruction i is exiting the WB stage.

Table 2.6 indicates that no forwarding path is used to reduce the penalty due to a branch instruction. If the leading instruction i is a branch instruction and given the addressing mode assumed for the TYP pipeline, the earliest point where the result is available is at the output of the MEM stage. For branch instructions, the branch target address and the branch condition are generated in the ALU stage. It is not until the MEM stage that the branch condition is checked and that the target address of the branch is loaded into the Pc. Consequently, only after the MEM stage can the PC be used to fetch the branch target. On the other hand, the PC must be available at the beginning of the IF stage to allow the fetching of the next instruction.

Hence the latest point where the result is needed is at the beginning of the IF stage.
As a result the critical forwarding path, or the best that can be done, is the current penalty path of updating the PC with the branch target in the MEM stage and starting the fetching of the branch target in the next cycle if the branch is taken. If, however, the branch condition can be generated early enough in the ALU stage to allow updating the PC with the branch target address toward the end of the ALU stage, then in that case the branch penalty can be reduced from four cycles to three cycles.



#### 2.2.3.5 Implementation of Pipeline Interlock.  
The resolving of pipeline hazards via hardware mechanisms is referred to as pipeline interlock. Pipeline interlock hardware must detect all pipeline hazards and ensure that all the dependences are satisfied. Pipeline interlock can involve stalling certain stages of the pipeline as well as controlling the forwarding of data via the forwarding paths.

With the addition of forwarding paths, the scalar pipeline is no longer a simple sequence of pipeline stages with data flowing from the first stage to the last stage.
The forwarding paths now provide potential feedback paths from outputs of later stages to inputs of earlier stages. For example, the three forwarding paths needed to support a leading ALU instruction involved in a pipeline hazard are illustrated in Figure 2.l7 . These are referred to as ALU forwarding paths. As the leading ALU instruction i traverses down the pipeline stages, there could be multiple trailing instructions that are data (RAW) dependent on instruction i. The right side of Figure 2.17 illustrates how multiple dependent trailing instructions are satisfied during three consecutive machine cycles. During cycle tl, instruction i forwards its result to dependent instruction i + 1 via forwarding path a. During the next cycle, t2, instruction i forwards its result to dependent instruction i + 2 via forwarding path b. If instruction i + 2 also requires the result of instruction i + 1, this result can also be forwarded to i + 2 by i + 1 via forwarding path a during this cycle. During cycle t3 , instruction i can forward its result to instruction i + 3 via forwarding path c. Again, path a or path b can also be activated during this cycle if instruction i + 3 also requires the result of i + 2 or i + 1, respectively.



The physical implementation of the logical diagram of Figure 2.17 is shown in Figure 2.18. Note that RAW hazards are detected using comparators that compare the register specifiers of consecutive instructions. Four 5-bit (assuming 32 registers) comparators are shown in Figure 2.18. If the trailing instruction j is currently in the RD stage, that is, attempting to read its two register operands, then the first two comparators (to the left) are checking for possible RAW dependences between instructionj and instructionj - 1, which is now in the ALU stage. These two comparators are comparing the two source register specifiers of j with the destination register specifier of j - 1. At the same time the other two comparators (to the right) are checking for possible RAW dependences between j and j - 2, which is now in the MEM stage. These two comparators are comparing the two source register specifiers of j with the destination register specifier of j - 2. The outputs of these four comparators are used as control signals in the next cycle for activating the appropriate forwarding paths if dependences are detected.

Forwarding path a is activated by the first pair of comparators if any RAW dependences are detected between instructions j and j - 1. Similarly, forwarding path b is activated by the outputs of the second pair of comparators for satisfying any dependences between instructions j and j - 2. Both paths can be simultaneously activated if j depends on both j - 1 and j - 2.

Forwarding path c of Figure 2.17 is not shown in Figure 2.18; the reason is that this forwarding path may not be necessary if appropriate care is taken in the design of the multiported register file. If the physical design of the three-ported (two reads and one write) register file performs first the write and then the two reads in each cycle, then the third forwarding path is not necessary. Essentially instruction j will read the new, and correct, value of the dependent register when it traverses the RD stage. In other words, the forwarding is performed internally in the register file. There is no need to wait for one more cycle to read the dependent register or to forward it from the output of the WB stage to the input of the ALU stage. This is a reasonable design choice, which can reduce either the penalty cycle by one or the number of forwarding paths by one, and it is actually implemented in the MIPS R20001R3000 pipeline.

To reduce the penalty due to pipeline hazards that involve leading load instructions, another set of forwarding paths is needed. Figure 2.19 illustrates the two forwarding paths needed when the leading instruction involved in a pipeline hazard is a load instruction. These are referred to as loadfOlwarding paths. Forwarding path d forwards the output of the MEM stage to the input of the ALU stage, whereas path e forwards the output of the WB stage to the input of the ALU stage. When the leading instruction i reaches the ALU stage, if instruction i + 1 is dependent on instruction i, it must be stalled in the RD stage for one cycle. In the next cycle, when instruction i is exiting the MEM stage, its result can be forwarded to the ALU stage via path d to allow instruction i + 1 to enter the ALU stage. In case instruction i + 2 also depends on instruction i, the same result is forwarded in the next cycle via path e from the WB stage to the ALU stage to allow instruction i + 2 to proceed into the ALU stage without incurring another stall cycle. Again, if the multiported register file performs first the write and then the read, then forwarding path e will not be necessary. For example, instruction i + 2 will read the result of instruction i in the RD stage while instruction i is simultaneously performing a register write in the WB stage.

The physical implementation of all the forwarding paths for supporting pipeline hazards due to both ALU and load leading instructions is shown in Figure 2.20. Forwarding path e is not shown, assuming that the register file is designed to perform first the write and then the read in each cycle. Note that while both ALU forwarding path b and load forwarding path d are shown in Figure 2.17 and Figure 2.19, respectively, as going from the output of the MEM stage to the input of the ALU stage, these are actually two different physical paths, as shown in Figure 2.20. These two paths feed into the first pair of multiplexers, and only one of the two can be selected depending on whether the leading instruction in the MEM stage is an ALU or a load instruction.

Forwarding path b originates from the buffer in the MEM stage that contains the output of the ALU from the previous machine cycle. Forwarding path d originates from the buffer in the MEM stage that contains the data accessed from the D-cache.
The same two pairs of comparators are used to detect register dependences regardless of whether the leading instruction is an ALU or a load instruction. Two pairs of comparators are required because the interlock hardware must detect possible dependences between instructions i and i + 1 as well as between instructions i and i + 2. When the register file is designed to perform first a write and then a read in each cycle, a dependence between instructions i and i + 3 is automatically satisfied when they traverse the WB and RD stages, respectively. The output of the first pair of comparators is used along with a signal from the ID stage indicating that the leading instruction is a load to produce a control signal for stalling the first three stages of the pipeline for one cycle if a dependence is detected between instructions i and i + 1, and that instruction i is a load.

Pipeline interlock hardware for the TYP pipeline must also deal with pipeline hazards due to control dependences. The implementation of the interlock mechanism for supporting control hazards involving a leading branch instruction is shown in Figure 2.21. Normally, in every cycle the IF stage accesses the I-cache to fetch the next instruction and at the same time increments the PC in preparation for fetching the next sequential instruction. When a branch instruction is fetched in the IF stage and then decoded to be such an instruction in the 1D stage, the IF stage can be stalled until the branch instruction traverses the ALU stage, in which both the branch condition and the target address are generated. In the next cycle, corresponding to the MEM stage of the branch instruction, the branch condition is used to load the branch target address into the PC via the right side of the PC multiplexer if the branch is taken. This results in a four-cycle penalty whenever a branch instruction is encountered. Alternatively, the branch instruction can be assumed to be not taken, and the IF stage continues to fetch subsequent instructions along the sequential path. In the case when the branch instruction is determined to be taken, the PC is updated with the branch target during the MEM stage of the branch instruction, and the sequential instructions in the IF, ID, RD, and ALU stages are invalidated and flushed from the pipeline. In this case, the fourcycle penalty is incurred only when the branch is actually taken. If the branch turns out to be not taken, then no penalty cycle is incurred.



### 2.2.4 Commercial Pipelined Processors 

Pipe lined processor design has become a mature and widely adopted technology.
The compatibility of the RISC philosophy with instruction pipelining is well known and well exploited. Pipe lining has also been successfully applied to CISC architectures. This subsection highlights two representative pipelined processors. The MIPS R20001R3000 pipeline is presented as representative of RISC pipeline processors [Moussouris et aI. , 1986; Kane, 1987]. The Intel i486 is presented as representative of CISC pipelined processors [Crawford, 1990]. Experimental data from an IBM study on RISC pipelined processors done by Tilak Agerwala and John Cocke in 1987 are presented as representative of the characteristics and the performance capabilities of scalar pipelined processors [Agerwala and Cocke, 1987].



#### 2.2.4.1 RISC Pipelined Processor Example.  
MIPS is a RISC architecture with 32-bit instructions. There are three different inst
Instructions can be divided into four types.
â€¢ Computational instructions perform arithmetic, logical, and shift operations on register operands. They can employ the R-type format if all the operands and the result are registers, or the I-type format if one of the operands is specified in the immediate field of the instruction.
â€¢ Load/store instructions move data between the memory and registers. They employ the I-type format. The only addressing mode is the base register plus the signed offset stored in the immediate field.
â€¢ Jump and branch instructions steer the control flow of the program. Jumps are unconditional and use the J-type format to jump to an absolute address composed of the 26-bit target and the high-order 4 bits of the Pc. Branches are conditional and use the I-type format to specify the target address as the PC plus the 16-bit offset in the immediate field.

â€¢ Other instructions in the instruction set are used to perform operations in the coprocessors and other special system functions. Coprocessor 0 (CPO) is the system control coprocessor. CPO instructions manipulate the memory management and exception handling facilities. Floating-point instructions are implemented as coprocessor instructions and are performed by a separate floating-point processor.

The MIPS R20001R3000 pipeline is a five-stage instruction pipeline quite similar to the TYP pipeline. However, each pipeline stage is further divided into two separate phases, identified as phase one (P1) and phase two (P2). The functions performed by each of the five stages and their phases are described in Table 2.7.

There are a number of interesting features in this five-stage pipeline. The I-cache access, which requires an entire cycle, actually takes place during P2 of 

the IF stage and I 1 of the RD stage. One translation lookaside buffer (TLB) is used to do address translation for both the I-cache and the D-cache. The TLB is accessed during P1 of the IF stage, for supporting I-cache access and is accessed during P2 of the ALU stage, for supporting D-cache access, which takes place during the MEM cycle. The register file performs first a write (P1 of WB stage), and then a read (P2 of RD stage) in every machine cycle. This pipeline requires a three-ported (two reads and one write) register file and a single-ported I-cache and a single-ported D-cache to support the IF and MEM stages, respectively.

With forwarding paths from the outputs of the ALU and the MEM stages back to the input of the ALU stage, no ALU leading hazards will incur a penalty cycle.
The load penalty, that is, the worst-case penalty incurred by a load leading hazard, is only one cycle with the forwarding path from the output of the MEM stage to the input of the ALU stage. The branch penalty is also only one cycle. This is made possible due to several features of the R20001R3000 pipeline. First, branch instructions use only PC-relative addressing mode. Unlike a register which must be accessed during the RD stage, the PC is available after the IF stage. Hence, the branch target address can be calculated, albeit using a separate adder, during the RD stage. The second feature is that no explicit condition code bit is generated and stored. The branch condition is generated during P1 of the ALU stage by comparing the contents of the referenced register(s). Normally with the branch condition being generated in the ALU stage (stage 3) and the instruction fetch being done in the IF stage (stage 1), the expected penalty would be two cycles. However, in this particular pipeline design the I-cache access actually does not start until P2 of the IF stage. With the branch condition being available at the end of P1 of the ALU stage and since the I-cache access does not begin until P2 of the IF stage, the branch target address produced at the end of the RD stage can be steered by the branch condition into the PC prior to the start of I-cache access in the middle of the IF stage. Consequently only a one-cycle penalty is incurred by branch instructions.

Compared to the six-stage TYP pipeline, the five-stage MIPS R2000/R3000 pipeline is a better design in terms of the penalties incurred due to pipeline hazards. Both pipelines have the same ALU and load penalties of zero cycles and one cycle, respectively. However, due to the above stated features in its design, the MIPS R20001R3000 pipeline incurs only one cycle, instead of four cycles, for its branch penalty. Influenced by and having benefited from the RISC research done at Stanford University, the MIPS R2000/R3000 has a very clean design and is a highly efficient pipelined processor.




#### 2.2.4.2 CISC Pipelined Processor Example.  In 1978 Intel introduced one of the first 16-bit microprocessors, the Intel 8086. 
Although preceded by earlier 8-bit microprocessors from Intel (8080 and 8085), the 8086 began an evolution that would eventually result in the Intel IA32 family of object code compatible microprocessors. The Intel IA32 is a CISC architecture with variable-length instructions and complex addressing modes, and it is by far the most dominant architecture today in terms of sales volume and the accompanying application software base. In 1985, the Intel 386, the 32-bit version of the IA32 family, was introduced [Crawford, 1986].

The first pipelined version of the IA32 family, the Intel 486, was introduced in 1989.
Table 2.8
Functionality of the Intel 486 fi ve-stage pipeline 
|Stage Name | Function Performed
|-|-|
|1. Instructi on fetch |Fetch instructi on from the 32-byte prefetch queue (prefetch unit fills and flu shes prefetch q ueue).
|2. Inst ruction decode- 1 | Tra nslate instru cti on into co ntrol sig nals or microcode add ress. Initi ate address generation and m emory access.
| 3. Instru cti on decode-2 | Access microcode memory. Out put micro instruction to execute unit.
| 4. Execute | Execute ALU and memory accessing operations.
| S. Register write-back |Write back result to register.

While the original 8086 chip had less than 30K transistors, the 486 chip has more than 1M transistors. The 486 is object code compatible with all previous members of the IA32 family, and it became the most popular microprocessor used for personal computers in the early 1990s [Crawford, 1990].

The 486 implemented a five- stage instruction pipeline. The functionality of the pipeline stages is described in Table 2.8. An instruction prefetch unit, via the bus interface unit, prefetches 16-byte blocks of instructions into the prefetch queue. During the instruction fetch stage, each instruction is fetched from the 32-byte prefetch queue. Instruction decoding is performed in two stages. Hardwired control signals as well as microinstructions are produced during instruction decoding. The execute stage performs both ALU operations as well as cache accesses. Address translation and effective address generation are carried out during instruction decoding; memory accessing is completed in the execute stage.

Hence, a memory load followed immediately by a use does not incur any penalty cycle; output of the execute stage is forwarded to its input. However, if an instruction that produces a register result is followed immediately by another instruction that uses the same register for address generation, then a penalty cycle is necessary because address generation is done during instruction decoding. The fifth stage in the pipeline performs a register write-back. Floating-point operations are carried out by an on-chip floating-point unit and can incur multiple cycles for their execution.

With the five-stage instruction pipeline, the 486 can execute many IA32 instructions in one cycle without using microcode. Some instructions require the accessing of micro-instructions and multiple cycles. The 486 clearly demonstrates the performance improvement that can be obtained via instruction pipelining. Based on typical instruction mix and the execution times for the frequently used IA32 instructions, the Intel 386 is able to achieve an average cycles per instruction (CPI) of 4.9 [Crawford, 1986]. The pipelined Intel 486 can achieve an average CPI of about 1.95.

This represents a speedup by a factor of about 2.5. In our terminology, the five-stage i486 achieved an effective degree of pipelining of 2.5. Clearly, significant pipelining overhead is involved, primarily due to the complexity of the IA32 instruction set architecture and the burden of ensuring object code compatibility. Nonetheless, for a CISC architecture, the speedup obtained is quite respectable. The 486 clearly demonstrated the feasibility of pipelining a CISC architecture.




#### 2.2.4.3 Scalar Pipelined Processor Performance.  A report documenting the
IBM experience with pipelined RISC machines by Tilak Agerwala and John Cocke in 1987 provided an assessment of the performance capability of scalar pipelined RISC processors [Agerwala and Cocke, 1987]. Some of the key observations from that report are presented here. In this study, it is assumed that the I-cache and D-cache are separate. The I-cache can supply one instruction per cycle to the processor. Only load/store instructions access the D-cache. In this study, the hit rates for both caches are assumed to be 100%. The default latency for both caches is one cycle. The following characteristics and statistics are used in the study.

1. Dynamic instruction mix
   1. ALU: 40% (register-register) b. Loads: 25%
   2. Stores: 15%
   3. Branches: 20%
2. Dynamic branch instruction mix 
   1. Unconditional: 33.3% (always taken) 
   2. Conditional-taken: 33.3%
   3. Conditional-not taken: 33.3% 
3.  Load scheduling
   1. Cannot be scheduled: 25% (no delay slot filled) 
   2. Can be moved back one or two instructions: 65% (fill two delay slots) 
   3. Can be moved back one instruction: 10% (fill one delay slot) 
4. Branch scheduling
   1. Unconditional: 100% schedulable (fill one delay slot) 
   2. Conditional: 50% schedulable (fill one delay slot) 

The performance of a processor can be estimated using the average cycles per instruction. The idealized goal of a scalar pipeline processor is to achieve a CPI = 1.
This implies that the pipeline is processing or completing, on the average, one instruction in every cycle. The IBM study attempted to quantify how closely this idealized goal can be reached. Initially, it is assumed that there is no ALU penalty and that the load and branch penalties are both two cycles. Given the dynamic instruction mix, the CPI overheads due to these two penalties can be computed.

* Load penalty overhead: 0.25 x 2 = 0.5 CPI
* Branch penalty overhead: 0.20 x 0.66 x 2 = 0.27 CPI 
* Resultant CPI: 1.0 + 0.5 + 0.27 = 1.77 CPI

Since 25% of the dynamic instructions are loads, if we assume each load incurs the two-cycle penalty, the CPI overhead is 0.5. Ifthe pipeline assumes that branch instructions are not taken, or biased for not taken, then only the 66.6% of the branch instructions that are taken will incur the two-cycle branch penalty.

Taking into account both the load and branch penalties, the expected CPI is 1.77.
This is far from the idealized goal of CPI = 1.
Assuming that a forwarding path can be added to bypass the register file for load instructions, the load penalty can be reduced from two cycles down to just one cycle. With the addition of this forwarding path, the CPI can be reduced to 1.0 + 0.25 + 0.27 = 1.52.

In addition, the compiler can be employed to schedule instructions into the load and branch penalty slots. Assuming the statistics presented in the preceding text, since 65% of the loads can be moved back by one or two instructions and 10% of the loads can be moved back by one instruction, a total of 75% of the load instructions can be scheduled, or moved back, so as to eliminate the load penalty of one cycle. For 33.3% of the branch instructions that are unconditional, they can all be scheduled to reduce the branch penalty for them from two cycles to one cycle. Since the pipeline is biased for not taken branches, the 33.3% of the branches that are conditional and not taken incur no branch penalty. For the remaining 33.3% of the branches that are conditional and taken, the assumption is that 50% of them are schedulable, that is, can be moved back one instruction. Hence 50% of the conditional branches that are taken wiIl incur only a one-cycle penalty, and the other 50% will incur the normal two-cycle penalty. The new CPI overheads and the resultant CPI are shown here.

* Load penalty overhead: 0.25 x 0.25 x 1 = 0.0625 CPI
* Branch penalty overhead: 0.20 x [0.33 x 1 + 0.33 x 0.5 x 1 + 0.33 x 0.5 x 2] = 0.167 CPI
* Resultant CPI: 1.0 + 0.063 + 0.167 = 1.23 CPI 

By scheduling the load and branch penalty slots, the CPI overheads due to load and branch penalties are significantly reduced. The resultant CPI of 1.23 is approaching the idealized goal of CPI = 1. The CPI overhead due to the branch penalty is still significant. One way to reduce this overhead further is to consider ways to reduce the branch penalty of two cycles. From the IBM study, instead of using the register-indirect mode of addressing, 90% of the branches can be coded as PC-relative. Using the PC-relative addressing mode, the branch target address generation can be done without having to access the register file. A separate adder can be included to generate the target address in parallel with the register read stage. Hence, for the branch instructions that employ PC-relative addressing, the branch penalty can be reduced by one cycle. For the 33.3% of the branches that are unconditional, they are 100% schedulable. Hence, the branch penalty is only one cycle. If 90% of them can be made PC-relative and consequently eliminate the branch penalty, then only the remaining 10% of the unconditional branches will incur the branch penalty of one cycle. The corresponding Table 2.9

Conditional branch penalties considering PC-relative addressing and scheduling of penalty slot
|PC-relative Addressing |Schedulable |Branch Penalty
|-|-|-
|Yes (90%) |Yes (50%) |0 cycles 
|Yes (90%) |No (50%) |1 cycle 
|No (10%) |Yes (50%) |1 cycle 
|No (10%) |No (50%) |2 cycles

CPI overhead for unconditional branches is then 0.20 x 0.33 x 0.10 x 1 = 0.0066 CPI.

With the employment of the PC-relative addressing mode, the fetch stage is no longer biased for the not taken branches. Hence all conditional branches can be treated in the same way, regardless of whether they are taken. Depending on whether a conditional branch can be made PC-relative and whether it can be scheduled, there are four possible cases. The penalties for these four possible cases for conditional branches are shown in Table 2.9.

Including both taken and not taken ones, 66.6% of the branches are conditional. The CPI overhead due to conditional branches is derived by considering the cases in Table 2.9 and is equal to 

 0.20 x 0.66 x { [0.9 x 0.5 x 1] + [0.1 x 0.5 xl] + [0.1 x 0.5 x 2]} = 0.079 CPI

Combining the CPI overheads due to unconditional and conditional branches results in the total CPI overhead due to branch penalty of 0.0066 + 0.079 = 0.0856 CPI.
Along with the original load penalty, the new overheads and the resultant overall CPI are shown here.
* Load penalty overhead: 0.0625 CPI 
* Branch penalty overhead: 0.0856 CPI 
* Resultant CPI: 1.0 + 0.0625 + 0.0856 = 1.149 CPI 

Therefore, with a series of refinements, the original CPI of 1.77 is reduced to 1.15. This is quite close to the idealized goal of CPI = 1. One way to view this is that CPI = 1 represents the ideal instruction pipeline, in which a new instruction is entered into the pipeline in every cycle. This is achievable only if the third point of pipelining idealism is true, that is, all the instructions are independent. In real programs there are inter-instruction dependences. The CPI = 1.15 indicates that only a 15% overhead or inefficiency is incurred in the design of a realistic instruction pipeline that can deal with inter-instruction dependences. This is quite impressive and reflects the effectiveness of instruction pipelining.


## 2.3 Deeply Pipelined Processors

Pipelining is a very effective means of improving processor performance, and there are strong motivations for employing deep pipelines. A deeper pipeline increases the number of pipeline stages and reduces the number of logic gate levels in each pipeline stage. The primary benefit of deeper pipelines is the ability to reduce the machine cycle time and hence increase the clocking frequency. During the 1980s most pipelined microprocessors had four to six pipeline stages. Contemporary high-end microprocessors have clocking frequencies in the multiple-gigahertz range, and pipeline depths have increased to more than 20 pipeline stages. Pipelines have gotten not only deeper, but also wider, such as superscalar processors. As pipelines get wider, there is increased complexity in each pipeline stage, which can increase the delay of each pipeline stage. To maintain the same clocking frequency, a wider pipeline will need to be made even deeper.

There is a downside to deeper pipelines. With a deeper pipeline the penalties incurred for pipeline hazard resolution can become larger. Figure 2.23 illustrates what can happen to the ALU, load, and branch penalties when a pipeline becomes wider and much deeper. Comparing the shallow and the deep pipelines, we see that the ALU penalty increases from zero cycles to one cycle, the load penalty increases from one cycle to four cycles, and most importantly, the branch penalty goes from three cycles to eleven cycles. With increased pipeline penalties, the average CPI increases. The potential performance gain due to the higher clocking frequency of a deeper pipeline can be ameliorated by the increase of CPI. To ensure overall performance improvement with a deeper pipeline, the increase in clocking frequency must exceed the increase in CPI.

There are two approaches that can be used to mitigate the negative impact of the increased branch penalty in deep pipelines; see Figure 2.24. Among the three pipeline penalties, the branch penalty is the most severe because it spans all the front-end pipeline stages. With a mispredicted branch, all the instructions in the front-end pipeline stages must be flushed . 

penalty is to reduce the number of pipeline stages in the front end_ For example, a CISe architecture with variable instruction length can require very complex instruction decoding logic that can require mUltiple pipeline stages. By using a RISe architecture, the decoding complexity is reduced, resulting in fewer front-end pipeline stages_ Another example is the use of pre-decoding logic prior to loading instructions into the I-cache. Pre-decoded instructions fetched from the I-cache require less decoding logic and hence fewer decode stages.

The second approach is to move some of the front-end complexity to the back end of the pipeline, resulting in a shallower front end and hence a smaller branch penalty. This has been an active area of research. When a sequence of instructions is repeatedly executed by a pipeline, the front-end pipeline stages repeatedly perform the same work of fetching, decoding, and dispatching on the same instructions. Some have suggested that the result of the work done can be cached and reused without having to repeat the same work. For example, a block of decoded instructions can be stored in a special cache. Subsequent fetching of these same instructions can be done by accessing this cache, and the decoding pipeline stage(s) can be bypassed. Other than just caching these decoded instructions, additional optimization can be performed on these instructions, leading to further elimination of the need for some of the front-end pipeline stages. Both the caching and the optimization can be implemented in the back end of the pipeline without impacting the front-end depth and the associated branch penalty. In order for deep pipelines to harvest the performance benefit of a higher clocking frequency, the pipeline penalties must be kept under controL There are different forms of tradeoffs involved in designing deep pipelines.

As indicated in Section 2.1, a k-stage pipeline can potentially achieve an increase of throughput by a factor of k relati ve to a nonpipelined design. When cost is taken into account, there is a tradeoff involving cost and performance. This tradeoff dictates that the optimal value of k not be arbitrarily large. This is illustrated in Figure 2.3. This form of tradeoff deals with the hardware cost of implementing the pipeline, and it indicates that there is a pipeline depth beyond which the additional cost of pipe lining cannot be justified by the diminishing return on the performance gain.

There is another form of tradeoff based on the foregoing analysis of CPI impact induced by deep pipelines. This tradeoff involves the increase of clocking frequency versus the increase of CPI. According to the iron law of processor performance (Sec. 1.3.1, Eq. 1.1), performance is determined by the product of clocking frequency and the average IPC, or the Jrequency/CPI ratio. As pipelines get deeper, frequency increases but so does CPI. Increasing the pipeline depth is profitable as long as the added pipeline depth brings about a net increase in performance. There is a point beyond which pipelining any deeper will lead to little or no performance improvement. The interesting question is, How deep can a pipeline go before we reach this point of diminishing returns? A number of recent studies have focused on determining the optimum pipeline depth [Hartstein and Puzak, 2002, 2003; Sprangle and Carmean, 2002; Srinivasan et a!., 2002] for a microprocessor. As pipeline depth increases, frequency can be increased. However the frequency does not increase linearly with respect to the increase of pipeline depth. The sublinear increase of frequency is due to the overhead of adding latches. As pipeline depth increases, CPI also increases due to the increase of branch and load penalties. Combining frequency and CPI behaviors yields the overall performance. As pipeline depth is increased, the overall performance tends to increase due to the benefit of the increased frequency . However when pipeline depth is further increased, there reaches a point where the CPI overhead overcomes the benefit of the increased frequency; any further increase of pipeline depth beyond this point can actually bring about the gradual decrease of overall performance. In a recent study, Hartstein and Puzak [2003] showed, based on their performance model, this point of diminishing return, and hence the optimum pipeline depth, occurs around pipeline depth of -25 stages. Using more aggressive assumptions, Sprangle and Carmean [2002] showed that the optimum pipeline depth is actually around 50 stages.

If power consumption is taken into account, the optimum pipeline depth is significantly less than 25 or 50 pipe stages. The higher frequency of a deeper pipeline leads to a significant increase of power consumption. Power consumption can become prohibitive so as to render a deep pipeline infeasible, even if there is more performance to be harvested. In the same study, Hartstein and Puzak [2003] developed a new model for optimum pipeline depth by taking into account power consumption in addition to performance. They use a model based on the BIPSI\I\3IW metric, where BIPSI\I\3 is billions of instructions per second to the third power, and W is watt. This model essentially favors performance (BIPS) to power (W) by a ratio of 3 to 1. Oi ven their model, the optimum pipeline depth is now more in the range of 6-9 pipe stages. Assuming lower latching overhead and with increasing leakage power, they showed the optimum pipeline depth could potentially be in the range of 10-15 pipe stages. While in recent years we have witnessed the relentless push towards ever higher clocking frequencies and ever deeper pipelines, the constraints due to power consumption and heat dissipation can become serious impediments to this relentless push.




## 2.4 Summary

Pipelining is a microarchitecture technique that can be applied to any ISA. It is true that the features of RISC architectures make pipelining easier and produce more efficient pipeline designs. However, pipelining is equally effective on CISC architectures. Pipelining has proved to be a very powerful technique in increasing processor performance, and in terms of pipeline depth there is still plenty of headroom. We can expect much deeper pipelines.

The key impediment to pipe lined processor performance is the stalling of the pipeline due to inter-instruction dependences. A branch penalty due to control dependences is the biggest culprit. Dynamic branch prediction can alleviate this problem so as to incur the branch penalty only when a branch misprediction occurs. When a branch is correctly predicted, there is no stalling of the pipeline; however, when a branch misprediction is detected, the pipeline must be flushed.

As pipelines get deeper, the branch penalty increases and becomes the key challenge. One strategy is to reduce the branch penalty by reducing the depth of the front end of the pipeline, that is, the distance between the instruction fetch stage and the stage in which branch instructions are resolved. An alternative is to increase the accuracy of the dynamic branch prediction algorithm so that the frequency of branch misprediction is reduced; hence, the frequency of incurring the branch penalty is also reduced. We did not cover dynamic branch prediction in this chapter. This is a very important topic, and we have chosen to present branch prediction in the context of superscalar processors. We will get to it in Chapter 5.

Pipelined processor design alters the relevance of the classic view of CPU design. The classic view partitions the design of a processor into data path design and control path design. Data path design focuses on the design of the ALU and other functional units as well as the accessing of registers. Control path design focuses on the design of the state machines to decode instructions and generate the sequence of control signals necessary to appropriately manipulate the data path.

This view is no longer relevant. In a pipelined processor this partition is no longer obvious. Instructions are decoded in the decode stage, and the decoded instructions, including the associated control signals, are propagated down the pipeline and used by various subsequent pipeline stages. Each pipeline stage simply uses the appropriate fields of the decoded instruction and associated control signals.

Essentially there is no longer the centralized control performed by the control path. Instead, a form of distributed control via the propagation of the control signals through the pipeline stages is used. The traditional sequencing through multiple control path states to process an instruction is now replaced by the traversal through the various pipeline stages. Essentially, not only is the data path pipelined, but also the control path. Furthermore, the traditional data path and the control path are now integrated into the same pipeline.

## REFERENCES

Agerwala, T. , and J. Cocke: "High performance reduced instruction set processors," Technical report, IBM Computer Science, 1987.

Bloch, E.: "The engineering design of the STRETCH computer," Proc. Fall Joint Computer Coni , 1959,pp.48-59.

Bucholtz, W.: Planning a Computer System: Project Stretch. New York: McGraw-Hill, 1962.

Crawford, J.: "Architecture of the Intel 80386," Proc. IEEE Int. Coni on Computer Design: VLSI in Computers, 1986, pp. 155-160.

Crawford, 1.: "The execution pipeline of the Intel i486 CPU," Proc. COMPCON Spring '90, 1990, pp. 254-258.

Hartstein, A., and T. R. Puzak: "Optimum power/performance pipeline depth," Proc. of the 36th Annual International Symposium on Microarchitecture (MICRO), Dec. 2003.

Hartstein, A., and T. R. Puzak: "The optimum pipeline depth for a microprocessor," Proc. of the 29th Annual International Symposium on Computer Architecture (ISCA), June 2002.

Hennessy, J., and D. Patterson: Computer Architecture: A Quantitative Approach, 3rd ed., San Mateo, CA : Morgan Kaufmann Publishers, 2003.

Kane, G. : MIPS R2000lR3000 RISC Architecture. Englewood Cliffs, NJ: Prentice Hall, 1987.

Kogge, P. : The Architecture of Pipelined Computers. New York: McGraw-Hill, 1981.

Moussouris, J., L. Crudele, D. Frietas, C. Hansen, E. Hudson, R. March, S. Przybylski, and T. Riordan: "A CMOS RISC processor with integrated system functions," Proc.  COMPCON, 1986, pp. 126- 131.

Sprang Ie, E., and D. Carmean: "Increasing processor performance by implementing deeper pipelines," Proc. of the 29th Annual International Symposium on Computer Architecture (ISCA), June 2002.

Srinivasan, V., D. Brooks, M. Gschwind, P. Bose, V. Zyuban, P. N. Strenski, and P. G.  Emma: "Optimizing pipelines for power and performance," Proc. of the 35th Annual International Symposium on Microarchitecture (MICRO), Dec. 2002 .

Thornton, J. E.: "Parallel operation in the Control Data 6600," AFlPS Proc. FJCC part 2, vol. 26, 1964, pp. 33-40.

Waser, S., and M. Flynn: Introduction to Arithmeticfor Digital Systems Designers. New York: Holt, Rinehart, and Winston, 1982.


## HOMEWORK PROBLEMS

P2.t Equation (2.4), which relates the performance of an ideal pipeline to pipeline depth, looks very similar to Amdahl's law. Describe the relationship between the terms in these two equations, and develop an intuitive explanation for why the two equations are so similar.

P2.2 Using Equation (2.7), the cost/performance optimal pipeline depth k oP' can be computed using parameters G, T, L, and S. Compute kOPf for the pipelined floating-point multiplier example in Section 2.1 by using the chip count as the cost terms (G = 175 chips and L = 82/2 = 41 chips per interstage latch) and the delays shown for T and S (T = 400 ns, S = 22 ns). How different is k opt from the proposed pipelined design? P2.3 Identify and discuss two reasons why Equation (2.4) is only useful for naive approximations of potential speedup from pipelining.

P2.4 Consider that you would like to add a load-immediate instruction to the TYP instruction set and pipeline. This instruction extracts a 16-bit immediate value from the instruction word, sign-extends the immediate value to 32 bits, and stores the result in the destination register specified in the instruction word. Since the extraction and sign-extension can be accomplished without the ALU, your colleague suggests that such instructions be able to write their results into the register in the decode (ID) stage. Using the hazard detection algorithm described in Figure 2.15, identify what additional hazards such a change might introduce.

P2.S Ignoring pipeline interlock hardware (discussed in Problem 6), what additional pipeline resources does the change outlined in Problem 4 require? Discuss these resources and their cost.

P2.6 Considering the change outlined in Problem 4, redraw the pipeline interlock hardware shown in Figure 2.18 to correctly handle the loadimmediate instructions.

P2.7 Consider that you would like to add byte-wide ALU instructions to the TYP instruction set and pipeline. These instructions have semantics that are otherwise identical to the existing word-width ALU instructions, except that the source operands are only 1 byte wide and the destination operand is only 1 byte wide. The byte-wide operands are stored in the same registers as the word-wide instructions, in the low-order byte, and the register writes must only affect the low-order byte (i.e., the highorder bytes must remain unchanged). Redraw the RAW pipeline interlock detection hardware shown in Figure 2.18 to correctly handle these additional ALU instructions.

P2.8 Consider adding a store instruction with an indexed addressing mode to the TYP pipeline. This store differs from the existing store with the register + immediate addressing mode by computing its effective address as the sum of two source registers, that is, stx r3,r4,r5 performs r3f-MEM[r4+r5] . Describe the additional pipeline resources needed to support such an instruction in the TYP pipeline. Discuss the advantages and disadvantages of such an instruction.

P2.9 Consider adding a load-update instruction with register + immediate and postupdate addressing mode. In this addressing mode, the effective address for the load is computed as register + immediate, and the resulting address is written back into the base register. That is, lwu r3,8(r4) performs r3f-MEM[r4+8] ; r4f-r4+8. Describe thethe TYP pipeline.

P2.10 Given the change outlined in Problem 9, redraw the pipeline interlock hardware shown in Figure 2.20 to correctly handle the load-update instruction.

P2.11 Bypass network design: given the following 10, EX, MEM, and WB pipeline configuration, draw all necessary MuxO and Muxl bypass paths to resolve RAW data hazards. Assume that load instructions are always separated by at least one independent instruction [possibly a no-operation instruction (NOP)] from any instruction that reads the loaded register (hence you never stall due to a RAW hazard).

P2.12 Given the forwarding paths in Problem 11, draw a detailed design for MuxO and Muxl that clearly identifies which bypass paths are selected under which control conditions. Identify each input to eachMux 1. Possible inputs to the boolean equations are:
â€¢ ID.OP, EX.OP, MEM.OP = {'load', 'store', 'alu' , 'other'} â€¢ ID.ReadRegO, ID.ReadRegl = [0 .. 31,32] where 32 means a register is not read by this instruction â€¢ EX.ReadRegO, etc., as in ID stage â€¢ MEM.ReadRegO, etc., as in ID stage â€¢ ID.WriteReg, EX.WriteReg, MEM.WriteReg = [0 .. 31,33] where 33 means a register is not written by this instruction â€¢ Draw MuxO and Muxl with labeled inputs; you do not need to show the controls using gates. Simply write out the control equations using symbolic OP comparisons, etc. [e.g., Ctrll = (ID.op = 'load') & (ID.WriteReg=MEM.ReadRegO)] .

P2.13 Given the IBM experience outlined in Section 2.2.4.3, compute the CPI impact of the addition of a level-zero data cache that is able to supply the data operand in a single cycle, but only 75% of the time. The level-zero and level-one caches are accessed in parallel, so that when the level-zero cache misses, the level-one cache returns the result in the next cycle, resulting in one load-delay slot. Assume uniform distribution of level-zero hits across load-delay slots that can and cannot be filled. Show your work.

P2.14 Given the assumptions of Problem 13, compute the CPI impact if the level-one cache is accessed sequentially, only after the level-zero cache misses, resulting in two load-delay slots instead of one. Show your work.

P2.15 The IBM study of pipelined processor performance assumed an instruction mix based on popular C programs in use in the 1980s.
Since then, object-oriented languages like C++ and Java have become much more common. One of the effects of these languages is that object inheritance and polymorphism can be used to replace conditional branches with virtual function calls. Given the IBM instruction mix and CPI shown in the following table, perform the following transformations to reflect the use of C++ and Java, and recompute the overall CPI and speedup or slowdown due to this change:

* Replace 50% of taken conditional branches with a load instruction followed by a jump register instruction (the load and jump register implement a virtual function call).
* Replace 25% of not-taken branches with a load instruction followed by a jump register instruction.Type Load


P2.16 In a TYP-based pipeline design with a data cache, load instructions check the tag array for a cache hit in parallel with accessing the data array to read the corresponding memory location. Pipelining stores to such a cache is more difficult, since the processor must check the tag first, before it overwrites the data array. Otherwise, in the case of a cache miss, the wrong memory location may be overwritten by the store. Design a solution to this problem that does not require sending the store down the pipe twice, or stalling the pipe for every store instruction, or dual-porting the data cache. Referring to Figure 2.15, are there any new RAW, WAR, and/or WAW memory hazards?

P2.17 The MIPS pipeline shown in Table 2.7 employs a two-phase clocking scheme that makes efficient use of a shared TLB, since instruction fetch accesses the TLB in phase one and data fetch accesses in phase two.
However, when resolving a conditional branch, both the branch target address and the branch fall-through address need to be translated during phase one-in parallel with the branch condition check in phase one of the ALU stage-to enable instruction fetch from either the target or the fall-through during phase two. This seems to imply a dual-ported TLB.

Suggest an architected solution to this problem that avoids dual-porting the TLB.


Problems 18 through 24: Instruction Pipeline Design
This problem explores pipeline design. As discussed earlier, pipe lining involves balancing the pipe stages. Good pipeline implementations minimize both internal and external fragmentation to create simple balanced designs. Below is a nonpipelined implementation of a simple microprocessor that executes only ALU instructions, with no data hazards:

the figure that minimizes internal fragmentation. Each subblock in the diagram is a primitive unit that cannot be further partitioned into smaller ones. The original functionality must be maintained in the pipelined implementation. Show the diagram of your pipelined implementation.

Pipeline registers have the following timing requirements:

* O.S-ns setup time
* I-ns delay time (from clock to output) P2.19 Compute the latencies (in nanoseconds) of the instruction cycle of the nonpipelined and the pipe lined implementations.

P2.20 Compute the machine cycle times (in nanoseconds) of the non pipelined and the pipelined implementations.

P2.21 Compute the (potential) speedup of the pipe lined implementation in Problems 18-20 over the original nonpipelined implementation.

P2.22 What microarchitectural techniques could be used to further reduce the machine cycle time of pipelined designs? Explain how the machine cycle time is reduced.

P2.23 Draw a simplified diagram of the pipeline stages in Problem 18; you should include all the necessary data forwarding paths. This diagram should be similar to Figure 2.16.

P2.24 Discuss the impact of the data forwarding paths from Problem 23 on the pipeline implementation in Problem 18. How will the timing be affected? Will the pipeline remain balanced once these forwarding paths are added? What changes to the original pipeline organization of Problem 18 might be needed?
