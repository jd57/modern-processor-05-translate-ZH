Memory and 1/0 Systems

CHAPTER OUTLINE

* 3.1 Introduction 
* 3.2 Computer System Overview
* 3.3 Key Concepts: Latency and Bandwidth
* 3.4 Memory Hierarchy
* 3.5 Virtual Memory Systems
* 3.6 Memory Hierarchy Implementation
* 3.7 Input/Output Systems
* Summary
* References
* Homework Problems



## 3.1 Introdudion

The primary focus of this book is the design of advanced, high-perfonnance processors; this chapter examines the larger context of computer systems that incorporate such processors. Basic components, such as memory systems, input and output, and virtual memory, and the ways in which they are interconnected are described in relative detail to enable a better understanding of the interactions between high-performance processors and the peripheral devices they are connected to.

Clearly, processors do not exist in a vacuum. Depending on their intended application, processors will interact with other components internal to a computer system, devices that are external to the system, as well as humans or other external entities.
The speed with which these interactions occur varies with the type of communication that is necessary, as do the protocols used to communicate with them. Typically, interacting with perfonnance-critical entities such as the memory subsystem is accomplished via proprietary, high-speed interfaces, while communication with that sacrifice some performance for the sake of compatibility across multiple vendors. Usually such interfaces are balanced, providing symmetric bandwidth to and from the device. However, interacting with physical beings (such as humans) often leads to unbalanced bandwidth requirements. Even the fastest human typist can generate input rates of only a few kilobytes per second. In contrast, human visual perception can absorb more than 30 frames per second of image data, where each image contains several megabytes of pixel data, resulting in an output data rate of over 100 megabytes per second (Mbytes/s).

Just as the bandwidth requirements of various components can vary dramatically, the latency characteristics are diverse as well. For example, studies have shown that while subsecond response times (a response time is defined as the interval between a user issuing a command via the keyboard and observing the response on the display) are critical for the productivity of human computer users, response times much less than a second provide rapidly diminishing returns.

Hence, low latency in responding to user input through the keyboard or mouse is not that critical. In contrast, modem processors operate at frequencies that are much higher than main memory subsystems. For example, a state-of-the-art personal computer has a processor that is clocked at 3 GHz today, while the synchronous main memory is clocked at only 133 MHz. This mismatch in frequency can cause the processor to starve for instructions and data as it waits for memory to supply them, hence motivating high-speed processor-to-memory interfaces that are optimized for low latency.

Section 3.2 presents an overview of modem computer systems. There are numerous interesting architectural tradeoffs in the design of hardware subsystems, interfaces, and protocols to satisfy input/output requirements that vary so dramatically. In Section 3.3, we define the fundamental metrics of bandwidth and latency and discuss some of the tradeoffs involved in designing interfaces that meet requirements for both metrics. In Section 3.4, we introduce the concept of a memory hierarchy and discuss the components used to build a modem memory hierarchy as well as the key tradeoffs and metrics used in the design process. Section 3.5 introduces the notion of virtual memory, which is critically important in modern systems that timeshare physical execution resources. Finally, Section 3.7 discusses various input/ output devices, their key characteristics, and the interconnects used in modern systems to allow them to communicate with each other and with the processor.




## 3.2 Computer System Overview

As illustrated in Figure 3.1, a typical computer system consists of a processor or CPU, main memory, and an input/output (1/0) bridge connected to a processor bus, and peripheral devices such as a network interface, a disk controller driving one or more disk drives, a display adapter driving a display, and input devices such as a keyboard or mouse, all connected to the 110 bus. The main memory provides volatile storage for programs and data while the computer is powered up.

The design of efficient, high-performance memory systems using a hierarchical approach that exploits temporal and spatial locality is discussed in detail in Section 3.4. In contrast to volatile main memory, a disk drive provides persistent storage that survives even when the system is powered down. Disks can also be used to transparently increase effective memory capacity through the use of virtual memory, as described in Section 3.5. The network interface provides a physical connection for communicating across local area or wide area networks (LANs or WANs) with other computer systems; systems without local disks can also use the network interface to access remote persistent storage on file servers. The display subsystem is used to render a textual or graphical user interface on a display device such as a cathode-ray tube (CRT) or liquid-crystal display (LCD). Input devices enable a user or operator to enter data or issue commands to the computer system. We will discuss each of these types of peripheral devices (disks, network interfaces, display subsystems, and input devices) in Section 3.7.1.

Finally, a computer system must provide a means for interconnecting all these devices, as well as an interface for communicating with them. We will discuss various types of busses used to interconnect peripheral devices in Section 3.7.2 and will describe polling, interrupt-driven, and programmed means of communication with 110 devices in Section 3.7.3.




## 3.3 Key Concepts: Latency and Bandwidth

There are two fundamental metrics that are commonly used to characterize various subsystems, peripheral devices, and interconnections in computer systems. These two metrics are latency, measured in unit time, and bandwidth, measured in quantity per unit time. Both metrics are important for understanding the behavior of a system, so we provide definitions and a brief introduction to both in this section.a particular subsystem and receiving a response or reply. It is measured either in units of time (seconds, microseconds, milliseconds, etc.) or cycles, which can be trivially translated to time given cycle time or frequency. Latency provides a measurement of the responsiveness of a particular system and is a critical metric for any subsystem that satisfies time-critical requests. An example of such a system is the memory subsystem, which must provide the processor with instructions and data; latency is critical because processors will usually stall if the memory subsystem does not respond rapidly. Latency is also sometimes called response time and can be decomposed into the inherent delay of a device or subsystem, called the service time, which forms the lower bound for the time required to satisfy a request, and the queueing time, which results from waiting for a particular resource to become available. Queueing time is greater than zero only when there are multiple concurrent requests competing for access to the same resource, and one or more of those requests must delay while waiting for another to complete.

Bandwidth is defined as the throughput of a subsystem; that is, the rate at which it can satisfy requests. Bandwidth is measured in quantity per unit time, where the quantity measured varies based on the type of request. At its simplest, bandwidth is expressed as the number of requests per unit time. If each request corresponds to a fixed number of bytes of data, for example, bandwidth can also be expressed as the number of bytes per unit time.

Naively, bandwidth can be defined as the inverse of latency. That is, a device that responds to a single request with latency I will have bandwidth equal to or less than 111, since it can accept and respond to one request every l units of time.
However, this naive definition precludes any concurrency in the handling of requests. A high-performance subsystem will frequently overlap multiple requests to increase bandwidth without affecting the latency of a particular request. Hence, bandwidth is more generally defined as the rate at which a subsystem is able to satisfy requests. If bandwidth is greater than 1Il, we can infer that the subsystem supports multiple concurrent requests and is able to overlap their latencies with each other. Most high-performance interfaces, including processor-to-memory interconnects, standard input/output busses like peripheral component interfaces (PCls), and device interfaces like small computer systems interface (SCSI), support multiple concurrent requests and have bandwidth significantly higher than lIZ.

Quite often, manufacturers will also report raw or peak bandwidth numbers, which are usually derived directly from the hardware parameters of a particular interface. For example, a synchronous dynamic random-access memory (DRAM) interface that is 8 bytes wide and is clocked at 133 MHz may have a reported peak bandwidth of I Gbyte/s. These peak numbers will usually be substantially higher than sustainable bandwidth, since they do not account for request and response transaction overheads or other bottlenecks that might limit achievable bandwidth. Sustainable bandwidth is a more realistic measure that represents bandwidth that the subsystem can actually deliver. Nevertheless, even sustainable bandwidth might be unrealistically optimistic, since it may not account for real-life access patterns andoverhead, and reduce delivered bandwidth.

In general, bandwidth is largely driven by product-cost constraints rather than fundamental limitations of a given technology. For example, a bus can always be made wider to increase the number of bytes transmitted per cycle, hence increasing the bandwidth of the interface. This will increase cost, since the chip pin count and backplane trace count for the bus may double, and while the peak bandwidth may double, the effective or sustained bandwidth may increase by a much smaller factor. However, it is generally true that a system that is performance-limited due to insufficient bandwidth is either poorly engineered or constrained by cost factors; if cost were no object, it would usually be possible to provide adequate bandwidth.

Latency is fundamentally more difficult to improve, since it is often dominated by limitations of a particular technology, or possibly even the laws of physics. For example, the electrical characteristics of a given signaling technology used in a multidrop backplane bus will determine the maximum frequency at which that bus can operate. It follows that the minimum latency of a transaction across that bus is bounded by the cycle time corresponding to that maximum frequency. A common strategy for improving latency, short of transitioning to a newer, faster, technology, is to decompose the latency into the portions that are due to various subcomponents and attempt to maximize the concurrency of those components. For example, a modern multiprocessor system like the IBM pSeries 690 exposes concurrency in handling processor cache misses by fetching the missing block from DRAM main memory in parallel with checking other processors' caches to try and find a newer, modified copy of the block. A less aggressive approach would first check the other processors' caches and then fetch the block from DRAM only if no other processor has a modified copy. The latter approach serializes the two events, leading to increased latency whenever a block needs to be fetched from DRAM.

However, there is often a price to be paid for such attempts to maximize concurrency, since they typically require speculative actions that may ultimately prove to be unnecessary. In the preceding multiprocessor example, if a newer, modified copy is found in another processor's cache, the block must be supplied by that cache. In this case, the concurrent DRAM fetch proves to be unnecessary and consumes excess memory bandwidth and wastes energy. However, despite such cost, various forms of speculation are commonly employed in an attempt to reduce the observed latency of a request. As another example, modern processors incorporate prefetch engines that look for patterns in the reference stream and issue speculative memory fetches to bring blocks into their caches in anticipation of demand references to those blocks. In many cases, these additional speCUlative requests or prefetches prove to be unnecessary, and end up consuming additional bandwidth.

However, when they are useful, and a subsequent demand reference occurs to a speculatively prefetched block, the latency of that reference corresponds to hitting in the cache and is much lower than if the prefetch had not occurred. Hence, average latency for all memory references can be lowered at the expense of consuming additional bandwidth to issue some number of useless prefetches.Bandwidth can usually be improved by adding cost to the system, but in a wellengineered system that maximizes concurrency, latency is usually much more difficult to improve without changing the implementation technology or using various forms of speculation. Speculation can be used to improve the observed latency for a request, but this usually happens at the expense of additional bandwidth consumption. Hence, in a well-designed computer system, latency and bandwidth need to be carefully balanced against cost, since all three factors are interrelated.




## 3.4 Memory Hierarchy

One of the fundamental needs that a computer system must meet is the need for storage of data and program code, both while the computer is running, to support storage of temporary results, as well as while the computer is powered off, to enable the results of computation as well as the programs used to perform that computation to survive across power-down cycles. Fundamentally, such storage is nothing more than a sea of bits that is addressable by the processor. A perfect storage technology for retaining this sea of bits in a computer system would satisfy the following memory idealisms:

* Infinite capacity. For storing large data sets and large programs.
* Infinite bandwidth. For rapidly streaming these large data sets and programs to and from the processor.
* Instantaneous or zero latency. To prevent the processor from stalling while waiting for data or program code.
* Persistence or non volatility. To allow data and programs to survive even when the power supply is cut off.
* Zero or very low implementation cost.

Naturally, the system and processor designers must strive to approximate these idealisms as closely as possible so as to satisfy the performance and correctness expectations of the user. Obviously, the final factor-cost-plays a large role in how easy it is to reach these goals, but a well-designed memory system can in fact maintain the illusion of these idealisms quite successfully. This is true despite the fact that the perceived requirements for the first three-capacity, bandwidth, and latency-have been increasing rapidly over the past few decades. Capacity requirements grow because the programs and operating systems that users demand are increasing in size and complexity, as are the data sets that they operate over.

Bandwidth requirements are increasing for the same reason. Meanwhile, the latency requirement is becoming increasingly important as processors continue to become faster and faster and are more easily starved for data or program code if the perceived memory latency is too long.
Components of a Modern Memory Hierarchy

A modem memory system, often referred to as a memory hierarchy, incorporates various storage technologies to create a whole that approximates each of the five memory idealisms. Figure 3.2 illustrates five typical components in a modem memory hierarchy and plots each on approximate axes that indicate their relative latency and capacity (increasing on the y axis) and bandwidth and cost per bit (increasing on the x axis). Some important attributes of each of these components are summarized in Table 3.1.


Magnetic Disks. Magnetic disks provide the most cost-efficient storage and the largest capacities of any memory technology today, costing less than one-tenmillionth of a cent per bit (i.e., roughly $1 per gigabyte of storage), while providing hundreds of gigabytes of storage in a 3.5-inch (in.) standard form factor. However, this tremendous capacity and low cost comes at the expense of limited effective bandwidth (in the tens of megabytes per second for a single disk) and extremely long latency (roughly 10 ms per random access). On the other hand, magnetic storage technologies are nonvolatile and maintain their state even when power is turned off.

10 Mbytes (?)more expensive at approximately two hundred-thousandths of a cent per bit (i.e., roughly $200 per gigabyte of storage) but provides much higher bandwidth (several gigabytes per second even in a low-cost commodity personal computer) and much lower latency (averaging less than 100 ns in a modern design). We study various aspects of main memory design at length in Section 3.4.4.


Cache Memory. On-chip and off-chip cache memories, both secondary (L2) and primary (Ll), utilize static random-access memory (SRAM) technology that pays a much higher area cost per storage cell than DRAM technology, resulting in much lower storage density per unit of chip area and driving the cost much higher.

Of course, the latency of SRAM-based storage is much lower-as low as a few hundred picoseconds for small Ll caches or several nanoseconds for larger L2 caches. The bandwidth provided by such caches is tremendous, in some cases exceeding 100 Gbytes/s. The cost is much harder to estimate, since high-speed custom cache SRAM is available at commodity prices only when integrated with high-performance processors. However, ignoring nonrecurring expenses and considering only the $50 estimated manufacturing cost of a modern x86 processor chip like the Pentium 4 that incorporates 5l2K bytes of cache and ignoring the cost of the processor core itself, we can arrive at an estimated cost per bit of one hundredth of a cent per bit (i.e., roughly $100,000 per gigabyte).

Register File. Finally, the fastest, smallest, and most expensive element in a modern memory hierarchy is the register file. The register file is responsible for supplying operands to the execution units of a processor at very low latency-usually a few hundred picoseconds, corresponding to a single processor cycle-and at very high bandwidth, to satisfy multiple execution units in parallel. Register file bandwidth can approach 200 Gbytes/s in a modern eight-issue processor like the IBM PowerPC 970, that operates at 2 GHz and needs to read two and write one 8-byte operand for each of the eight issue slots in each cycle. Estimating the cost per bit in the register file is virtually impossible without detailed knowledge of a particular design and its yield characteristics; suffice it to say that it is likely several orders of magnitude higher than our estimate of $100,000 per gigabyte for on-chip cache memory.

These memory hierarchy components are attached to the processor in a hierarchical fashion to provide an overall storage system that approximates the five idealisms-infinite capacity, infinite bandwidth, zero latency, persistence, and zero cost-as closely as possible. Proper design of an effective memory hierarchy requires careful analysis of the characteristics of the processor, the programs and operating system running on that processor, and a thorough understanding of the capabilities and costs of each component in the memory hierarchy. Table 3.1 summarizes some of the key attributes of these memory hierarchy components and illustrates that bandwidth can vary by four orders of magnitude, latency can vary by eight orders of magnitude, while cost per bit can vary by seven orders of magnitude. These drastic variations, which continue to change at nonuniform rates asspace for the system architect.



### 3.4.2 Temporal and Spatial Locality
How is it possible to design a memory hierarchy that reasonably approximates the infinite capacity and bandwidth, low latency, persistence, and low cost specified by the five memory idealisms? If one were to assume a truly random pattern of accesses to a vast storage space, the task would appear hopeless: the excessive cost of fast storage technologies prohibits large memory capacity, while the long latency and low bandwidth of affordable technologies violates the performance requirements for such a system. Fortunately, an empirically observed attribute of program execution called locality of reference provides an opportunity for designing the memory hierarchy in a manner that satisfies these seemingly contradictory requirements.

Locality of reference describes the propensity of computer programs to access the same or nearby memory locations frequently and repeatedly. Specifically, we can break locality of reference down into two dimensions: temporal locality and spatial locality. Both types of locality are common in both the instruction and data reference streams and have been empirically observed in both user-level application programs, shared library code, as well as operating system kernel code.

Temporal locality refers to accesses to the same memory location that occur close together in time; many real application programs exhibit this tendency for both program text or instruction references, as well as data references. Figure 3.3(a) annotates an example sequence of memory references with arrows representing temporal locality; each arrow connects an earlier and later memory reference to the same address. Temporal locality in the instruction reference stream can be easily explained, since it is caused by loops in program execution. As each iteration of a loop is executed, the instructions forming the body of the loop are fetched again  Furthermore, even programs that contain very few discernible loop structures can still share key subroutines that are called from various locations; each time the subroutine is called, temporally local instruction references occur.
Within the data reference stream, accesses to widely used program variables lead to temporal locality, as do accesses to the current stack frame in call-intensive programs. As call-stack frames are deallocated on procedure returns and reallocated on a subsequent call, the memory locations corresponding to the top of the stack are accessed repeatedly to pass parameters, spill registers, and return function results. All this activity leads to abundant temporal locality in the data access stream.

Spatial locality refers to accesses to nearby memory locations that occur close together in time. Figure 3.3(b) annotates an example sequence of memory references with arrows representing temporal locality; an earlier reference to some address (for example, A) is followed by references to adjacent or nearby addresses (A+l, A+2, A+3, and so on). Again, most real application programs exhibit this tendency for both instruction and data references. In the instruction stream, the instructions that make up a sequential execution path through the program are laid out sequentially in program memory. Hence, in the absence of branches or jumps, instruction fetches sequence through program memory in a linear fashion, where subsequent accesses in time are also adjacent in the address space, leading to abundant spatial locality. Even when branches or jumps cause discontinuities in fetching , the targets of branches and jumps are often nearby, maintaining spatial locality, though at a slightly coarser level.

Spatial locality within the data reference stream often occurs for algorithmic reasons. For example, numerical applications that traverse large matrices of data often access the matrix elements in serial fashion . As long as the matrix elements are laid out in memory in the same order they are traversed, abundant spatial locality occurs. Applications that stream through large data files , like audio MP3 decoder or encoders, also access data in a sequential, linear fashion, leading to many spatially local references. Furthermore, accesses to automatic variables in call-intensive environments also exhibit spatial locality, since the automatic variables for a given function are laid out adjacent to each other in the stack frame corresponding to the current function.

Of course, it is possible to write programs that exhibit very little temporal or spatial locality. Such programs do exist, and it is very difficult to design a costefficient memory hierarchy that behaves well for such programs. If these programs or classes of applications are deemed important enough, special-purpose high-cost systems can be built to execute them. In the past, many supercomputer designs optimized for applications with limited locality of reference avoided using many of the techniques introduced in this chapter (cache memories, virtual memory, and DRAM main memory), since these techniques require locality of reference in order to be effective. Fortunately, most important applications do exhibit locality and can benefit from these techniques. Hence, the vast majority of computer systems designed today incorporate most or all of these techniques.The principle of caching instructions and data is paramount in exploiting both temporal and spatial locality to create the illusion of a fast yet capacious memory.

Caches were first proposed by Wilkes [1965] and first implemented in the IBM System 360/85 in 1968 [Liptay, 1968]. Caching is accomplished by placing a small, fast, and expensive memory between the processor and a slow, large, and inexpensive main memory, and by placing instructions and data that exhibit temporal and spatial reference locality into this cache memory. References to memory locations that are cached can be satisfied very quickly, reducing average memory reference latency, while the low latency of a small cache also naturally provides high bandwidth. Hence, a cache can effectively approximate the second and third memory idealisms-infinite bandwidth and zero latency-for those references that can be satisfied from the cache. Since temporal and spatial locality are so prevalent in most programs, even small first-level caches can satisfy in excess of 90% of all references in most cases; such references are said to hit in the cache.

Those references that cannot be satisfied from the cache are called misses and must be satisfied from the slower, larger, memory that is behind the cache.


#### 3.4.3.1 Average Reference Latency. 
Caching can be extended to multiple levels
by adding caches of increasing capacity and latency in a hierarchical fashion, using the technologies enumerated in Table 3.1 . As long as each level of the cache is able to capture a reasonable fraction of the references sent to it, the reference latency perceived by the processor is substantially lower than if all references were sent directly to the lowest level in the hierarchy. The average memory reference latency can be computed using Equation (3.1), which computes the weighted average based on the distribution of references satisfied at each level in the cache.

The latency to satisfy a reference from each level in the cache hierarchy is defined as Ii' while the fraction of all references satisfied by that level is hi.

This equation makes clear that as long as the hit rates hi for the upper levels in the cache (those with low latency I;) are relatively high, the average latency observed by the processor will be very low. For example, a two-level cache hierarchy with hi = 0.95, II = 1 ns, h2 = 0.04,12 = 10 ns, h3 = 0.01, and 13 = 100 ns will deliver an average latency of 0.95 x 1 ns + 0.04 x 10 ns + 0.01 x 100 ns = 2.35 ns, which is nearly two orders of magnitude faster than simply sending each reference directly to the lowest level.



#### 3.4.3.2 Miss Rates and Cycles per Instruction Estimates.  Equation (3.1)
assumes that hi hit rates are specified as global hit rates, which specify the fraction of all memory references that hit in that level of the memory hierarchy. It is often useful to also understand local hit rates for caches, which specify the fraction of all memory references serviced by a particular cache that hit in that cache. For a firstlevel cache, the global and local hit rates are the same, since the first-level cachethird-level cache only services references that miss in the second-level cache, and so on. Hence, the local hit rate Ih; for cache level i is defined in Equation (3.2).


Returning to our earlier example, we see that the local hit rate of the second-level cache Ih; = 0.04/(1 - 0.95) = 0.8. This tells us that 0.8 or 80% of the references serviced by the second-level cache were also satisfied from that cache, while 1 - 0.8 = 0.2 or 20% were sent to the next level. This latter rate is often called a local miss rate, as it indicates the fraction of references serviced by a particular level in the cache that missed at that level. Note that for the first-level cache, the local and global hit rates are equivalent, since the first-level cache services all references. The same is true for the local and global miss rates of the first-level cache.

Finally, it is often useful to report cache miss rates as per-instruction miss rates.
This metric reports misses normalized to the number of instructions executed, rather than the number of memory references performed and provides an intuitive basis for reasoning about or estimating the performance effects of various cache organizations. Given the per-instruction miss rate m; and a specific execution-time penalty p; for a miss in each cache in a system, one can quickly estimate the performance effect of the cache hierarchy using the memory-time-per-instruction (MTPI) metric, as defined in Equation (3.3).

In this equation the p; term is not equivalent to the latency term I; used in Equation (3.1). Instead, it must reflect the penalty associated with a miss in level i of the hierarchy, assuming the reference can be satisfied at the next level. The miss penalties are computed as the difference between the latencies to adjacent levels in the hierarchy, as shown in Equation (3.4).

Returning to our earlier example, if hi = 0.95, II = 1 ns, h2 =0.04,12= 10 ns, h3= 0.01 , and 13 = 100 ns, then PI = (12 - II) = (10 ns - 1 ns) = 9 ns, which is the difference between the II and 12 latencies and reflects the additional penalty of missing the first level and having to fetch from the second level. Similarly, P2 = (/3 - 12) = (100 ns - 10 ns) = 90 ns, which is the difference between the 12 and 13 latencies.

The m ; miss rates are also expressed as per-instruction miss rates and need to be converted from the global miss rates used earlier. To perform this conversion, we need to know the number of references performed per instruction. If we assume thator stores, we have a total of n = (1 + 0.4) = 1.4 references per instruction. Hence, we can compute the per-instruction miss rates using Equation (3.5).

Returning to our example, we would find that ml = (1 - 0.95) X 1.4 = 0.07 misses per instruction, while m2 = [1 - (0.95 + 0.04)] X 1.4 = 0.014 misses per instruction.
Finally, substituting into Equation (3.3), we can compute the memory-time-perinstruction metric MTPI = (0.07 X 9 ns) + (0.014 X 90 ns) = 0.63 + 1.26 = 1.89 ns per instruction. This can also be conveniently expressed in terms of cycles per instruction by normalizing to the cycle time of the processor. For example, assuming a cycle time of 1 ns, the memory-cycles-per-instruction (MCPI) would be 1.89 cycles per instruction.

Note that our definition of MTPI in Equation (3.3) does not account for the latency spent servicing hits from the first level of cache, but only time spent for misses. Such a definition is useful in performance modeling, since it cleanly separates the time spent in the processor core from the time spent outside the core servicing misses. For example, an ideal scalar processor pipeline would execute instructions at a rate of one per cycle, resulting in a core cycles per instruction (CPI) equal to one. This CPI assumes that all memory references hit in the cache; a core CPI is also often called a perfect cache CPI, since the cache is perfectly able to satisfy all references with a fixed hit latency. As shown in Equation (3.6), the core CPI can be added to the MCPI computed previously to reach the actual CPI of the processor:

CPI = 1.0 + 1.89 = 2.89 cycles per instruction for our recurring example.
CPI = CoreCPI + MCPI 

However, one has to be careful using such equations to reason about absolute performance effects, since they do not account for any overlap or concurrency between cache misses. In Chapter 5, we will investigate numerous techniques that exist for the express purpose of maximizing overlap and concurrency, and we will see that performance approximations like Equation (3.3) are less effective at predicting the performance of cache hierarchies that incorporate such techniques.




#### 3.4.3.3 Effective Bandwidth. 
Cache hierarchies are also useful for satisfying
the second memory idealism of infinite bandwidth. Each higher level in the cache hierarchy is also inherently able to provide higher bandwidth than lower levels, due to its lower access latency, so the hierarchy as a whole manages to maintain the illusion of infinite bandwidth. In our recurring example, the latency of the firstlevel cache is 1 ns, so a single-ported nonpipelined implementation can provide a bandwidth of 1 billion references per second. In contrast, the second level, if also not pipelined, can only satisfy one reference every 10 ns, resulting in a bandwidth ofin the lower levels to provide greater effective bandwidth by either multiporting or banking (see Section 3.4.4.2 for an explanation of banking or interleaving) the cache or memory, or pipelining it so that it can initiate new requests at a rate greater than the inverse of the access latency. Goodman [1983] conducted a classic study of the bandwidth benefits of caches.



#### 3.4.3.4 Cache Organization and Design. 
Each level in a cache hierarchy must
be designed in a way that matches the requirements for bandwidth and latency at that level. Since the upper levels of the hierarchy must operate at speeds comparable to the processor core, they must be implemented using fast hardware techniques, necessarily limiting their complexity. Lower in the cache hierarchy, where latency is not as critical, more sophisticated schemes are attractive, and even software techniques are widely deployed. However, at all levels, there must be efficient policies and mechanisms in place for locating a particular piece or block of data, for evicting existing blocks to make room for newer ones, and for reliably handling updates to any block that the processor writes. This section presents a brief overview of some common approaches; additional implementation details are provided in Section 3.6.


Locating a Block. Each level must implement a mechanism that enables lowlatency lookups to check whether or not a particular block is cache-resident. There are two attributes that determine the process for locating a block; the first is the size of the block, and the second is the organization of the blocks within the cache.

Block size (sometimes referred to as line size) describes the granularity at which the cache operates. Each block is a contiguous series of bytes in memory and begins on a naturally aligned boundary. For example, in a cache with 16-byte blocks, each block would contain 16 bytes, and the first byte in each block would be aligned to 16-byte boundaries in the address space, implying that the low-order 4 bits of the address of the first byte would always be zero (i.e., Ob ... 0000). The smallest usable block size is the natural word size of the processor (i.e., 4 bytes for a 32-bit machine, or 8 bytes for a 64-bit machine), since each access will require the cache to supply at least that many bytes, and splitting a single access over multiple blocks would introduce unacceptable overhead into the access path. In practice, applications with abundant spatial locality will benefit from larger blocks, as a reference to any word within a block will place the entire block into the cache.

Hence, spatially local references that fall within the boundaries of that block can now be satisfied as hits in the block that was installed in the cache in response to the first reference to that block.
Whenever the block size is greater than 1 byte, the low-order bits of an address must be used to find the byte or word being accessed within the block. As stated above, the low-order bits for the first byte in the block must always be zero, corresponding to a naturally aligned block in memory. However, if a byte other than the first byte needs to be accessed, the low-order bits must be used as a block offset to index into the block to find the right byte. The number of bits needed for the block offset is the log2 of the block size, so that enough bits are available to Address span all the bytes in the block. For example, if the block size is 64 bytes, log2(64) =6 low-order bits are used as the block offset. The remaining higher-order bits are then used to locate the appropriate block in the cache memory.
The second attribute that determines how blocks are located, cache organization, determines how blocks are arranged in a cache that contains multiple blocks.

Figure 3.4 illustrates three fundamental approaches for organizing a cache that directly affect the complexity of the lookup process: direct-mapped, fully associative, and set-associative.
The simplest approach, direct-mapped, forces a many-to-one mapping between addresses and the available storage locations in the cache. In other words, a particular address can reside only in a single location in the cache; that location is usually determined by extracting n bits from the address and using those n bits as a direct index into one of 2n possible locations in the cache.

Of course, since there is a many-to-one mapping, each location must also store a tag that contains the remaining address bits corresponding to the block of data stored at that location. On each lookup, the hardware must read the tag and compare it with the address bits of the reference being performed to determine whether a hit or miss has occurred. We describe this process in greater detail in Section 3.6.

In the degenerate case where a direct-mapped memory contains enough storage locations for every address block (i.e., the n index bits include all bits of the address), no tag is needed, as the mapping between addresses and storage locations is now one-to-one instead of many-to-one. The register file inside the processor is an example of such a memory; it need not be tagged since all the address bits (all bits of the register identifier) are used as the index into the register file.

The second approach, fully associative, allows an any-to-any mapping between addresses and the available storage locations in the cache. In this organization, any memory address can reside anywhere in the cache, and all locations must be searched to find the right one; hence, no index bits are extracted from the address to determine the storage location. Again, each entry must be tagged withof the current reference. Whichever entry matches is then used to supply the data; if no entry matches, a miss has occurred.

The final approach, set-associative, is a compromise between the other two.
Here a many-to-few mapping exists between addresses and storage locations. On each lookup, a subset of address bits is used to generate an index, just as in the direct-mapped case. However, this index now corresponds to a set of entries, usually two to eight, that are searched in parallel for a matching tag. In practice, this approach is much more efficient from a hardware implementation perspective, since it requires fewer address comparators than a fully associative cache, but due to its flexible mapping policy behaves similarly to a fully associative cache. Hill and Smith [1989] present a classic evaluation of associativity in caches.

Evicting Blocks. Since each level in the cache has finite capacity, there must be a policy and mechanism for removing or evicting current occupants to make room for blocks corresponding to more recent references. The replacement policy of the cache determines the algorithm used to identify a candidate for eviction. In a direct-mapped cache, this is a trivial problem, since there is only a single potential candidate, as only a single entry in the cache can be used to store the new block, and the current occupant of that entry must be evicted to free up the entry.

In fully associative and set-associative caches, however, there is a choice to be made, since the new block can be placed in anyone of several entries, and the current occupants of all those entries are candidates for eviction. There are three common policies that are implemented in modern cache designs: first in, first out (FIFO), least recently used (LRU), and random.

The FIFO policy simply keeps track of the insertion order of the candidates and evicts the entry that has resided in the cache for the longest amount of time.
The mechanism that implements this policy is straightforward, since the candidate eviction set (all blocks in a fully associative cache, or all blocks in a single set in a set-associative cache) can be managed as a circular queue. The circular queue has a single pointer to the oldest entry which is used to identify the eviction candidate, and the pointer is incremented whenever a new entry is placed in the queue. This results in a single update for every miss in the cache.

However, the FIFO policy does not always match the temporal locality characteristics inherent in a program's reference stream, since some memory locations are accessed continually throughout the execution (e.g., commonly referenced global variables). Such references would experience frequent misses under a FIFO policy, since the blocks used to satisfy them would be evicted at regular intervals, as soon as every other block in the candidate eviction set had been evicted.

The LRU policy attempts to mitigate this problem by keeping an ordered list that tracks the recent references to each of the blocks that form an eviction set.
Every time a block is referenced as a hit or a miss, it is placed on the head of this ordered list, while the other blocks in the set are pushed down the list. Whenever a block needs to be evicted, the one on the tail of the list is chosen, since it has been referenced least recently (hence the name least recently used). Empirically, this policystoring an ordered list in hardware and updating that list, not just on every cache miss, but on every hit as well. Quite often, a practical hardware mechanism will only implement an approximate LRU policy, rather than an exact LRU policy, due to such implementation challenges. An instance of an approximate algorithm is the not-most-recently-used (NMRU) policy, where the history mechanism must remember which block was referenced most recently and victimize one of the other blocks, choosing randomly if there is more than one other block to choose from. In the case of a two-way associative cache, LRU and NMRU are equivalent, but for higher degrees of associativity, NMRU is less exact but simpler to implement, since the history list needs only a single element (the most recently referenced block).

The final policy we consider is random replacement. As the name implies, under this policy a block from the candidate eviction set is chosen at random.
While this may sound risky, empirical studies have shown that random replacement is only slightly worse than true LRU and still significantly better than FIFO.
Clearly, implementing a true random policy would be very difficult, so practical mechanisms usually employ some reasonable pseudo-random approximation for choosing a block for eviction from the candidate set.
Handling Updates to a Block. The presence of a cache in the memory subsystem implies the existence of more than one copy of a block of memory in the system. Even with a single level of cache, a block that is currently cached also has a copy still stored in the main memory. As long as blocks are only read, and never written, this is not a problem, since all copies of the block have exactly the same contents. However, when the processor writes to a block, some mechanism must exist for updating all copies of the block, in order to guarantee that the effects of the write persist beyond the time that the block resides in the cache. There are two approaches for handling this problem: write-through caches and writeback caches.

A write-through cache, as the name implies, simply propagates each write through the cache and on to the next level. This approach is attractive due to its simplicity, since correctness is easily maintained and there is never any ambiguity about which copy of a particular block is the current one. However, its main drawback is the amount of bandwidth required to support it. Typical programs contain about 15% writes, meaning that about one in six instructions updates a block in memory. Providing adequate bandwidth to the lowest level of the memory hierarchy to write through at this rate is practically impossible, given the current and continually increasing disparity in frequency between processors and main memories.

Hence, write-through policies are rarely if ever used throughout all levels of a cache hierarchy.
A write-through cache must also decide whether or not to fetch and allocate space for a block that has experienced a miss due to a write. A write-allocate policy implies fetching such a block and installing it in the cache, while a write-no-allocate policy would avoid the fetch and would fetch and install blocks only on read misses. The main advantage of a write-no-allocate policy occurs when streaming writes overwrite most or all of an entire block before any unwritten part of the(the fetched data is useless since it is overwritten before it is read).

A writeback cache, in contrast, delays updating the other copies of the block until it has to in order to maintain correctness. In a writeback cache hierarchy, an implicit priority order is used to find the most up-to-date copy of a block, and only that copy is updated. This priority order corresponds to the levels of the cache hierarchy and the order in which they are searched by the processor when attempting to satisfy a reference. In other words, if a block is found in the highest level of cache, that copy is updated, while copies in lower levels are allowed to become stale, since the update is not propagated to them. If a block is only found in a lower level, it is promoted to the top level of cache and is updated there, once again leaving behind stale copies in lower levels of the hierarchy.

The updated copy in a writeback cache is also marked with a dirty bit or flag to indicate that it has been updated and that stale copies exist at lower levels of the hierarchy. Ultimately, when a dirty block is evicted to make room for other blocks, it is written back to the next level in the hierarchy, to make sure that the update to the block persists. The copy in the next level now becomes the most up-to-date copy and must also have its dirty bit set, in order to ensure that the block will get written back to the next level when it gets evicted.

Writeback caches are almost universally deployed, since they require much less write bandwidth. Care must be taken to design these caches correctly, so that no updates are ever dropped due to losing track of a dirty cache line. We revisit writeback hierarchies in greater depth in Chapter 11 in the context of systems with multiple processors and multiple cache hierarchies.

However, despite the apparent drawbacks of write-through caches, several modern processors, including the IBM Power4 [Tendler et aI., 2001] and Sun UltraSPARC III [Lauterbach and Horel, 1999], do use a write-through policy for the first level of cache. In such schemes, the hierarchy propagates all writes to the second-level cache, which is also on the processor chip. Since the next level of cache is on the chip, it is relatively easy to provide adequate bandwidth for the write-through traffic, while the design of the first-level cache is simplified, since it no longer needs to serve as the sole repository for the most up-to-date copy of a cache block and never needs to initiate writebacks when dirty blocks are evicted from it. However, to avoid excessive off-chip bandwidth consumption due to write-throughs, the second-level cache maintains dirty bits to implement a writeback policy.

Figure 3.5 summarizes the main parameters-block size, block organization, replacement policy, write policy, and write-allocation policy-that can be used to describe a typical cache design.



#### 3.4.3.5 Cache Miss Classification. 
As discussed in Section 3.4.3.1, the average
reference latency delivered by a multilevel cache hierarchy can be computed as the average of the latencies of each level in the hierarchy, weighted by the global hit rate of each level. The latencies of each level are determined by the technology used and the aggressiveness of the physical design, while the miss rates are a function of the 1------ Block size

organization of the cache and the access characteristics of the program that is running on the processor. Attaining a deeper understanding of the causes of cache misses in a particular cache hierarchy enables the designer to realize the shortcomings of the design and discover creative and cost-effective solutions for improving the hierarchy. The 3 C's model proposed by Mark Hill [Hill, 1987] is a powerful and intuitive tool for classifying cache misses based on their underlying root cause. This model introduces the following mutually exclusive categories for cache misses:


* Cold or compulsory misses. These are due to the program' s first reference to a block of memory. Such misses are considered fundamental since they cannot be prevented by any caching technique.
* Capacity misses. These are due to insufficient capacity in a particular cache. Increasing the capacity of that cache can eliminate some or all capacity misses that occur in that cache. Hence, such misses are notfundamental, but rather a by-product of a finite cache organization.
* Conflict misses. These are due to imperfect allocation of entries in a particular cache. Changing the associativity or indexing function used by a cache can increase or decrease the number of conflict misses. Hence, again, such misses are not fundamental, but rather a by-product of an imperfect cache organization. A fully associative cache organization can eliminate all conflict misses, since it removes the effects of limited associativity or indexing functions.

Cold, capacity, and conflict misses can be measured in a simulated cache hierarchy by simulating three different cache organizations for each cache of interest.let's assume it experiences ma cache misses. The second organization is a fully associative cache with the same capacity and block size as the actual cache; it experiences mfcache misses. The third and final organization is a fully associative cache with the same block size but infinite capacity; it experiences mc misses. The number of cold, capacity, and conflict misses can now be computed as * Cold misses = mc' number of misses in fully associative, infinite cache.

* Capacity misses = mf - me> number of additional misses in finite but fully associative cache over infinite cache.
* Conflict misses =ma - mf' number of additional misses in actual cache over number of misses in fully associative, finite cache.
Cold misses are fundamental and are determined by the working set of the program in question, rather than by the cache organization. However, varying the block size directly affects the number of cold misses experienced by a cache.
Intuitively, this becomes obvious by considering two extreme block sizes: a cache with a block size of one word will experience a cold miss for every unique word referenced by the program (this forms the upper bound for the number of cold misses in any cache organization), while a cache with an infinite block size will experience only a single cold miss. The latter is true because the very first reference will install all addressable memory into the cache, resulting in no additional misses of any type. Of course, practical cache organizations have a finite block size somewhere between these two endpoints, usually in the range of 16 to 512 bytes.

Capacity misses are not fundamental but are determined by the block size and capacity of the cache. Clearly, as capacity increases, the number of capacity misses is reduced, since a larger cache is able to capture a larger share of the program's working set. In contrast, as block size increases, the number of unique blocks that can reside simultaneously in a cache of fixed capacity decreases. Larger blocks tend to be utilized more poorly, since the probability that the program will access all the words in a particular block decreases as the block gets bigger, leading to a lower effective capacity. As a result, with fewer unique blocks and a decreased probability that all words in each block are useful, a larger block size usually results in an increased number of capacity misses. However, programs that efficiently utilize all the contents of large blocks would not experience such an increase.

Conflict misses are also not fundamental and are determined by the block size, the capacity, and the associativity of the cache. Increased capacity invariably reduces the number of conflict misses, since the probability of a conflict between two accessed blocks is reduced as the total number of blocks that can reside in the cache simultaneously increases. As with capacity misses, a larger number of smaller blocks reduces the probability of a conflict and improves the effective capacity, resulting in likely fewer conflict misses. Similarly, increased associativity will almost invariably reduce the number of conflict misses. (Problem 25 in the homework will ask you to construct a counterexample to this case.)Interaction of cache organization and cache misses 

|Cache |Parameter |Cold Misses |Capacity Misses |Conflict Misses |Overall Misses
|-|-|-|-|-|-
|Reduced capacity |No effect |Increase |Likely increase |Likely increase
|Increased capacity |No effect |Decrease |Likely decrease |Likely decrease
|Reduced block size |Increase |Likely decrease |Likely decrease |Varies
|Increased block size |Decrease |Likely increase |Likely increase |Varies
|Reduced associativity |No effect |No effect |Likely increase |Likely increase
|Increased associativity |No effect |No effect |Likely decrease |Likely decrease
|Write back vs. writethrough |No effect |No effect |No effect |No effect
|Write-no-allocate |Possible decrease |Possible decrease |Possible decrease |Possible decrease

Table 3.2 summarizes the effects of cache organizational parameters on each category of cache misses, as well as overall misses. Note that some parameters can have unexpected effects, but empirical evidence tells us that for most programs, the effects are as summarized in Table 3.2. The possible decrease noted for writeno-allocate caches is due to blocks that are only written to and never read; these blocks are never fetched into the cache, and hence never incur any type of misses.

This directly reduces the number of cold misses and can indirectly reduce capacity misses, conflict misses, and overall misses.


#### 3.4.3.6 Example Cache Hierarchy. 
Figure 3.6 illustrates a typical two-level
cache hierarchy, where the CPU or processor contains a register file and is directly connected to a small, fast level-one instruction cache (L1 1-$) and a small, fast level-one data cache (Ll D-$). Since these first-level or primary caches are relatively small, typically ranging from 8 up to 64K bytes, they can be accessed quickly, usually in only a single processor cycle, and they can provide enough bandwidth to keep the processor core busy with enough instructions and data. Of course, only in rare cases do they provide enough capacity to contain all the working set of a program. Inevitably, the program will issue a reference that is not found in the first-level cache. Such a reference results in a cache miss, or a reference that needs to be forwarded to the next level in the memory hierarchy. In the case of the example in Figure 3.6, this is the level-two cache, which contains both program text and data and is substantially larger. Modem second-level caches range from 256K bytes to 16 Mbytes, with access latencies of a few nanoseconds up to 10 or 15 ns for large, off-chip level-two caches.

Modem processors usually incorporate a second level of cache on chip, while recent processor designs like the Intel Xeon and Itanium 2 actually add a third level of cache onboard the processor chip. High-end system designs like IBM xSeries 445«I-ns latency (typical) multiprocessors that employ Itanium 2 processors and are intended for extremely memory-intensive server applications even include a fourth level of cache memory on the system board.
Finally, the physical memory hierarchy is backed up by DRAM that ranges in size from 128 Mbytes in entry-level desktop PCs to 100 Gbytes or more in highend server systems. The latency for a reference that must be satisfied from DRAM is typically at least 100 ns, though it can be somewhat less in a single-processor system. Systems with multiple processors that share memory typically pay an overhead for maintaining cache coherence that increases the latency for main memory accesses, in some cases up to 1000 ns. Chapter 11 discusses many of the issues related to efficient support for coherent shared memory.

In light of the example shown in Figure 3.6, let's revisit the five memory idealisms introduced earlier in the chapter:
* Infinite capacity. For storing large data sets and large programs.
* Infinite bandwidth. For rapidly streaming these large data sets and programs to and from the processor.
* Instantaneous or zero latency. To prevent the processor from stalling while waiting for data or program code.
* Persistence or non volatility. To allow data and programs to survive even when the power supply is cut off.
* Zero or very low implementation cost.caches-are able to supply near-infinite bandwidth and very low average latency to the processor core, satisfying the second and third idealisms. The first idealisminfinite capacity-is satisfied by the lowest level of the memory hierarchy, since the capacities of DRAM-based memories are large enough to contain the working sets of most modern applications; for applications where this is not the case, Section 3.5 describes a technique called virtual memory that extends the memory hierarchy beyond random-access memory devices to magnetic disks, which provide capacities that exceed the demands of all but the most demanding applications.

The fourth idealism-persistence or nonvolatility--can also be supplied by magnetic disks, which are designed to retain their state even when they are powered down.
The final idealism-low implementation cost-is also satisfied, since the high per-bit cost of the upper levels of the cache hierarchy is only multiplied by a relatively small number of bits, while the lower levels of the hierarchy provide tremendous capacity at a very low cost per bit. Hence, the average cost per bit is kept near the low cost of commodity DRAM and magnetic disks, rather than the high cost of the custom SRAM in the cache memories.



### 3.4.4 Main Memory
In a typical modem computer system, the main memory is built from standardized commodity DRAM chips organized in a flexible, expandable manner to provide substantial capacity and expandability, high bandwidth, and a reasonably low access latency that should be only slightly higher than the access latency of the DRAM chips themselves. Since current-generation DRAM chips have a capacity of 256 megabits, a computer system with 1 Gbyte of memory would require approximately 32 DRAM chips for storage; including overhead for parity or error-correction codes to detect and tolerate soft errors would typically increase the count to 36 chips. Nextgeneration DRAM chips, which are just around the comer, will provide 1 gigabit of capacity each, reducing the chip count by a factor of 4. However, demand for increased memory capacity in future systems will likely keep the total number of DRAM chips required to satisfy that capacity relatively constant.

Clearly, there are many possible ways to configure a large number of DRAM chips to optimize for cost, latency, bandwidth, or expandability. Figure 3.7 illustrates one possible approach for arranging and interconnecting memory chips. In this configuration, multiple DRAM chips are mounted on a dual inline memory module (DIMM); multiple DIMMs are connected to a shared port or bank, and one or more banks are connected to a memory controller. In tum, the memory controller connects to the system' s processor bus and responds to the processor's memory requests by issuing appropriate commands to one or both memory banks. Section 3.4.4.1 introduces the basic principles of DRAM chip organization, and Section 3.4.4.2 discusses several key issues in memory controller design.



#### 3.4.4.1 DRAM Chip Organization. 
DRAM chips are a commodity product that
are manufactured by several competing vendors worldwide. DRAM manufacturers collaborate on standardizing the specification of the capacities and interfaces of proces~

each generation of DRAM chips in order to guarantee compatibility and consistent performance. Conceptually, the function and organization of DRAM chips is quite straightforward, since they are designed to store as many bits as possible in as compact an area as possible, while minimi zing area and product cost and maximizing bandwidth. While all these factors are considered in DRAM design, historically the primary design constraints have been capacity and cost. DRAM manufacturing is an extremely competitive business, where even minor increases in product cost, potentially caused by complex designs that reduce process yields, can drive a vendor out of business. Hence, DRAM vendors are typically very conservative about adopting dramatically new or different approaches for building DRAM chips.

As semiconductor process geometries have shrunk, DRAM capacity per chip has increased at a rate more or less directly proportional to Moore' s law, which predicts a doubling of devices per chip every two years or so. This has resulted in exponential growth in the capacity of DRAM chips with a fixed die size, which in turn has tended to hold product cost roughly constant. Despite the fact that device switching times improve with reduced process geometries, DRAM chip latency has not improved dramatically. This is due to the fact that DRAM access latency is dominated by wire delay, and not device switching times. Since wire delay has not improved nearly as dramatically as device switching delay, and the overall dimension of the memory array has remained largely fixed (to accommodate increased capacity), the end-to-end latency to retrieve a word from a DRAM chip has only improved at a compound rate of about 10% per year. This provides a stark contrastmicroprocessors. This divergence in device frequency has led many computer system designers and researchers to search for new techniques that will surmount what is known as the memory wall [Wulf and McKee, 1995].

The other main contributor to DRAM product cost-packaging, as driven by per-chip pin count-has also remained relatively stable over the years, resulting in a dearth of dramatic improvements in bandwidth per chip. The improvements that have been made for bandwidth have been largely in the realm of enhanced signaling technology, synchronous interfaces [synchronous DRAM (SDRAM)], higher interface frequencies (e.g., PC 100 which runs at 100 MHz, while PC133 runs at 133 MHz), and aggressive use of both rising and falling clock edges to transmit twice the amount of data per clock period [known as double-data rate (DDR)].

Figure 3.8 shows the internal organization of a typical DRAM chip. At its heart, there is an array of binary storage elements organized in rows and columns.
The storage elements are tiny capacitors, which store a charge to represent ai, or store no charge to represent a O. Each capacitor-based cell is connected by a transistor to a vertical bit line that stretches from the top of the array to the bottom.
The transistor is controlled by a horizontal word line which selects a particular row in the array for either reading or writing. The bit line is used to read the state of the cell: a charged capacitor will drive the bit line to a higher voltage; this higher voltage will be sensed by a high-gain amplifier at one end of the bit line that converts the signal to a standard logic level 1, while a discharged capacitor will drain  The high-gain analog amplifier is called a sense amp and relies on bit-line precharging, a process that presets the bit-line voltage prior to a read to an intermediate value that can swing high or low very quickly depending on the state of the accessed cell's capacitor. The bit line is also used to store a value in the cell by driving the bit line high to store a charge in the capacitor, or driving it low to drain the charge from the capacitor. Since the charge stored by a capacitor decays over time, cells in a DRAM chip must be refreshed periodically to maintain their state.

This dynamic behavior lends itself to the naming of DRAM chips; the acronym stands for dynamic random-access memory. In contrast, SRAM or static randomaccess memory, employed in higher levels of the cache hierarchy, does not need to be refreshed since the storage cells are static complementary metal-on-semiconductor (CMOS) circuits (a pair of cross-coupled inverters) that can hold their state indefinitely, as long as power is supplied to them.

DRAM Addressing. The word lines in Figure 3.8 are used to select a row within the array to either read from that row (i.e., let the capacitors from that row drive the bit lines) or write to it (i.e., let the bit lines drive or drain the capacitors in that row). A row address of n bits must be supplied to the DRAM chip, and a decoder circuit activates one of the 2 n word lines that corresponds to the supplied row address. At each storage cell, the word line controls a pass transistor that either isolates the capacitor from the bit line or connects it to the bit line, to enable selective reading or writing of a single row. During a read, the bits in the selected row are sensed by the sense amps and are stored in a row buffer. In a subsequent command cycle, a column address of m bits must also be supplied; this is used to select one of 2 m words from within the row .

DRAM Access Latency. The latency to access a random storage location within a DRAM chip is determined by the inherent latency of the storage array augmented by the overhead required to communicate with the chip. Since DRAM chips are very cost-sensitive, and an increased pin count drives cost higher, DRAM interfaces typically share the same physical pin for several purposes. For example, a DRAM chip does not provide enough address pins to directly address any word within the array; instead, the address pins are time-multiplexed to first provide the row address when the row address strobe (RAS) control line is asserted, followed by a column address strobe (CAS) while the column address is provided. As long as there are an equal number of rows and columns, only half the number of address pins are needed, reducing cost significantly. On the other hand, two transactions are required across the interface to provide a complete address, increasing the latency of an access.

The decoupling of the row and column addresses creates an opportunity for optimizing the access latency for memory references that exhibit spatial locality.
Since the DRAM chip reads the entire row into a row buffer, and then selects a word out of that row based on the column address, it is possible to provide multiple column addresses in back-to-back cycles and read bursts of words from the same row at a very high rate, usually limited only by the interface frequency and data rate.accesses. These same-row accesses complete much faster (up to 3 times faster in current-generation DRAM) since they do not incur the additional latency of providing a row address, decoding the row address, precharging the bit lines, and reading the row out of the memory cell array into the row buffer. This creates a performance-enhancing scheduling opportunity for the memory controller, which we will revisit in Section 3.4.4.2.

DRAM chips also share the pins of the data bus for both reads and writes.
While it is easy to pipeline a stream of reads or a stream of writes across such an interface, alternating reads with writes requires a bus turnaround. Since the DRAM chip is driving the data bus during reads, while the memory controller is driving it during writes, care must be taken to ensure that there is never a time during which both the memory controller and DRAM chip are attempting to drive the bus, as this could result in a short-circuit condition between the respective drivers. Hence, the interface must be carefully designed to allow enough timing margin for the current bus master (e.g., the DRAM chip on a read) to stop driving the bus before the new bus master (e.g., the memory controller on a write) starts to drive the bus. This results in additional delay and reduces the efficiency of the DRAM interface.

Rambus DRAM. In the late 1990s, an alternative standard called Rambus DRAM (RDRAM) emerged from research in high-speed signaling at Stanford University. RDRAM employed many of the techniques that more recent standards for synchronous DRAM (e.g., DDR2) have since adopted, including advanced signaling, higher interface frequencies, and multiple data words per clock period.

RDRAM chips also provide a row buffer cache which contains several of the most recently accessed DRAM rows; this increases the probability that a random access can be satisfied with much lower latency from one of the row buffer entries, avoiding the row access latency. To improve interface bandwidth, RDRAM carefully specifies the physical design for board-level traces used to connect RDRAM chips to the controller, and uses source-synchronous clocking (i.e., the clock signal travels with the data) to drive clock frequencies several times higher than more conventional SDRAM approaches. As a result of these optimizations, RDRAM is able to provide substantially higher bandwidth per pin, albeit at a higher product cost and increased design time due to the stringent physical design requirements.

High bandwidth per pin is very useful in systems that require lots of bandwidth but relatively little capacity. Examples of such systems that use RDRAM are the Sony Playstation 2 and Microsoft X-Box game controllers, which provide only 32 Mbytes of memory in their base configuration while requiring lots of memory bandwidth to support intensive three-dimensional gaming. A modest capacity requirement of only 32 Mbytes could be satisfied by a single currentgeneration 256-Mbit DRAM chip. However, since that DRAM chip only has a small number of data pins (between 2 and 16, typically 4 or 8), each pin must provide very high bandwidth to satisfy the system's overall bandwidth demand.

In contrast, general-purpose computer systems such as personal computers typically contain at least an order of magnitude more memory, ranging from atimportant, since many DRAM chips are required to provide the requisite capacity anyway, and these chips can be arranged in parallel to provide a wide interface that supplies the required bandwidth. Of course, this increases the package cost of the memory controller chip, since it has to have enough pins to access multiple DRAM chips in parallel. On the other hand, product cost can be lowered by using less aggressive and less expensive circuit board technology, since each pin signals at a lower rate.

Detailed performance evaluation and comparison of various modem DRAM technologies can be found in two recent studies [Cuppu et aI., 1999; Cuppu and Jacob, 2001].


#### 3.4.4.2 Memory Controller Organization. 
Memory controllers serve as the
interface between a system' s processor bus, which communicates reads and writes issued by the processor, and the standard DRAM command interface, which expects a tightly specified sequence of commands for supplying row addresses, column addresses, and read or write commands to each DRAM chip. There are many alternatives for arranging the mapping between the physical addresses provided by the processor and the DRAM addresses needed to access the actual storage locations. Furthermore, there are various optimizations that can be applied within the memory controller to improve read performance and increase spatial locality in the DRAM reference stream. The net effect is that the design of the memory controller can substantially impact sustainable memory bandwidth and the observed latency of memory references. The following sections discuss some of these issues in detail.

Memory Module Organization. The desired memory capacity for a system determines the total number of DRAM chips needed to provide that capacity. In practice, most systems are designed to support a range of capacities to enable use of several generations of memory chips and to allow for future expansion from an initial configuration. However, for simplicity we will assume a fixed capacity only, resulting in a fixed number of DRAM chips in the memory subsystem. Figure 3.9 illustrates four possible organizations for four DRAM chips in a system, as determined by two fundamental attributes: serial vs. parallel and interleaved vs. non-interleaved. In the top left case, the DRAM chips all share the same address, command, and data lines, but only a single chip is active at a time when it is selected via its chip select (CS) control line. In this case, the number of pins required in the controller is minimized, since all pins except for the CS control lines are shared. However, data bandwidth is limited by the data bus width of each DRAM chip. DRAM chips have 2, 4, 8, or 16 data lines-typically 4 or 8 in current-generation chips-with a price premium charged for wider interfaces. In this organization, transaction bandwidth is restricted to one concurrent command, since the address and command lines are shared across all chips.

The top right case in Figure 3.9 shows a parallel organization, in which all chips are active for all commands (hence, no chip selects), and the n-bit data busses from each chip are concatenated to form a 4n-bit interface. This configuration provides much better data bandwidth, but at a higher memory controller cost due to 133

the private data bus pins for each DRAM chip. It provides no increase in transaction bandwidth, since the address and command lines are still shared.
The bottom left shows an interleaved (or banked) organization where half the DRAM chips are connected to one set of chip select, address, command, and data busses, while the other half is connected to a second set. This organization provides twice the data bandwidth, and also twice the transaction bandwidth, since each interleaved bank can now operate independently, at the cost of twice as many address and command pins. The final configuration, on the bottom right, combines the parallel and interleaved organizations, providing twice the transaction bandwidth through interleaving, and four times the data bandwidth through two 2n-bit wide data busses. Of course, this final configuration requires the highest cost and largest number of pins in the memory controller.

There are other possible combinations of these techniques (for example, a semi-parallel scheme where only half-instead of all-DRAM chips are in parallel, and chip selects are still used to select which half drives the 2n-bit wide data bus).
In all these combinations, however, the linear physical address presented by the processor needs to be translated to an n-tuple that describes the actual physical location of the memory location being addressed. Table 3.3 summarizes the contents of the DRAM address n-tuple for several approaches for organizing memory modules. Linear physical addresses are mapped to these n-tuples by selecting appropriately sized subsets of bits from the physical address to form each element Translating linear physical address to DRAM address Organization Examples assume 4 x 256-kbit DRAM with 8-bit data path and 8-kbit row, for a total of 128 kB of addressable memory.

of the n-tuple. In general, this selection problem has (x) possible solutions, where x is the number of physical address bits and y is the total number of bits needed to specify all the n-tuple elements. Table 3.3 shows only one of these many possible ways of choosing the bits for each element in the DRAM address n-tuple.

Regardless of which organization is chosen, the RAS bits should be selected to maximize the number of row hits; careful study of the access patterns of important applications can reveal which address bits are the best candidates for RAS . Furthermore, in an interleaved design , the bits used for bank selection need to be selected carefully to ensure even distribution of references across the memory banks, since a poor choice of bank bits can direct all references to a single bank, negating any bandwidth benefit expected from the presence of multiple banks.

Components of a Memory Controller. As shown in Figure 3.7, a memory controller contains more than just an interface to the processor bus and an interface to the DRAM chips. It also contains logic for buffering read and write commands from the processor bus (the ReadQ and WriteQ), a response queue (RespQ) for buffering responses heading back to the processor, scheduling logic for issuing DRAM commands to satisfy the processor' s read and write requests, and buffer space to assemble wide data responses from multiple narrow DRAM reads. This reassembly is needed whenever the processor bus issues read commands that are wider than the DRAM data interface; in such cases multiple DRAM reads have to be performed to assemble a block that matches the width of the processor' s read request (which is usually the width of a cache block in the processor' s cache). In a similar fashion , wide writes from the processor bus need to be decomposed into multiple narrower writes to the DRAM chips.

Although Figure 3.7 shows the memory controller as a physically separate entity, recent designs, exemplified by the AMD Opteron [Keltcher et aI., 2003], integrate the memory controller directly on chip to minimize memory latency, simplify the chipset design, and reduce overall system cost. One of the drawbacks of an on-chip memorystandards. As an example, the Opteron processors must be redesigned to take advantage of the new DDR2 DRAM standard, since the onboard controller will only work the older DDR standard. In contrast, an off-chip memory controller (North Bridge in the IntelJPC terminology) can be more quickly redesigned and replaced to match new memory standards.

The ReadQ is used to buffer multiple outstanding reads; this decouples completion of a read from accepting the next one. Quite often the processor will issue reads in bursts, as cache misses tend to occur in clusters and accepting multiple reads into the ReadQ prevents the bus from stalling. Queueing up mUltiple requests may also expose more locality that the memory controller can exploit when it schedules DRAM commands. Similarly, the WriteQ prevents the bus and processor from stalling by allowing multiple writes to be outstanding at the same time. Furthermore, the WriteQ enables a latency-enhancing optimization for reads: since writes are usually not latency-critical, the WriteQ can delay them in favor of outstanding reads, allowing the reads to be satisfied first from the DRAM. The delayed writes can be retired whenever there are no pending reads, utilizing idle memory channel cycles.

Memory Reference Scheduling. Of course, reference reordering in the memory controller is subject to the same correctness requirements as a pipelined processor for maintaining read-after-write (RAW), write-after-read (WAR), and write-afterwrite (W AW) dependences. In effect, this means that reads cannot be reordered past pending writes to the same address (RAW), writes cannot be reordered past pending reads from the same address (WAR), and writes cannot bypass pending writes to the same address (WA W). If we are only reordering reads with respect to outstanding writes, only the RAW condition needs to be checked. If a RAW condition exists between a pending write and a newer read, the read must either stall and wait for the write to be performed against the DRAM, or the read can be satisfied directly from the write queue. Either solution will maintain correctness, while the latter should improve performance, since the latency of the read from the on-chip WriteQ will be lower than a read from an external DRAM chip.

However, in Section 3.4.4.1 we showed how DRAM chips can exploit spatial locality by fetching multiple words from the same row by issuing different column addresses to the DRAM in back-to-back cycles. These references can be satisfied much more quickly than references to different rows, which incur the latency for a row address transfer and row read in the internal DRAM array. In current-generation DRAMs, rows can be as large as 8 kilobits; an eight-wide parallel organization (extrapolating from the two-wide parallel scheme shown in Figure 3.9) would result in an 8-kilobits row in physical memory. Accesses to the same row can be satisfied much more quickly than references to other rows. Hence, the scheduling logic in advanced memory controllers will attempt to find references to the same row in the ReadQ and WriteQ and attempt to schedule them together to increase the number of row hits. This type of scheduling optimization can substantially reduce average DRAM read latency and improve sustained memory bandwidth, but can dramatically complicate the scheduling logic as well as the ReadQ and WriteQ bypass logic.typical chip will contain four independent banks that replicate most of the structures shown in Figure 3.8, while sharing the external address, data, and control lines.

Internal DRAM banking allows the memory controller to overlap different types of commands to each bank; for example, bank 0 can begin a bit-line precharge cycle while bank 1 is performing a row access and bank 2 is performing a column access.
Furthermore, each bank has a separate row buffer, which allows the memory controller to leave multiple rows open concurrently, increasing the probability that a future request will hit an open row, reducing the access latency for that request.
Finally, banking or interleaving the DRAM interface increases the transaction bandwidth for the memory controller, since multiple banks can operate independently and concurrently, as long as the ReadQ and WriteQ contain references to different banks. High-end memory controllers in multiprocessor server systems have many independent memory banks; commodity PC systems typically have one or two.

As a final note, the parallel and interleaved organizations described here for DRAM systems can also be applied to SRAM caches in higher levels of the memory hierarchy. In particular, multi banked caches are commonly used to increase transaction bandwidth to a cache. For example, the Intel Pentium processor incorporates an eight-way interleaved primary data cache to support concurrent memory accesses from its dual pipelines [Intel Corp., 1993]. Similarly, the IBM Power four-chip multiprocessor that is described in Chapter 6 has a three-way interleaved on-chip level-2 cache to support concurrent requests from the two processor cores that are on the chip [Tendler et aI., 2001].




## 3.5

Virtual Memory Systems

So far, we have only considered levels of the memory hierarchy that employ randomaccess storage technology. However, in modern high-performance computer systems, the lowest level of the memory hierarchy is actually implemented using magnetic disks as a paging device or backing store for the physical memory, comprising a virtual memory system. The backing store contains blocks of memory that have been displaced from main memory due to capacity reasons, just the same as blocks are displaced from caches and placed either in the next level of the cache hierarchy or in main memory.

Historically, virtual memory predates caches and was first introduced 40 years ago in time-shared mainframe computers to enable sharing of a precious commoditythe main memory-among multiple active programs [Kilburn et aI., 1962]. Virtual memory, as the name implies, virtualizes main memory by separating the programmer's view of memory from the actual physical placement of blocks in memory.

It does so by adding a layer of cooperating hardware and software that manages the mappings between a program's virtual address and the physical address that actually stores the data or program text being referenced. This process of address translation is illustrated in Figure 3.10. The layer of cooperating hardware and software that enables address translation is called the virtual memory system and is Y

responsible for maintaining the illusion that all virtually addressable memory is resident in physical memory and can be transparently accessed by the program, while also efficiently sharing the limited physical resources among competing demands from multiple active programs.
In contrast, time-sharing systems that predated or failed to provide virtual memory handicapped users and programmers by requiring them to explicitly manage physical memory as a shared resource. Portions of physical memory had to be statically allocated to concurrent programs; these portions had to be manually replaced and evicted to allocate new space; and cumbersome techniques such as data and program overlays were employed to reduce or minimize the amount of space consumed by each program. For example, a program would have to explicitly load and unload overlays that corresponded to explicit phases of program execution, since loading the entire program and data set could either overwhelm all the physical memory or starve other concurrent programs.

Instead, a virtual memory system allows each concurrent program to allocate and occupy as much memory as the system's backing store and its virtual address space allows: up to 4 Gbytes for a machine with 32-bit virtual addresses, assuming adequate backing store is available. Meanwhile, a separate demand paging mechanism manages the placement of memory in either the limited physical memory or in the system's capacious backing store, based on the policies of the virtual memory system. Such a system is responsible for providing the illusion that all virtually addressable memory is resident in physical memory and can be transparently accessed by the program.

The illusion of practically infinite capacity and a requirement for transparent access sound quite similar to the principles for caching described in Section 3.4.3; in fact, the underlying principles of temporal and spatial locality, as well as policies for locating, evicting, and handling updates to blocks, are all conceptually very similar in virtual memory subsystems and cache memories. However, since the relative latencies for accessing the backing store are much higher than the latencies for satisfying a cache miss from the next level of the physical memory hierarchy, the policies and particularly the mechanisms can and do differ substantially. A reference to a block that resides only in the backing store inflicts 10 ms or more of latency to read the block from disk. A pure hardware replacement scheme that stalls the processor while waiting for this amount of time would result in very poor utilization, since 10 ms corresponds to approximately 10 million instruction execution opportunities in a processor that executes one instruction per nanosecond. Hence, virtual memory subsystems are implemented as a hybrid of hardware and software, where references to blocks that reside in physical memory are satisfied quickly and efficiently by the hardware, while references that miss invoke the operating system through a page fault exception, which initiates the disk transfer but is also able to schedule some other, ready task to execute in the window of time between initiating and completing the disk request. Furthermore, the operating system now becomes responsible for implementing a policy for evicting blocks to allocate space for the new block being fetched from disk. We will study these issues in further detail in Section 3.5.1.

However, there is an additional complication that arises from the fact that multiple programs are sharing the same physical memory: they should somehow be protected from accessing each others' memory, either accidentally or due to a malicious program attempting to spy on or subvert another concurrent program. In a typical modem system, each program runs in its own virtual address space, which is disjoint from the address space of any other concurrent program. As long as there is no overlap in address spaces, the operating system need only ensure that no two concurrent address mappings from different programs ever point to the same physical location, and protection is ensured. However, this can limit functionality, since two programs cannot communicate via a shared memory location, and can also reduce performance, since duplicates of the same objects may need to exist in memory to satisfy the needs of multiple programs. For these two reasons, virtual memory systems typically provide mechanisms for protecting the regions of memory that they map into each program's address space; these protection mechanisms allow efficient sharing and communication to occur. We describe them further in Section 3.5.2.

Finally, a virtual memory system must provide an architected means for translating a virtual address to a physical address and a structure for storing these mappings. We outline several schemes for doing so in Section 3.5.3.



### 3.5.1 Demand Paging
Figure 3.11 shows an example of a single process that consumes virtual address space in three regions: for program text (to load program binaries and shared libraries); for the process stack (for activation records and automatic storage); and for the process heap (for dynamically allocated memory). Not only are these three regions noncontiguous, leaving unused holes in the virtual address space, but each of these regions can be accessed relatively sparsely. Practically speaking, only the regions that are currently being accessed need to reside in physical memory (shown as shaded in the figure), while the unaccessed or rarely accessed regions can be stored on the paging device or backing store, enabling the use of a system with a limited amount of physical memory for programs that consume large fractions of their address space, or, alternatively, freeing up main memory for other applications in a time-shared system.

A virtual memory demand paging system must track regions of memory at some reasonable granularity. Just as caches track memory in blocks, a demand paging system must choose some page size as the minimum granularity for locating and evicting blocks in main memory. Typical page sizes in current-generation systems are 4K or 8K bytes. Some modem systems also support variable-sized pages or multiple page sizes to more efficiently manage larger regions of memory. However, we will restrict our discussion to fixed-size pages.

Providing a virtual memory subsystem relieves the programmer from having to manually and explicitly manage the program's use of physical memory. Furthermore, it enables efficient execution of classes of algorithms that use the virtual address space greedily but sparsely, since it avoids allocating physical memory for untouched regions of virtual memory. Virtual memory relies on lazy allocation to achieve this very purpose: instead of eagerly allocating space for a program's needs, it defers allocation until the program actually references the memory.

This requires a means for the program to communicate to the virtual memory subsystem that it needs to reference memory that has previously not been accessed. In a demand-paged system, this communication occurs through a pagefault exception. Initially, when a new program starts up, none of its virtual address space may be allocated in physical memory. However, as soon as the program attempts to fetch an instruction or perform a load or store from a virtual memory location that is not currently in virtual memory, a page fault occurs. The hardware registers a page fault whenever it cannot find a valid translation for the current virtual address. This is conceptually very similar to a cache memory experiencing a miss whenever it cannot find a matching tag when it performs a cache lookup.rather, it transfers control to the operating system, which then allocates a page for the virtual address, creates a mapping between the virtual and physical addresses, installs the contents of the page into physical memory (usually by accessing the backing store on a magnetic disk), and returns control to the faulting program. The program is now able to continue execution, since the hardware can satisfy its virtual address reference from the corresponding physical memory location.

Detecting a Page Fault. To detect a page fault, the hardware must fail to find a valid mapping for the current virtual address. This requires an architected structure that the hardware searches for valid mappings before it raises a page fault exception to the operating system. The operating system's exception handler code is then invoked to handle the exception and create a valid mapping. Section 3.5.3 discusses several schemes for storing such mappings.

Page Allocation. Allocating space for a new virtual memory page is similar to allocating space for a new block in the cache, and depends on the page organization. Current virtual memory systems all use a fully-associative policy for placing virtual pages in physical memory, since it leads to efficient use of main memory, and the overhead of performing an associative search is not significant compared to the overall latency for handling a page fault. However, there must be a policy for evicting an active page whenever memory is completely full. Since a leastrecently-used (LRU) policy would be too expensive to implement for the thousands of pages in a reasonably sized physical memory, some current operating systems use an approximation of LRU called the clock algorithm. In this scheme, each page in physical memory maintains a reference bit that is set by the hardware whenever a reference occurs to that page. The operating system intermittently clears all the reference bits. Subsequent references will set the page reference bits, effectively marking those pages that have been referenced recently. When the virtual memory system needs to find a page to evict, it randomly chooses a page from the set of pages with cleared reference bits. This scheme avoids evicting pages that have been referenced since the last time the reference bits were cleared, providing a very coarse approximation of the LRU policy.

Alternatively, the operating system can easily implement a FIFO policy for evicting pages by maintaining an ordered list of pages that have been fetched into main memory from the backing store. While not optimal, this scheme can perform reasonably well and is easy to implement since it avoids the overhead of the clock algorithm.

Once a page has been chosen for eviction, the operating system must place it in the backing store, usually by performing a write of the contents of the page to a magnetic disk. This write can be avoided if the hardware maintains a change bit or dirty bit for the page, and the dirty bit is not set. This is similar in principle to the dirty bits in a writeback cache, where only the blocks that have their dirty bit set need to be written back to the next level of the cache hierarchy when they are evicted.

Accessing the Backing Store. The backing store needs to be accessed to supply the paged contents of the virtual page that is about to be installed in physical memory.

Typically, this involves issuing a read to a magnetic disk, which can have a latency exceeding 10 ms. Multitasking operating systems will put a page-faulting task to sleep for the duration of the disk read and will schedule some other active task to run on the processor instead.
Figure 3.12 illustrates the steps that occur to satisfy a page fault: first, the current process 1 fails to find a valid translation for a memory location it is attempting to access; the operating system supervisor is invoked to search the page table for a valid translation via the page fault handler routine; failing to find a translation, the supervisor evicts a physical page to make room for the faulting page and initiates an I/O read to the backing store to fetch the page; the supervisor scheduler then runs to find a ready task to occupy the CPU while process 1 waits for the page fault to be satisfied; process 2 runs while the backing store completes the read; the supervisor is notified when the read completes, and runs its scheduler to find the waiting process 1; finally, process 1 resumes execution on the CPU.




### 3.5.2 Memory Protection
A system that time-shares the physical memory system through the use of virtual memory allows the physical memory to concurrently contain pages from multiple processes. In some scenarios, it is desirable to allow multiple processes to access the same physical page, in order to enable communication between those processes or to avoid keeping duplicate copies of identical program binaries or shared libraries in memory. Furthermore, the operating system kernel, which also has resident physical pages, must be able to protect its internal data structures from user-level programs.

The virtual memory subsystem must provide some means for protecting shared pages from defective or malicious programs that might corrupt their state.
Furthermore, even when no sharing is occurring, protecting various address ranges from certain types of accesses can be useful for ensuring correct execution or for debugging new programs, since erroneous references can be flagged by the protection mechanism.write, and execute permissions. The hardware is then responsible for checking that instruction fetches occur only to pages that grant execute permission, loads occur only to pages that grant read permission, and writes occur only to pages that grant write permission. These permissions are maintained in parallel with the virtual to physical translations and can only be manipulated by supervisor-state code running in the operating system kernel. Any references that violate the permissions specified for that page will be blocked, and the operating system exception handler will be invoked to deal with the problem, usually resulting in termination of the offending process.

Permission bits enable efficient sharing of read-only objects like program binaries and shared libraries. If there are multiple concurrent processes executing the same program binary, only a single copy of the program needs to reside in physical memory, since the kernel can map the same physical copy into the address space of each process. This will result in multiple virtual-physical address mappings where the physical address is the same. This is referred to as virtual address aliasing.

Similarly, any other read-only objects can be shared. Furthermore, programs that need to communicate with each other can request shared space from the operating system and can communicate directly with each other by writing to and reading from the shared physical address. Again, the sharing is achieved via multiple virtual mappings (one per process) to the same physical address, with appropriate read and/or write permissions set for each process sharing the memory.

3.S.3 Page Table Architectures
The virtual address to physical address mappings have to be stored in a translation memory. The operating system is responsible for updating these mappings whenever they need to change, while the processor must access the translation memory to determine the physical address for each virtual address reference that it performs.

Each translation entry contains the fields shown in Figure 3.13: the virtual address, the corresponding physical address, permission bits for reading (Rp), writing (Wp), and executing (Ep), as well as reference (Ref) and change (Ch) bits, and possibly a caching-inhibited bit (Ca). The reference bit is used by the demand paging systems eviction algorithm to find pages to replace, while the change bit plays the part of a 
dirty bit, indicating that an eviction candidate needs to be written back to the backing store. The caching-inhibited bit is used to flag pages in memory that should not, for either performance or correctness reasons, be stored in the processor's cache hierarchy. Instead, all references to such addresses must be communicated directly through the processor bus. We will learn in Section 3.7.3 how this caching-inhibited bit is vitally important for communicating with 110 devices with memory-mapped control registers.

The translation memories are usually called page tables and can be organized either as forward page tables or inverted page tables (the latter are often called hashed page tables as well). At its simplest, a forward page table contains a page table entry for every possible page-sized block in the virtual address space of the process using the page table. However, this would result in a very large structure with many unused entries, since most processes do not consume all their virtual address space. Hence, forward page tables are usually structured in multiple levels, as shown in Figure 3.14. In this approach, the virtual address is decomposed into multiple sections. The highest-order bits of the address are added to the page table base register (PTBR), which points to the base of the first level of the page table. This first lookup provides a pointer to the next table; the next set of bits from the virtual address are added to this pointer to find a pointer to the next level.

Finally, this pointer is added to the next set of virtual address bits to find the final leaf-level page table entry, which provides the actual physical address and permission bits corresponding to the virtual address. Of course, the multilevel page table can be extended to more than the three levels shown in Figure 3.14.

A multilevel forward page table can efficiently store translations for a sparsely populated virtual address space, since leaf nodes are only needed for those portions Multi level Forwa rd Page Tabl e.are lazily allocated only when the operating system actually allocates storage in those portions of the address space. Furthermore, the page table entries themselves can be stored in virtual memory, allowing them to be paged out to the backing store.
This can lead to nested page faults, when the initial page fault experiences a second page fault as it is trying to find the translation information for its virtual address. If paging of the page table is allowed, the root level of the page table needs to remain resident in physical memory to avoid an unserviceable page fault.

An alternative page table organization derives from the observation that there is little motivation to provide translation entries for more pages than can actually fit in physical memory. In an inverted page table, there are only enough entries to map all the physical memory, rather than enough entries to map all the virtual memory. Since an inverted page table has far fewer entries and fits comfortably into main memory, there is no need to make it pageable. Rather, the operating system can access it directly with physical addressing.

Figure 3.15 illustrates how translation entries are found in an inverted or hashed page table. The virtual address is hashed, usually by applying an exclusive-OR function to nonoverlapping portions of the virtual address, and is added to the page table base register. The resulting address is used directly as a physical address to find a set of page table entries (PTEO through PTE3 in Figure 3.15). These page table entries are then checked sequentially to find a matching entry. Multiple entries need to be searched and provided, since it is possible for multiple virtual addresses to hash to the same location. In fact, it is possible for the number of virtual page numbers that map to the same page table entry group to exceed the capacity of the group; this results in an overflow condition that induces additional page faults. In effect, space in physical memory is now allocated in a set-associative manner, rather than a the PowerPC virtual memory architecture, which uses a hashed page table, they are further mitigated by providing a secondary hashing scheme that differs substantially from the primary hash. Whenever the primary page table entry group fails to provide a valid translation, the secondary hash is used to find a second group that is also searched. The probability of failing to find a valid translation in either of the two groups is further minimized, though still not completely avoided.

One drawback of an inverted page table is that it only contains mappings for resident physical pages. Hence, pages that have been evicted from physical memory to the backing store need their mappings stored elsewhere. This is handled by the operating system, which maintains a separate software page table for tracking pages that reside in the backing store. Of course, this software page table maintains mapping from virtual addresses to the corresponding disk blocks, rather than to physical memory addresses.

As a final alternative, page tables need not be architected to reside in physical memory in a particular organization. Instead, a structure called the translation lookaside buffer (TLB, further described in Section 3.6 and illustrated in Figures 3.21 and 3.22) can be defined as part of the supervisor state of the processor. The TLB contains a small number (typically 64) of entries that look just like the entry illustrated in Figure 3.13, but arranged in a fully-associative fashion. The processor must provide fast associative lookup hardware for searching this structure to translate references for every instruction fetch, load, or store. Misses in an architected TLB result in page faults, which invoke the operating system. The operating system uses its own page table or other mapping structure to find a valid translation or create a new one and then updates the TLB using supervisor-mode instructions that can directly replace and update entries in the TLB. In such a scheme, the operating system can structure the page table in whatever way it deems best, since the page table is searched only by the page fault handler software, which can be modified to adapt to a variety of page table structures. This approach to handling translation misses is called a software TLB miss handler and is specified by the MIPS, Alpha, and SPARC instruction set architectures.

In contrast, a processor that implements an architecture that specifies the page table architecture provides a hardware state machine for accessing memory to search the page table and provide translations for all memory references. In such an architecture, the page table structure is fixed, since not just the operating system page fault handler has to access it, but a hardware state machine must also be able to search it. Such a system provides a hardware TLB miss handler. The PowerPC and Intel IA-32 instruction set architectures specify hardware TLB miss handlers.




## 3.6

Memory Hierarchy Implementation

To conclude our discussion of memory hierarchies, we address several interesting issues that arise when they are realized in hardware and interfaced to a highperformance processor. Four topics are covered in this section: memory accessing mechanisms, cache memory implementations, TLB implementations, and interaction between cache memory and the TLB.multientry memory: indexing via an address, associative search via a tag, or a combination of the two. An indexed memory uses an address to index into the memory to select a particular entry; see Figure 3.4(a). A decoder is used to decode the n-bit address in order to enable one of the 2n entries for reading or writing. There is a rigid or direct mapping of an address to the data which requires the data to be stored in a fixed entry in the memory. Indexed or direct-mapped memory is rigid in this mapping but less complex to implement. In contrast, an associative memory uses a key to search through the memory to select a particular entry; see Figure 3.4(b). Each entry of the memory has a tag field and a comparator that compares the content of its tag field to the key. When a match occurs, that entry is selected. Using this form of associative search allows the data to be flexibly stored in any location of the memory. This flexibility comes at the cost of implementation complexity. A compromise between the indexed memory and the associative memory is the set-associative memory which uses both indexing and associative search; see Figure 3.4(c). An address is used to index into one of the sets, while the multiple entries within a set are searched with a key to identify one particular entry. This compromise provides some flexibility in the placement of data without incurring the complexity of a fully associative memory.

Main memory is normally implemented as a large indexed memory. However, a cache memory can be implemented using anyone of the three memory accessing schemes shown in Figure 3.4. When a cache memory is implemented as an indexed memory, it is referred to as a direct-mapped cache (illustrated in Figure 3.16).

Since the direct-mapped cache is smaller and has fewer entries than the main memory, it requires fewer address bits and its smaller decoder can only decode a subset of the main memory address bits. Consequently, many main memory addresses can be mapped to the same entry in the direct-mapped cache. To ensure the selected entry contains the correct data, the remaining, i.e., not decoded, When an entry of the cache is selected, its tag field is accessed and compared with the un decoded bits of the original address to ensure that the entry contains the data being addressed.

Figure 3.16(a) illustrates a direct-mapped cache with each entry, or block, containing one word. In order to take advantage of spatial locality, the block size of a cache usually contains multiple words as shown in Figure 3.16(b). With a multiword block, some of the bits from the original address are used to select the particular word being referenced. Hence, the original address is now partitioned into three portions: the index bits are used to select a block; the block offset bits are used to select a word within a selected block, and the tag bits are used to do a tag match against the tag stored in the tag field of the selected entry.

Cache memory can also be implemented as a fully associative or a set-associative memory, as shown in Figures 3.17 and 3.18, respectively. Fully associative caches have the greatest flexibility in terms of the placement of data in the entries of the cache. Other than the block offset bits, all other address bits are used as a key for associatively searching all the entries of the cache. This full associativity facilitates the most efficient use of all the entries of the cache, but incurs the greatest implementation complexity. Set-associative caches permit the flexible placement of data among all the entries of a set. The index bits select a particular set, the tag bits select an entry within the set, and the block offset bits select the word within the selected entry.

As discussed in Section 3.5, virtual memory requires mapping the virtual address space to the physical address space. This requires the translation of the virtual address into the physical address. Instead of directly accessing the main memory with the address generated by the processor, the virtual address generated by the processor must first be translated into a physical address. The physical address is then used to access the physical main memory, as shown in Figure 3.10.


As discussed in Section 3.5.3, address translation can be done using a translation memory that stores the virtual-to-real mappings; this structure is usually called a page table. The virtual address is used to index into or search the translation memory. The data retrieved from the selected entry in the translation memory are then used as the physical address to index the main memory. Hence, physical addresses that correspond to the virtual addresses are stored in the corresponding entries of the translation memory. Figure 3.19 illustrates the use of a translation memory to translate word addresses; i.e., it maps a virtual address of a word in the virtual address space into a physical address of a word in the physical main memory.

There are two weaknesses to the nai ve translation scheme shown in Figure 3.19.
First, translation of word addresses will require a translation memory with the  same number of entries as the main memory. This can result in doubling the size of the physical main memory. Translation is usually done at a coarser granularity.
Multiple (usually in powers of 2) words in the main memory can be grouped together into a page, and only addresses to each page need to be translated. Within the page, words can be selected using the lower-order bits of the virtual address, which form the page offset. This is illustrated in Figure 3.20. Within a virtual memory paging system, the translation memory is called the page table.

The second weakness of the translation memory scheme is the fact that two memory accesses are required for every main memory reference by an instruction.
First the page table must be accessed to obtain the physical page number, and then the physical main memory can be accessed using the translated physical page number along with the page offset. In actual implementations the page table is typically stored in the main memory (usually in the portion of main memory allocated to the operating system); hence, every reference to memory by an instruction requires two sequential accesses to the physical main memory. This can become a serious bottleneck to performance. The solution is to cache portions of the page table in a small, fast memory called a translation lookaside buffer (TLB).

A TLB is essentially a cache memory for the page table. Just like any other cache memory, the TLB can be implemented using anyone of the three memory accessing schemes of Figure 3.4. A direct-mapped TLB is simply a smaller (and faster) version of the page table. The virtual page number is partitioned into aninto the physical page number, which is concatenated with the page offset to form the physical address.

To ensure more flexible and efficient use of the TLB entries, associativity is usually added to the TLB implementation. Figure 3.22 illustrates the set-associative and fully associative TLBs. For the set-associative TLB , the virtual address bits are partitioned into three fields: index, tag, and page offset. The size of the page offset field is dictated by the page size which is specified by the architecture and the operating system. The remaining fields, i.e., index and tag, constitute the virtual V irtual address

Caching a portion of the page table into the TLB allows fast address translation; however, TLB misses can occur. Not all the virtual page to physical page mappings in the page table can be simultaneously present in the TLB. When accessing the TLB, a cache miss can occur, in which case the TLB must be filled from the page table sitting in the main memory. This can incur a number of stall cycles in the pipeline. It is also possible that a TLB miss can lead to a page fault. A page fault occurs when the virtual page to physical page mapping does not even exist in the page table. This means that the particular page being referenced is not resident in the main memory and must be fetched from secondary storage. To service a page fault requires invoking the operating system to access the disk storage and can require potentially tens of thousands of machine cycles. Hence, when a page fault is triggered by a program, that program is suspended from execution until the page fault is serviced by the operating system. This process is illustrated in Figure 3.12.

A data cache is used to cache a portion of the main memory; a TLB is used to cache a portion of the page table. The interaction between the TLB and the data cache is illustrated in Figure 3.23. The n-bit virtual address shown in Figure 3.23 is the effective address generated by the first pipe stage. This virtual address consists of a virtual page number (v bits) and a page offset (g bits). If the TLB is a setassociative cache, the v bits of the virtual page number is further split into a k-bit index and a (v-k)-bit tag. The second pipe stage of the load/store unit corresponds to the accessing of the TLB using the virtual page number. Assuming there is no TLB miss, the TLB will output the physical page number (p bits), which is then concatenated with the g-bit page offset to produce the m-bit physical address where m =p + g and m is not necessarily equal to n. During the third pipe stage the m-bit physical address is used to access the data cache. The exact interpretation of the Virtual block contains multiple words, then the lower-order b bits are used as a block offset to select the referenced word from the selected block. The selected block is determined by the remaining (m - b) bits. If the data cache is a set-associative cache, then the remaining (m - b) bits are split into a t-bit tag and an i-bit index. The value of i is determined by the total size of the cache and the set associativity; i.e., there should be i sets in the set-associative data cache. If there is no cache miss, then at the end of the third pipe stage (assuming the data cache can be accessed in a single cycle) the data will be available from the data cache (assuming a load instruction is being executed).

The organization shown in Figure 3.23 has a disadvantage because the TLB must be accessed before the data cache can be accessed. Serialization of the TLB and data cache accesses introduces an overall latency that is the sum of the two latencies. Hence, one might assume that address translation and memory access are done in two separate pipe stages. The solution to this problem is to use a virtually indexed data cache that allows the accessing of the TLB and the data cache to be performed in parallel. Figure 3.24 illustrates such a scheme.

A straightforward way to implement a virtually indexed data cache is to use only the page offset bits to access the data cache. Since the page offset bits do not require translation, they can be used without translation. The g bits of the page offset can be used as the block offset (b bits) and the index (i bits) fields in accessing the data cache. For simplicity, let's assume that the data cache is a direct-mapped cache of i entries with each entry, or block, containing 2b words. Instead of storing the remaining bits of the virtual address, i.e., the virtual page number, as its tag field, the data cache can store the translated physical page number in its tag field.

This is done at the time when a data cache line is filled. At the same time as the page offset bits are being used to access the data cache, the remaining bits of the virtual address, i.e. , the virtual page number, are used to access the TLB. Assuming the TLB and data cache access latencies are comparable, at the time when the 
physical page number from the TLB becomes available, the tag field (also containing the physical page number) of the data cache will also be available. The two p-bit physical page numbers can then be compared to determine whether there is a hit (matching physical page numbers) in the data cache or not. With a virtually indexed data cache, address translation and data cache access can be overlapped to reduce the overall latency. A classic paper by Wang, Baer, and Levy discusses many of the tradeoffs involved in designing a multilevel virtually addressed cache hierarchy [Wang et aI., 1989].




## 3.7 Input/Output Systems

Obviously, a processor in isolation is largely useless to an end user and serves no practical purpose. Of course, virtually everyone has interacted with computers of various types, either directly, through a keyboard and display device, or indirectly, through the phone system or some other interface to an embedded computing system. The purpose of such interaction is to either log information into a computer system and possibly request it to perform certain computations (input) and then either observe the result or allow the computer to directly interact with external devices (output). Thus, the computer system as a whole can be thought of as a black box device with some set of inputs, provided through various interfaces, and some set of outputs, also provided through a set of interfaces. These interfaces can interact directly with a human (by capturing keystrokes on a keyboard, movements of a mouse, or even spoken commands, and by displaying text or graphics or playing audio that are comprehensible to humans) or can instead interact with other digital devices at various speeds. This section discusses some of these devices and their attributes.

Table 3.4 summarizes some attributes of common input/output devices. For each device type, the table specifies how the device is connected to the system; whether it is used for input, output, both input and output, or storage; whether it communicates with a human or some other machine; and approximate data rates for these devices. The table makes clear that I/O devices are quite diverse in their characteristics, with data rates varying by seven orders of magnitude.


This section briefly discusses the I/O devices enumerated in Table 3.4 (mouse, keyboard, graphical displays, modems, LANs, and disk drives), and also provides an overview of high-performance and fault-tolerant disk arrays.
Mouse and Keyboard. A mouse and keyboard are used to provide direct user input to the system. The keyboard and mouse devices are usually connected to the system via a low-speed serial port. The universal serial bus (USB) is an example of a standardized serial port available on many systems today. The data rates for keyboards and mice are very low, as they are limited by the speed at which humans can type on a keyboard or operate a mouse. Since the data rates are so low, keyboard and mouse input are typically communicated to the CPU via external interrupts. Every key press or movement of the mouse ultimately invokes the operating system's interrupt handler, which then samples the current state of the mouse or the keyboard to determine which key was pressed or which direction the mouse moved so it can respond appropriately. Though this may appear to create an excessive rate of interrupts that might seriously perturb the processor, the low data rates of these devices generally avoid that problem on a single-user system. However, in a large-scale time-shared system that services keyboard input from hundreds or thousands of users, the interrupt rates quickly become prohibitive. In such environments, it is not unusual to provide terminal I/O controllers that handle keyboard interrupts from users and only communicate with the main processor once a cluster of keyboard activity has been aggregated at the controller. The modem-day equivalent of this type of aggregation of interactive 110 activity occurs when users enter data into a form on their Web browser: all the data entry is captured by the user' s Web browser client, and the Web server does not get involved until the user clicks on a submit button that transmits all the Web form data in a single transaction to the server. In this fashion, load on the server as well as the communication links between the client and the server is minimized, since only the aggregated information is communicated, rather than every keystroke.

Graphical Display. A graphical display conveys video or image data, illustrations, and formatted documents to the user, and also presents a user interface that simplifies the user's interaction with the system. Graphical displays must render a million or more pixels on the screen using a 24-bit color representation per pixel and usually update the screen at a rate of 60 or more frames per second. The contents of the screen are rendered in aframe buffer which contains a pixel-by-pixel representation of the contents of the screen. A random access memory digital-to-analog converter (RAMDAC) uses a high-speed interface to the frame buffer' s memory and converts the digitally represented image into an analog image that is displayed on a CRT (cathode-ray tube) or LCD (liquid-crystal display) monitor. The frame buffer contents are updated by a graphics processor that typically supports various schemes for accelerating two-dimensional and three-dimensional graphics transformations.

For example, dedicated hardware in the graphics processor pipeline can perform visibility checks to see if certain objects are hidden behind others, can correct for perspective in a three-dimensional environment, and can perform lighting, shading, andextremely high bandwidth to the frame buffer memory, as well as to main memory to access the image database, where objects are represented as collections of polygons in three-dimensional space. Hence, while graphical display adapters are connected to the main I/O bus of the system to interact with the main CPU, they also often utilize a special-purpose memory port [the accelerated graphics port (AGP) is an example of such a port] to enable high memory bandwidth for performing these transforms.

Modem. Modems are used to interconnect digital systems over an analog communication line, usually a standard telephone connection. Because of the nature of standard phone lines, they are only able to provide limited bandwidth, with a maximum of 56 kbits/s with the latest standard. Hence, because of the low overall data rates, modems are usually connected to the system via a low-speed serial port, like a USB or even older RS-232 serial port.  LAN. Local area network adapters are used to connect computer systems to each other. A LAN adapter must provide a physical layer interface that converts the computer's internal signal level digital data to the signaling technology employed by the LAN interface. Fast Ethernet, running at 100 Mbits/s, dominates the industry today, while Gbit Ethernet is rapidly being adopted. LAN adapters, due to their reasonably high data rates, are usually connected directly to the I/O backplane bus of the system to provide high bandwidth access to the system's main memory and to the processor. Originally, Ethernet was conceived as a shared bus-based interconnect scheme, but over time it has evolved into a switched, point-to-point organization where each computer system has a dedicated link to a centralized switch that is responsible for routing data packets to and from each of its ports based on the destination addresses of the packets. Ethernet switches can be connected hierarchically to allow larger number of systems to communicate.

Disk Drives. Magnetic disk drives store information on a platter by changing the orientation of a magnetic field at each individually addressable location on the platter.
As shown in Figure 3.25, a disk drive may contain multiple platters per spindle. Each platter has multiple concentric tracks that are divided into sectors. Read/write heads rest on a cushion of air on top of each spinning platter and seek to the desired track via a mechanical actuator. The desired sector is found by waiting for the disk to rotate to the desired position. Typical disks today rotate from 3000 to 15,000 revolutions per minute (rpm), contain anywhere from 500 to 2500 tracks with 32 or more sectors per track, and have platters with diameters ranging in size from 1 to 3.5 in.

Recent drives have moved to placing a variable number of sectors per track, where outer tracks with greater circumference have more sectors, and inner tracks with lesser circumference contain fewer tracks. This approach maintains constant areal bit density on the platter substrate, but complicates the read/write head control logic, since the linear velocity of the disk under the head varies with the track (with a higher velocity for outer tracks). Hence, the rate at which bits pass underneath the head also varies with the track, with a higher bit rate for outer tracks. In contrast, in older disks with a constant number of sectors and variable bit density, the bit rate that the head observed remained constant, independent of which track was being accessed. Some older disk drives, most notably the floppy drives in the original Apple Macintosh computers, held the bit rate and linear velocity constant by varying the rotational speed of the disk based on the position of the head, leading to an audible variation in the sound the drive generates. This approach substantially complicates the motor and its control electronics, making it infeasible for high-speed hard drives spinning at thousands of rpm, and has been abandoned in recent disk designs.

Latency = rotational + seek + transfer + queueing (3.7)

As shown in Equation (3.7), the access latency for a disk drive consists of the sum of four terms: the rotational latency, the seek latency, the transfer latency, and queueing delays. Rotational latency is determined by the speed at which the disk rotates. For example, a 5400-rpm drive completes a single revolution in (60 s)/ (5400 rpm) = 11 .1 ms. On average, a random access will have to wait half a revolution, leading to an average rotational latency of 11 .1 ms/2 = 5.5 ms for our 5400-rpm drive. The seek latency is determined by the number of tracks, the size of the platter, and the design of the seek actuator, and varies depending on the distance from the head' s current position to the target track. Typical average seek latencies range from 5 to 15 ms. The transfer latency is determined by the read/write head's data transfer rate divided by the block size. Data transfer rates vary from 1 to 4 Mbytes/s or more, while typical blocks are 512 bytes; assuming a 4-Mbyte transfer rate for a 512-b block, a drive would incur 0.12 ms of transfer latency. Finally, queueing delays in the controller due to multiple outstanding requests can consume 1 ms or more of latency. The final average latency for our example drive would add up to 5.5 ms (rotational latency) + 5 ms (seek latency) + 0.1 ms (transfer latency) + I ms (queueing latency) = 11.6 ms.

Modern disk drives also provide cache buffers ranging in size from 2 to 8 Mbytes that are used to capture temporal and spatial locality in the disk reference stream. These operate very similarly to processor caches and are often able toreducing the average disk latency by a considerable amount. Of course, worst-case access patterns that exhibit little spatial or temporal locality will still incur access latencies determined by the physical design of the disk, since they cannot be satisfied from the disk buffer.

Subsequent references to the same or nearby tracks or sectors can be satisfied much more quickly than the average case, since the rotational and seek latencies are minimized in those cases. Hence, modem operating systems attempt to reorder references in order to create a schedule that maximizes this type of spatial locality, hence minimizing average reference latency. As long as the operating system is aware of the physical disk layout of the blocks it is referencing, such scheduling is possible and desirable. Disk drive performance and modeling issues are discussed at length in a classic paper by Ruemmler and Wilkes [1994].

Disk Arrays. High-performance computer systems typically contain more than one disk to provide both increased capacity as well as higher bandwidth to and from the file system and the demand-paged backing store. Quite often, these disks are arranged in arrays that can be configured to provide both high performance as well as some degree of fault tolerance. In such arrays, data can be striped across multiple disks at varying levels of granularity to enable either higher data bandwidth or higher transaction bandwidth by accessing multiple disks in parallel.

Source: Patterson et aI., 1988.also increased dramatically. However, a single disk failure will cause the entire array to fail. RAID levell, also known as disk mirroring, addresses this by providing fault tolerance through mirroring of all data. This approach is simple to implement, but has very high overhead and provides no improvement in write bandwidth (since both copies must be updated), and only a doubling of read and read transaction bandwidth.

Higher levels of RAID protection use parity or error-correction codes (ECCs)1 to reduce the overhead of fault tolerance to much less than the 100% overhead required by RAID level l. In RAID level 2, word-level ECCs based on Hamming codes are used to identify and correct single errors. Conceptually, an ECC contains both a parity bit (used to check for a bit error in the coded word), as well as an offset that points to the data bit that is in error. Both the parity bit and offset are encoded using a Hamming code to minimize storage overhead and are used together to correct a bit error by flipping the bit at the specified offset whenever the parity bit indicates an error.

Unfortunately, the inherent overhead of word-level ECCs is high enough that RAID level 2 is impractical for all but the largest disk arrays, where large words can be spread across dozens of drives to reduce the ECC overhead. For example, the ECC SEeDED 2 overhead for a 64-bit word size is a minimum of 7 bits, requiring a disk array with 71 drives (64 data drives and 7 ECC drives) to achieve a reasonable 11 % overhead. Since the ECC SECDED overhead is much higher for smaller word sizes, RAID level 2 is rarely employed in arrays with few drives.

RAID level 3 replaces the ECCs with just parity, since failing drives can typically be detected by the disk array controller without the explicit error-correction-coded offset that identifies the failing bit (modern disks include diagnostic firmware that is able to report disk failure and even predict imminent failure to the disk controller). Using only parity reduces overhead and simplifies RAID implementation.

However, since data are striped at a very fine grain in RAID level 3 (at the bit level), each transaction requires the coordinated participation of all the drives in the parity set; hence, transaction bandwidth does not scale well. Instead, RAID level 4 maintains parity at a coarser block level, reducing the transaction overhead and supplying much better transaction bandwidth scaling. As illustrated in Figure 3.27, RAID level 4 places all parity blocks on the same drive. This leads to the parity drive bottleneck, since all writes must access this single drive to update their block set's parity. RAID level 5 solves the parity block bottleneck by rotating the parity blocks across all the drives, as shown at right in Figure 3.27.

RAID level 5 is widely used to provide both high performance and fault tolerance to protect against single disk failure. In RAID level 5, data blocks are independently stored across the disks in the array, while parity blocks covering a group of i For background information on error-correcting codes, which are not covered in detail in this book, the interested reader is referred to Blahut [1983] and Rao and Fujiwara [1989].


data blocks are interleaved in the drives as well. In terms of capacity overhead, this means that 1 of n disks is consumed for storing parity, in exchange for tolerance of single drive failures . Read performance is still very good, since each logical read requires only a single physical read of the actual data block. Write performance suffers slightly compared to RAID level 0, since the parity block must be updated in addition to writing the new data block. However, RAID level 5 provides a powerful combination of fault tolerance, reasonable overhead, and high performance, and is widely deployed in real systems.

Finally, RAID level 6 extends level 5 by maintaining two dimensions of parity for each block, requiring double the storage and write overhead of level 5 but providing tolerance of multiple disk failures. RAID level 6 is typically employed only in environments where data integrity is extremely important. The interested reader is referred to the original work by Patterson et al. [1988] for a more in-depth treatment of the advantages and disadvantages of the various RAID levels.

RAID controllers can be implemented either completely in hardware, with limited or no operating system involvement, or in the operating system's device driver (also known as software RAID). For example, the open-source Linux kernel supports software RAID levels 0, 1, and 5 over arrays of inexpensive, commodity integrated drive electronics (IDE) drives, and even across drives on separate LANconnected machines that are configured as network block devices (nbd). This makes it possible to implement fault-tolerant RAID arrays using very inexpensive, commodity PC hardware.

Hardware RAID controllers, typically used in higher-end server systems, implement RAID functionality in the controller's firmware. High-end RAID controllers often also support hot-swappable drives, where a failed or failing drive can be replaced on the fly, while the RAID array remains on-line. Alternatively, a RAID array can be configured to contain hot spare drives, and the controller can automatically switch in a hot spare drive for a drive that is about to fail or has already failed (this is called automated failover). During the period of time that a failed disk is still part of the array, all accesses to blocks stored on the failed diskreconstructing the contents of the missing block using the parity function. For example, in an array employing even parity across four disks, where the failing disk is the third disk, the controller might read a parity bit of < 1> and <O,l,?, 1> from the remaining good drives. Since even parity implies an even number of "I" bits across the parity set, the missing "?" bit is inferred to be a "I." Since each access to the failed drive requires coordinated reads to all the remaining drives in the parity set, this on-line forward error correction process can result in very poor disk performance until the failed disk has been replaced.

In a similar fashion, a RAID array with hot spares can automatically reconstruct the contents of the failed drive and write them to the spare disk, while alerting the system operator to replace the failed drive. In a RAID array that does not support hot spares, this reconstruction process has to be conducted either off line, after the array has been powered down and the failed disk replaced, or on line, as soon as the operator has hot-swapped the failed drive with a functioning one.



### 3.7.2 Computer System Busses

A typical computer system provides various busses for interconnecting the components we have discussed in the preceding sections. In an ideal world, a single communication technology would satisfy the needs and requirements of all system components and I/O devices. However, for numerous practical reasons-including cost, backward compatibility, and suitability for each application-numerous interconnection schemes are employed in a single system. Figure 3.28 shows three types of busses: the processor bus, the I/O bus, and a storage bus.

Processor busses are used to connect the CPU to main memory as well as to an I/O bridge. Since CPU performance depends heavily on a high-bandwidth, all the devices that connect to the processor bus (typically the CPU, the memory controller, and the 110 bridge; often referred to as the chip set) need to be updated at regular intervals. Because of this de facto update requirement, there is little or no pressure on processor busses to maintain backward compatibility beyond more than one processor generation. Hence, not only does the signaling technology evolve quickly, but also the protocols used to communicate across the bus adapt quickly to take advantage of new opportunities for improving performance. Section 11.3.7 provides some additional discussion on the design of processor busses and the coherent memory interface that a modern CPU needs to provide to communicate with a processor bus. Processor busses are also designed with electrical characteristics that match very short physical distances, since the components attached to this bus are usually in very close proximity inside the physical computer package. This enables very high speed signaling technologies that would be impossible or very expensive for longer physical distances.

In contrast to the processor bus, a typical 110 bus evolves much more slowly, since backward compatibility with legacy I/O adapters is a primary design constraint. In fact, systems will frequently support multiple generations of 110 busses to enable use of legacy adapters as well as modem ones. For example, many PC systems support both the peripheral component interface (PCI) 110 bus and the industry standard architecture (lSA) bus, where the ISA bus standard stretches back 15 years into the past. Also, for cost and physical design reasons, 110 busses usually employ less aggressive signaling technology, run at much lower clock frequencies , and employ less complex communication protocols than processor busses. For example, a modem PC system might have a 533-MHz processor bus with an 8-byte datapath, while the PCI 110 bus would run at 33 MHz with a 4-byte datapath. Since most peripheral 110 devices cannot support higher data rates anyway , for cost reasons the I/O busses are less aggressive in their design. The only standard peripheral that requires much higher bandwidth is a modem graphics processing unit (or display adapter); modern PCs provide a dedicated accelerated graphics port (AGP) to supply this bandwidth to main memory, while control and other communication with the display adapter still occurs through the PCI 110 bus.

110 busses typically need to span physical distances that are limited by the computer system enclosure; these distances are substantially longer than what the processor bus needs to span, but are still limited to less than 12 in. (30 cm) in most cases.
Finally, storage busses, used primarily to connect magnetic disk drives to the system, suffer even more from legacy issues and backward compatibility. As a result, they are often hobbled in their ability to adopt new signaling technology in a clean, straightforward fashion that does not imply less-than-elegant solutions.

For example, most storage busses are limited in their use of newer technology or signaling by the oldest peer sharing that particular bus. The presence of one oldperformance.
Storage busses must also be able to span much greater physical distances, since the storage devices they are connecting may reside in an external case or adjacent rack. Hence, the signaling technology and communication protocol must tolerate long transmission latencies. In the case of Fiber Channel, optical fiber links are used and can span several hundred meters, enabling storage devices to reside in separate buildings.

Simple busses support only a single concurrent transaction. Following an arbitration cycle, the device that wins the arbitration is allowed to place a command on the bus. The requester then proceeds to hold or occupy the bus until the command completes, which usually involves waiting for a response from some other entity that is connected to the same bus. Of course, if providing a response entails some long-latency event like performing a read from a disk drive, the bus is occupied for a very long time for each transaction. While such a bus is relatively easy to design, it suffers from very poor utilization due to these long wait times, during which the bus is effectively idle. In fact, virtually all modern bus designs support split transactions, which enable multiple concurrent requests on a single bus. On a split transaction bus, a requester first arbitrates for the bus, but then occupies the bus only long enough to issue the request, and surrenders the bus to the next user without waiting for the response. Some period of time later, the responder now arbitrates for the bus and then transmits the response as a separate bus event. In this fashion, transactions on the bus are split into two-and sometimes more than two-separate events. This interleaving of multiple concurrent requests leads to much better efficiency, since the bus can now be utilized to transfer independent requests and responses while a long-latency request is pending. Naturally, the design complexity of such a bus is much higher, since all devices connected to the bus must now be able to track outstanding requests and identify which bus transactions correspond to those requests. However, the far higher effective bandwidth that results justifies the additional complexity.

Figure 3.29 summarizes the key design parameters that describe computer system busses. First of all, the bus topology must be set as either point-to-point, which enables much higher frequency, or mUltidrop, which limits frequency due to the added capacitance of each electrical connection on the shared bus, but provides more flexible connectivity. Second, a particular signaling technology must be chosen to determine voltage levels, frequency , receiver/transmitter design, use of differential signals, etc. Then, several parameters related to actual data transfer must be set: the width of the data bus; whether or not the data bus lines are shared or multiplexed with the control lines; and either a single-word data transfer granularity, or support for multiword transfers, possibly including support for burst-mode operation that can saturate the data bus with back-to-back transfers. Also, a bidirectional bus that supports mUltiple signal drivers per data wire must provide a mechanism for turning the bus around to switch from one driver to another; this usually leads to dead cycles on the bus and reduces sustainable bandwidth (a unidirectional bus avoids this problem). Next, a clockingTopology 


strategy must also be set: the simplest option is to avoid bus clocks, instead employing handshaking sequences using request and valid lines to signal the presence of valid commands or data on the bus. As an alternative, a single shared clock can be used on a synchronous bus to avoid handshaking and improve bus utilization. Finally, an aggressive source-synchronous clocking approach can be used, where the clock travels with the data and commands, enabling the highest operating frequency and wave pipelining with multiple packets in flight at the same time. Finally, for bus designs that allow multiple bus masters to control themechanisms include daisy-chained arbiters, centralized arbiters, or distributed arbiters; while switching policies are either circuit-switched (also known as blocking), where a single transaction holds the bus until it completes, or packetswitched (also known as nonblocking, pipelined, or split transaction buses), where bus transactions are split into two or more packets and each packet occupies a separate slot on the bus, allowing for interleaving of packets from multiple distinct requests.

Modern high-performance bus designs are trending toward the following characteristics to maximize signaling frequency, bandwidth, and utilization: pointto-point connections with relatively few data lines to minimize crosstalk, sourcesynchronous clocking with support for burst mode transfers, distributed arbitration schemes, and support for split transactions. One interesting alternative bus design that has emerged recently is the simultaneous bidirectional bus: in this scheme, the bus wires have multiple pipelined source-synchronous transfers in flight at the same time, with the additional twist of signaling simultaneously in both directions across the same set of wires. Such advanced bus designs conceptually treat the digital signal as an analog waveform traveling over a well-behaved waveguide (i.e., a copper wire), and require very careful driver and receiver design that borrows concepts and techniques from the signal processing and advanced communications transceiver design communities.



### 3.7.3 Communication with 1/0 Devices
Clearly, the processor needs to communicate with I/O devices in the system using some mechanism. In practice, there are two types of communication that need to occur: control flow, which communicates commands and responses to and from the 110 device; and data flow, which actually transfers data to and from the I/O device. Control flow can further be broken down into commands which flow from the processor to the I/O device (outbound control flow), and responses signaling completion of the commands or other status information back to the processor (inbound control flow). Figure 3.30 summarizes the main attributes of I/O device communication that will be discussed in this section.


Outbound Control Flow. There are two basic approaches for communicating commands (outbound control flow) from the processor to the I/O device. The first of these is through programmed 110: certain instruction set architectures provide specific instructions for communicating with I/O devices; for example, the Intel IA-32 instruction set provides such primitives. These programmed 110 instructions are directly connected to control registers in the 110 devices, and the 110 devices react accordingly to changes written to these control registers. The main shortcoming of this approach is that the ISA provides only a finite set of 110 port interfaces to the processor, and in the presence of multiple I/O devices, they need to be shared or virtualized in a manner that complicates operating system device driver software. Furthermore, these special-purpose instructions do not map cleanly to1 - - - - - Control flow granularity f - - - - - - Fine-grained (shallow adapters) the pipelined and out-of-order designs described in this book, but require complex specialized handling within the processor core.

A more general approach for outbound control flow is to use memorymapped /10. In this approach, the device-specific control registers are mapped into the memory address space of the system. Hence, they can be accessed with conventional load and store instructions, with no special support in the ISA.

However, care must be taken in cache-based systems to ensure that the effects of loads and stores to and from these memory-mapped 110 registers are actually visible to the 110 device, rather than being masked by the cache as references that hit in the cache hierarchy. Hence, virtual memory pages corresponding to memory-mapped control registers are usually marked as caching inhibited or uncacheable in the virtual memory page table (refer to Section 3.5.3 for more information on page table design) . References to uncacheable pages must be routed off the processor chip and to the I/O bridge interface, which then satisfies them from the control registers of the 110 device that is mapped to the address in question.

Inbound Control Flow. For inbound control flow, i.e., responses or status information returned from the I/O devices back to the processor, there are two fundamental approaches: polling or interrupts. In a polling system, the operating system will intermittently check either a programmed I/O or memory-mapped control register to determine the status of the 110 device. While straightforward to implement, both in hardware and software, polling systems suffer from inefficiency, since theacross the processor and I/O busses of the system, these busses can become overwhelmed and begin to suffer from excessive queueing delays. Hence, a much cleaner and scalable approach involves utilizing the processor's support for external interrupts. Here, the processor is not responsible for polling the I/O device for completion. Rather, the I/O device instead is responsible for asserting the external interrupt line of the processor when it completes its activity, which then initiates the operating system' s interrupt handler and conveys to the processor that the I/O is complete. The interrupt signal is routed from the I/O device, through the 110 bridge, to the processor's external interrupt controller.

Control Flow Granularity. Command and response control flow can also vary in granularity. In typical PC-based systems, most 110 devices expose a very simple interface through their control registers. They can perform fairly simple activities to support straightforward requests like reading or writing a simple block of memory to or from a peripheral device. Such devices have very fine-grained control flow, since the processor (or the operating system device driver running on the processor) has to control such devices in a very fine-grained manner, issuing many simple commands to complete a more complex transaction with the peripheral I/O device.

In contrast, in the mainframe and minicomputer world, 110 devices will often expose a much more powerful and complex interface to the processor, allowing the processor to control those devices in a very coarse-grained fashion. For example, 110 channel controllers in IBM S/390-compatible mainframe systems can actually execute separate programs that contain internal control flow structures like loops and conditional branches. This richer functionality can be used to off-load such fine-grained control from the main CPU, freeing it to focus on other tasks. Modem three-dimensional graphics adapters in today's desktop PC systems are another example ofl/O devices with coarse-grained control flow. The command set available to the operating system device drivers for these adapters is semantically rich and very powerful, and most of the graphics-related processing is effectively offloaded from the main processor to the graphics adapter.


Data Flow. Data flow between the processor and I/O devices can occur in two fundamental ways. The first of these relies on instruction set support for programmed 110. Again, the ISA must specify primitives for communicating with 110 devices, and these primitives are used not just to initiate requests and poll for completion, but also for data transfer. Hence, the processor actually needs to individually read or write each word that is transferred to or from the 110 device through an internal processor register, and move it from there to the operating system's inmemory buffer. Of course, this is extremely inefficient, as it can occupy the processor for thousands of cycles whenever large blocks of data are being transferred.

These data transfers will also unnecessarily pollute and cause contention in the processor's cache hierarchy.devices. In effect, these devices can issue reads and writes directly to the main memory controller, just as the processor can. In this fashion, an 110 device can update an operating system's in-memory receive buffer directly, with no intervention from the processor, and then signal the processor with an interrupt once the transfer is complete. Conversely, transmit buffers can be read directly from main memory, and a transmission completion interrupt is sent to the processor once the transmission completes.

Of course, just as with memory-mapped control registers, DMA can cause problems in cache-based systems. The operating system must guarantee that any cache blocks that the 110 device wants to read from are not currently in the processor's caches, because otherwise the I/O device may read from a stale copy of the cache block in main memory. Similarly, if the 110 device is writing to a memory location, the processor must ensure that it does not satisfy its next read from the same location from a cached copy that has now become stale, since the 110 device only updated the copy in memory. In effect, the caches must be kept coherent with the latest updates to and from their corresponding memory blocks. This can be done manually, in the operating system software, by using primitives in the ISA that enable flushing blocks out of cache. This approach is called software cache coherence.

Alternatively, the system can provide hardware that maintains coherence; such a scheme is called hardware cache coherence. To maintain hardware cache coherence, the 110 devices' DMA accesses must be made visible to the processor's cache hierarchy. In other words, DMA writes must either directly update matching copies in the processor's cache, or those matching copies must be marked invalid to prevent the processor from reading them in the future. Similarly, DMA reads must be satisfied from the processor's caches whenever a matching and more up-to-date copy of a block is found there, rather than being satisfied from the stale copy in main memory. Hardware cache coherence is often achieved by requiring the processor's caches to snoop all read and write transactions that occur across the processor-memory bus and to respond appropriately to snooped commands that match cached lines by either invalidating them (when a bus write is snooped) or supplying the most up-todate data (when a bus read to a dirty line is snooped). Chapter 11 provides more details on hardware mechanisms for enforcing cache coherence.



### 3.7.4 Interaction of 1/0 Devices and Memory Hierarchy

As discussed in Section 3.7.3, direct memory access (DMA) by 110 devices causes a cache coherence problem in cache-based systems. Cache coherence is a more general and pervasive problem in systems that contain multiple processors, since each processor can now also update copies of blocks of memory locally in their cache, whereas the effects of those updates should be made visible to other processors in the system as well (i.e., their caches should remain coherent). We revisit this problem at length in Section 11.3 and describe cache coherence protocols that can be used to elegantly and efficiently solve this problem in systems with a few or even hundreds of processors.


In this example, three users time-share the CPU, overlapping their CPU usage with the disk latency and think time of the other interactive users. This increases overall throughput, since the CPU is always busy, but can increase the latency observed by each user. Latency increases due to context switch overhead and queuing delay (waiting for the CPU while another user is occupying it).

Temporal and spatial locality are adversely affected by time-sharing.

However, there is another interesting interaction that occurs with the memory hierarchy due to long-latency I/O events. In our discussion of demand-paged virtual memory subsystems in Section 3.5.1, we noted that the operating system will put a faulting process to sleep while it fetches the missing page from the backing store and will schedule an alternative process to run on the processor. This process is called time-sharing the CPU and is illustrated in Figure 3.31. The top half of the figure shows a single process first consuming CPU, then performing a longlatency disk access, then consuming CPU time again, and finally shows think time while waiting for the user to respond to program output. In the bottom half of the figure, other processes with similar behavior are interleaved onto the processor while the first process is waiting for disk "access" or for user response. Clearly, much better CPU utilization results, since the CPU is no longer idle for long periods of time.

However, this increased utilization comes at a price: since each process's execution is now dilated in time due to the intervening execution of other processes, the temporal locality of each process suffers, resulting in high cache miss rates.
Furthermore, the fact that the processor's memory hierarchy must now contain the working sets of all the active processes, rather than just a single active process, places great strain on the caches and reduces the beneficial effects of spatial locality.
As a result, there can be substantial increases in cache miss rates and substantially worse average memory reference latency in such heavily time-shared systems.improves at a glacial rate, the ratio of active processes that need to be scheduled to cover the latency of a single process' s I/O event is increasing rapidly. As a result, the effects pointed out in Figure 3.31 are more and more pronounced, and the effectiveness of cache-based memory hierarchies is deteriorating. We revisit this problem in Chapter 11 as we discuss systems that execute mUltiple threads simultaneously.




## 3.8 Summary

This chapter introduces the basic concept of a memory hierarchy, discusses various technologies used to build a memory hierarchy, and covers many of the effects that a memory hierarchy has on processor performance. In addition, we have studied some of the key input and output devices that exist in systems, the technology used to implement them, and the means with which they are connected to and interact with the processor and the rest of the computer system.

We also discussed the following memory idealisms and showed how a welldesigned memory hierarchy can provide the illusion that all of them are satisfied, at least to some extent:
* Infinite capacity. For storing large data sets and large programs.
* Infinite bandwidth. For rapidly streaming these large data sets and programs to and from the processor.
* Instantaneous or zero latency. To prevent the processor from stalling while waiting for data or program code.
* Persistence or non volatility. To allow data and programs to survive even when the power supply is cut off.
* Zero or very low implementation cost.

We have learned that the highest levels of the memory hierarchy-register files and primary caches-are able to supply near-infinite bandwidth and very low average latency to the processor core, satisfying the second and third idealisms.
The first idealism-infinite capacity-is satisfied by the lowest level of the memory hierarchy, since the capacities of DRAM-based memories are large enough to contain the working sets of most modern applications; for applications where this is not the case, we learned about a technique called virtual memory that extends the memory hierarchy beyond random-access memory devices to magnetic disks, which provide capacities that exceed the demands of all but the most demanding applications. The fourth idealism-persistence or nonvolatilitycan also be supplied by magnetic disks, which are designed to retain their state even when they are powered down. The final idealism-low implementation cost-is also satisfied, since the high per-bit cost of the upper levels of the cache hierarchy is only multiplied by a relatively small number of bits, while the lower levels of the hierarchy provide tremendous capacity at a very low cost per bit.

Hence, the average cost per bit is kept near the low cost of commodity DRAMmemories and register files.

## REFERENCES
Blahut, R. E.: Theory and Practice of Error Control Codes . Reading, MA: Addison-Wesley Publishing Company, 1983.

Cuppu, V., and B. L. Jacob: "Concurrency, latency, or system overhead: Which has the largest impact on uniprocesor DRAM-system petformance?," Proc. 28th Int. Symposium on Computer Architecture, 2001 , pp. 62-71.

Cuppu, V., B. L. Jacob, B. Davis, and T. N. Mudge: "A performance comparison of contemporary DRAM architectures," Proc. 26th into Symposium on Computer Architecture, 1999, pp. 222-233.

Goodman, J.: "Using cache memory to reduce processor-memory traffic," Proc. 10th into Symposium on Computer Architecture, 1983, pp. 124-131.

Hill, M., and A. Smith: "Evaluating associativity in CPU caches," iEEE Trans. on Computers, 38,12, 1989,pp. 1612- 1630.

Hill, M. D.: Aspects of Cache Memory and Instruction Buffer Performance. PhD thesis, University of California at Berkeley, Computer Science Division, 1987.

Intel Corp.: Pentium Processor User's Manual, Vol. 3: Architecture and Programming Manual. Santa Clara, CA: Intel Corp., 1993.

Keltcher, c., K. McGrath, A. Ahmed, and P. Conway: "The AMD Opteron processor for multiprocessor servers," IEEE Micro, 23, 2,2003, pp. 66-76.

Kilburn, T. , D. Edwards, M. Lanigan, and F. Sumner: "One-level storage systems," IRE Transactions, EC-ll , 2, 1962, pp. 223-235.

Lauterbach, G., and T. Hore!: "UltraSPARC-III: Designing third generation 64-bit petformance," IEEE Micro, 19,3, 1999, pp. 56-66.

Liptay, J.: "Structural aspects of the systemJ360 model 85, part ii ," IBM Systems Journal, 7, 1,1968, pp. 15-21.

Patterson, D., G. Gibson, and R. Katz: "A case for redundant arrays of inexpensive disks (RAID)," Proc. ACM SIGMOD Conference, 1988, pp. 109-116.

Rao, T. R. N., and E. Fujiwara: Error-Control Coding for Computer Systems. Englewood Cliffs, NJ: Prentice Hall, 1989.

Ruemmler, c., and J. Wilkes: "An introduction to disk drive modeling," IEEE Computer, 27, 3, 1994, pp. 5-15 .

Tendler, J. M., S. Dodson, S. Fields, and B. Sinharoy: "IBM eServer POWER4 system microarchitecture," IBM Whitepaper, 2001.

Wang, W.-H., J.-L. Baer, and H. Levy: "Organization and performance of a two-level virtual-real cache hierarchy," Proc. 16th Annual Int. Symposium on Computer Architecture, 1989, pp. 140-148.

Wilkes, M.: "Slave memories and dynamic storage allocation," IEEE Trans. on Electronic Computers, EC-14, 2,1965, pp. 270-271.

Wulf, W. A., and S. A. McKee: "Hitting the memory wall: Implications of the obvious," Computer Architecture News, 23, 1, 1995, pp. 20-24.

P3.1 Given the following benchmark code and assuming a virtually-addressed fully-associative cache with infinite capacity and 64-byte blocks, compute the overall miss rate (number of misses divided by number of references). Assume that all variables except array locations reside in registers and that arrays A, B, and C are placed consecutively in memory. Assume that the generated code for the loop body first loads from B, then from C, and finally stores to A.


P3.2 Given the example code in Problem 3.1 and assuming a virtuallyaddressed direct-mapped cache of capacity 8K-byte and 64-byte blocks, compute the overall miss rate (number of misses divided by number of references). Assume that all variables except array locations reside in registers and that arrays A, B, and C are placed consecutively in memory.

P3.3 Given the example code in Problem 3.1 and assuming a virtuallyaddressed two-way set-associative cache of capacity 8K-byte and 64byte blocks, compute the overall miss rate (number of misses divided by number of references). Assume that all variables except array locations reside in registers and that arrays A, B, and C are placed consecutively in memory.

P3.4 Consider a cache with 256 bytes. The word size is 4 bytes, and the block size is 16 bytes. Show the values in the cache and tag bits after each of the following memory access operations for the following two cache organizations: direct mapped and two-way associative. Also indicate whether the access was a hit or a miss. Justify. The addresses are in hexadecimal representation. Use LRU (least recently used) replacement algorithm wherever needed.

P3.6 Describe a program that has very high temporal locality. Write pseudocode for such a program, and show that it will have a high cache hit rate.pseudocode for such a program, and show that it will have a high cache miss rate.

P3.7 Write the programs of Problems 3.5 and 3.6 and compile them on a platform that supports performance counters (for example, Microsoft Windows and the Intel VTune performance counter software). Collect and report performance counter data that verifies that the program with high temporal locality experiences fewer cache misses.

P3.8 Write the programs of Problems 3.5 and 3.6 in C, and compile them using the Simplescalar compilation tools available from http://www .simplescalar.com. Download and compile the Simplescalar 3.0 simulation suite and use the sim-cache tool to run both programs. Verify that the program with high temporal locality experiences fewer cache misses by reporting cache miss rates from both programs.

P3.9 Describe a program that has very high spatial locality. Write pseudocode for such a program, and show that it will have a high cache hit rate.

P3.10 Describe a program that has very low spatial locality. Write pseudocode for such a program, and show that it will have a high cache miss rate.

P3.11 Write the programs of Problems 3.9 and 3.10 and compile them on a platform that supports performance counters (for example, Linux and the Intel VTune performance counter software). Collect and report performance counter data that verifies that the program with high temporal locality experiences fewer cache misses.

P3.12 Write the programs of Problems 3.9 and 3.10 in C, and compile them using the Simplescalar compilation tools available from http:// www.simplescalar.com. Download and compile the Simple-scalar 

## 3.0 simulation suite and use the sim-cache tool to run both programs. Verify that the program with high temporal locality experiences fewer cache misses by reporting cache miss rates from both programs.
P3.13 Consider a processor with 32-bit virtual addresses, 4K-byte pages, and 36-bit physical addresses. Assume memory is byte-addressable (i.e., the 32-bit virtual address specifies a byte in memory).
* Ll instruction cache: 64K bytes, 128-byte blocks, four-way setassociative, indexed and tagged with virtual address.
* Ll data cache: 32K bytes, 64-byte blocks, two-way set-associative, indexed and tagged with physical address, writeback.
* Four-way set-associative TLB with 128 entries in all. Assume the TLB keeps a dirty bit, a reference bit, and three permission bits (read, write, execute) for each entry.structures in the following table. Also, compute the total size in number of bit cells for each of the tag and data arrays.

P3.14 Given the cache organization in Problem 3.13, explain why accesses to the data cache would take longer than accesses to the instruction cache.
Suggest a lower-latency data cache design with the same capacity and describe how the organization of the cache would have to change to achieve the lower latency.

P3.15 Given the cache organization in Problem 3.13, assume the architecture requires writes that modify the instruction text (i.e., self-modifying code) to be reflected immediately if the modified instructions are fetched and executed. Explain why it may be difficult to support this requirement with this instruction cache organization.

P3.16 Assume a two-level cache hierarchy with a private level-l instruction cache (LlI), a private level-l data cache (LID), and a shared level-two data cache (L2). Given local miss rates for the 4% for LlI, 7.5% for LID, and 35% for L2, compute the global miss rate for the L2 cache.

P3.17 Assuming 1 LlI access per instruction and 0.4 data accesses per instruction, compute the misses per instruction for the LlI, LID, and L2 caches of Problem 3.16.

P3.18 Given the miss rates of Problem 3.16 and assuming that accesses to the LIl and LID caches take 1 cycle, accesses to the L2 take 12 cycles, accesses to main memory take 75 cycles, and a clock rate of 1 GHz, compute the average memory reference latency for this cache hierarchy.

P3.19 Assuming a perfect cache ePl (cycles per instruction) for a pipelined processor equal to 1.15 ePl, compute the MePl and overall ePl for a pipelined processor with the memory hierarchy described in Problem 3.18 and the miss rates and access rates specified in Problems 3.16 and 3.17.

P3.20 Repeat Problem 3.16 assuming an LlI local miss rate of 7%, an LID local miss rate of 3.5%, and an L210cal miss rate of 75% .

P3.21 Repeat Problem 3.17 given the miss rates of Problem 3.20.

P3.22 Repeat Problem 3.18 given the miss rates of Problem 3.20.

P3.23 Repeat Problem 3.19 given the miss rates of Problem 3.20.for such a processor, given the following parameters:

* Infinite cache CPI of 1.15
* LI cache miss penalty of 12 cycles
* L2 cache miss penalty of 50 cycles
* LI instruction cache per-instruction miss rate of 3% (0.03 misses/ instruction)
* LI data cache per-instruction miss rate of 2% (0.02 misses/ instruction).
* L2 local cache miss rate of 25% (0.25 missesIL2 reference).

P3.25 It is usually the case that a set-associative or fully associative cache has a higher hit rate than a direct-mapped cache. However, this is not always true. To illustrate this, show a memory reference trace for a program that has a higher hit rate with a two-block direct-mapped cache than a fully associative cache with two blocks.

P3.26 Download and install the Simplescalar 3.0 simulation suite and instructional benchmarks from www.simplescalar.com.Using the sim-cache cache simulator, plot the cache miss rates for each benchmark for the following cache hierarchy: 16K-byte two-way set-associative LI instruction cache with 64-byte lines; 32K-byte four-way set-associative LI data cache with 32-byte lines; 12K-byte eight-way set-associative L2 cache with 64-byte lines.

P3.27 Using the benchmarks and tools from Problem26, plot several missrate sensitivity curves for each of the three caches (LII, LID, L2) by varying each of the following parameters: cache size 0.5x, lx, 2x, 4x; associativity 0.5x, lx, 2x, 4x; block size 0.25x, 0.5x, lx, 2x, 4x. Hold the other parameters fixed at the values in Problem 3.26 while varying each of the three parameters for each sensitivity curve. Based on your sensitivity curves, identify an appropriate value for each parameter near the knee of the curve (if any) for each benchmark.

P3.28 Assume a synchronous front-side processor-memory bus that operates at 100 MHz and has an 8-byte data bus. Arbitration for the bus takes one bus cycle 00 ns), issuing a cache line read command for 64 bytes of data takes one cycle, memory controller latency (including DRAM access) is 60 ns, after which data doublewords are returned in back-toback cycles. Further assume the bus is blocking or circuit-switched.

Compute the latency to fill a single 64-byte cache line. Then compute the peak read bandwidth for this processor-memory bus, assuming the processor arbitrates for the bus for a new read in the bus cycle following completion of the last read.transfers, but multiplexes data and address lines. Assume that a read command requires a single bus cycle, and further assume that the memory controller has infinite DRAM bandwidth. Compute the peak data bandwidth for this front side bus.

P3.30 Building on the assumptions of Problem 3.29, assume the bus now has dedicated data lines and a separate arbitration mechanism for addresses/ commands and data. Compute the peak data bandwidth for this front side bus.
P3.31 Consider finite DRAM bandwidth at a memory controller, as follows.
Assume double-data-rate DRAM operating at 100 MHz in a parallel non-interleaved organization, with an 8-byte interface to the DRAM chips. Further assume that each cache line read results in a DRAM row miss, requiring a precharge and RAS cycle, followed by row-hit CAS cycles for each of the doublewords in the cache line. Assuming memory controller overhead of one cycle (10 ns) to initiate a read operation, and one cycle latency to transfer data from the DRAM data bus to the processor-memory bus, compute the latency for reading one 64-byte cache block. Now compute the peak data bandwidth for the memory interface, ignoring DRAM refresh cycles.

P3.32 Two page-table architectures are in common use today: multilevel forward page tables and hashed page tables. Write out a pseudocode function matching the following function declaration that searches a three-level forward page table and returns 1 on a hit and 0 on a miss, and assigns realaddress on a hit.

P3.34 Assume a single-platter disk drive with an average seek time of 4.5 ms, rotation speed of 7200 rpm, data transfer rate of 10 Mbytes/s per head, and controller overhead and queueing of 1 ms. What is the average access latency for a 4096-byte read?
